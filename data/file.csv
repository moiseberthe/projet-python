"Help!: DB structure MongoDB";"2023-01-09 01:19:33";"https://www.reddit.com/r/nosql/comments/106z4xb/help_db_structure_mongodb/";"I'm making a 'Choose your own adventure' web application, and I'm trying to figur out how to structure the database in order to relate the choice to the next passage of text that will show up. My original design was like so: Story Collection { ""title"": ""Tutorial"", ""author"": ""Rosie"", ""description"":""Learn how to play here!"" ""datePublished"": Date.Now, ""tags"": [""beginner"", ""learn"", ""start""], paths: [               [""0"":""Welcome to the game, to play,                       choose an option below.""],               [""0"":""This is boring already"",                ""1"":""I can't wait to play!""],               [""0"":""Oh dear, hopefully you'll change                 your mind once you get into a proper                 game!"",               ""1"":""AWESOME! You're already a pro at                      this I can tell!"" }  So basically the paths section is a 2D array so the paths[0][0] is the introduction passage of text, then paths[1][0] & paths[1][1] are the choice options... And the choice you pick leads to the corresponding passage in the next array... So paths[1][0] leads to paths[2][0] and paths[1][1] leads to paths[2][1].  But then my brain hurts once I get past that point... Because then each outcome will have its own options that aren't the same... So it doesn't work (just realised that after typing that all out!  TLDR; how would you structure database for a choose your own adventure game using NoSQL?";"RosieLMusic";"4";"Reddit"
"Is a NoSQL database the best option to handle the next model";"2022-12-27 17:49:28";"https://www.reddit.com/r/nosql/comments/zwk0re/is_a_nosql_database_the_best_option_to_handle_the/";"Hello guys, I'm learning by myself MongoDB and Firebase, my intention is to develop an application using the MERN stack, I've finished a CRUD to manage product's categories but now I need to manage product's subcategories; clearly this is a relational database model (category has many subcategories), so I would like to request your comments about these ones:  1. how can I handle this relationship in a NoSQL database? 2. in the subcategories table should be a categoryId field (foreign key)? 3. do you have any resources (books, links, etc) where I can clarify my actual and future questions about how to migrate from a RDBMS to a NoSQL?  Thanks a lot for your time.";"hftamayo";"5";"Reddit"
"CFP Open Cosmos Conf 2023";"2022-12-05 16:11:15";"https://www.reddit.com/r/nosql/comments/zd9dom/cfp_open_cosmos_conf_2023/";"Hey everyone!  Please check out the CFP for Cosmos Conf 2023!  Share how you've added speed, scale, and reliability to your applications with Azure Cosmos DB.  Event is March 28 and CFP closes Feb 1.  [https://aka.ms/CosmosConf2023CFP](https://aka.ms/CosmosConf2023CFP)  &#x200B;  Any questions about the event?  Leave them in the comments!";"jaydestro";"0";"Reddit"
"[Noob] Which NoSQL DB to choose for report data?";"2022-09-08 22:48:36";"https://www.reddit.com/r/nosql/comments/x9bcd4/noob_which_nosql_db_to_choose_for_report_data/";"Sorry for this noob question:    I think about trying to automate some reports  I need to create periodically.   And as a first step, I think about collecting (text and) data in documents within a NoSQL database.     There will be just a few documents for each report, like ""Chapter 1 text"", ""Chapter 1 prepared data"", Chapter 1 raw data"", and so on for like 6-8 chapters, and the prepared and raw data will be unstructured: tables, pictures, graphs, whatever. Each document will also include (or be tagged with) the current date and the customer name, so I'll be able to easily select all documents that are necessary for the report about customer x in month y.   So... which NoSQL database will be suited for my strange requirements? Maybe one with an easy to use frontend/client which allows me to easily interact with the database, display & manipulate documents etc.?   Thanks for your hints :)";"e_hyde";"3";"Reddit"
"How do I populate message id for the chat application in NoSql?";"2022-08-02 08:13:44";"https://www.reddit.com/r/nosql/comments/we4170/how_do_i_populate_message_id_for_the_chat/";"I plan to use dynamoDB to store the chat messages across the rooms. I use a table (rooms) in RDS to create room\_id when the room is created and then use this id (pk) in the DynamoDB when storing messages.  My table in DynamoDB might look like this.  PK: Room#<room\_id>  SK: Msg\_\_<msg\_id>  Since I storing messages just in DynamoDB unlike room\_id I don't have the ids for it from RDS. How do I generate them as I store the messages?";"Ok-Outlandishness-74";"0";"Reddit"
"Best key/query to access document in MongoDB collection?";"2022-07-25 02:11:39";"https://www.reddit.com/r/nosql/comments/w7a33n/best_keyquery_to_access_document_in_mongodb/";" Im currently creating a backend using FastAPI and MongoDB. I have a users collection that stores a bunch of User documents. Originally, for all my REST routes I planned on using a path like /user/{id} , but, correct me if I'm wrong, it would be difficult using Mongo's ID field as that ID is completely inaccessible from my frontend. Instead, I'm now considering using a field like the User's unique Spotify user id which is obtained when the user authenticates with Spotify. I'm also considering compositing this user\_id key with their phone number for an even more secure key. I would appreciate any direction regarding the best way to securely access a user's entry in the DB/collection. Thanks.";"charbrinks";"0";"Reddit"
"Best NoSQL system for economic database";"2022-07-16 17:56:58";"https://www.reddit.com/r/nosql/comments/w0jsh4/best_nosql_system_for_economic_database/";"I work in the economic team of a non-profit where I am responsible for all data related matters. Our team produces and consumes a lot of economic indicators and numbers. At the moment a lot of these numbers are buried within PDF documents (papers, articles,...) on our sharepoint.  I have been playing with the idea to save all relevant numbers in a database system. The numbers would have different tags depending on the subject and the specific indicator, a timestamp and a specific indicator name. Preferably, I would also be able to store the tags in a hierarchical way.  The following examples come to mind:  * **E-Commerce Sales for the fashion industry for Belgium in 2021:** The number should be saved in the following way:  &#8203;          {             ""tags"": {                 ""level 1"": ""E-Commerce"",                 ""level 2"": ""Fashion""             },             ""geography"": {""land"":""Belgium""}             ""timestamp"": {""year"":""2021""},             ""indicator"": ""E-Commerce Sales for the fashion industry for Belgium in 2021""                       }  * **Fashion Sales in Brussels in Q1 of 2022**:  &#8203;          {             ""tags"": {                 ""level 1"": ""Fashion"",             },             ""geography"": {""land"":""Belgium"", ""region"":""Brussels""}             ""timestamp"": {""year"":""2021"", ""quarter"":""Q4""},             ""indicator"": ""Fashion Sales in Brussels in Q1 of 2022""                       }  As you can see, values do not necessarily have the same amount of hierarchical levels in tags. Furthermore, both timestamp and geography have different levels of precision for different data points. I would like to save the possible tags in a table somewhere, so that for the different hierarchical tag levels, there is some kind of lookup to that specific table. I am quite convinced that my use case asks for a NoSQL implementation but since I am a newbie in NoSQL I don't know which system I should pick.  So my question is the following: Which system should I pick based on my specific use case? Thanks!";"SDR3078";"6";"Reddit"
"How hard is NoSQL to learn?";"2022-06-24 19:54:36";"https://www.reddit.com/r/nosql/comments/vjuanp/how_hard_is_nosql_to_learn/";"Coming from SQL and so used to relationship data. How hard is is to change gears? Is there a GUI that can show me the basics?";"seanner_vt2";"8";"Reddit"
"Ever worried about testing a NoSQL & .NET bundle?";"2022-06-19 12:40:42";"https://www.reddit.com/r/nosql/comments/vfsg1l/ever_worried_about_testing_a_nosql_net_bundle/";"Integration testing can be a real pain/cost of NoSQL development and may affect (or contribute to) the choice of the NoSQL engine on the basis of development convenience.  [Here](https://alex-klaus.com/ravendb-yabt-testing/) I compared the .NET SDKs and NuGet packages for 4 NoSQL engines: **RavenDB**, **CosmosDB**, **MongoDB** and **DynamoDB**. It seems that RavenDB comes first in the race, CosmosDB – last, and the rest sit in the middle.  What do you guys think? Did I miss anything?";"AlKla";"0";"Reddit"
"Which free and opensource NoSQL database provides feature for creating group/bucket of documents?";"2022-05-27 17:03:54";"https://www.reddit.com/r/nosql/comments/uz06rz/which_free_and_opensource_nosql_database_provides/";"I am learning CouchDB. As I understand it, documents in the database cannot be grouped into categories, such as, for example, all receipt documents can be put into a receipt bucket, invoices can be put into invoice bucket etc.  Are there any free and opensource NoSQL databases that provide this feature of grouping documents according to category?";"_448";"0";"Reddit"
"Post/Comment DB design: Postgresql v/s CouchDB";"2022-03-28 16:09:34";"https://www.reddit.com/r/nosql/comments/tqa57l/postcomment_db_design_postgresql_vs_couchdb/";"I am comparing DB design for a simple ""Post and Comment"" system using Postgres and CouchDB. With Postgres I can design the following tables:   user_info {email, pass_hash, pass_salt, ...}  post_info {post_id, creator_email, title, text, ...}  comment_info {comment_id, creator_email, post_id, parent_comment_id, text, ...}   But if I use CouchDB, there is a concept of creating per-user tables. So I was thinking of the following design:   user_table {email, table_id}  user_<table_id> {email, pass_hash, pass_salt, ...}  post_<table_id> {post_id, <table_id>_creator_email, title, text, ...}  comment_<table_id> {comment_id, <table_id>_creator_email, <table_id>_post_id, <table_id>_parent_comment_id, text, ...}   I am in no way expert in Postgres and CouchDB, so my question is, is this the correct way to design per-user CouchDB tables? What is the better way? And what is the efficient way to create/use CRUD queries?";"_448";"1";"Reddit"
"A little toolkit to help manage Redis";"2022-02-21 10:01:37";"https://www.reddit.com/r/nosql/comments/sxp2e1/a_little_toolkit_to_help_manage_redis/";"Docker image with several Redis CLI tools, and a tool to dump Redis data. The image includes browser-based IDE, filebrowser, and scheduler.  Simply run on a server or k8s cluster and schedule tasks.   It also has local Redis running and can be used as a better substitute for the local development environment instead of a standard Redis docker image.  [https://github.com/bluxmit/alnoda-workspaces/tree/main/workspaces/codeserver-workspace](https://github.com/bluxmit/alnoda-workspaces/tree/main/workspaces/codeserver-workspace)";"Bluxmit";"0";"Reddit"
"Noob Question - I've never used NoSQL before";"2022-02-02 17:16:09";"https://www.reddit.com/r/nosql/comments/siskfb/noob_question_ive_never_used_nosql_before/";"So I've worked with SQL practically every day ever since I started my career with data. I've never had to work with NoSQL but I do know that it's the opposite of SQL right, so non-relational. Now my question is how do you work with NoSQL? If I wanted to pull some data from a NoSQL database how do you query that? I've tried to google this but I haven't found anything - unless I have and it's nothing like querying something from MySQL or MSSQL. If someone can provide an example that'd be awesome.";"kkjeb";"11";"Reddit"
"Looking for good use cases for NoSQL";"2022-01-08 16:23:09";"https://www.reddit.com/r/nosql/comments/rz27cq/looking_for_good_use_cases_for_nosql/";"I’m fairly experienced with RDBMS and have watched a few tutorials and videos explaining NoSQL databases. I generally understand the technical differences at a theoretical level but am struggling to come up with some good use cases where NoSQL (particularly a document db such as MongoDB) is clearly a better choice over a RDBMS. I would also be interested in examples of use cases for a graph db such as Gremlin. Could anyone provide examples? Links to videos or blogs are also welcome.";"";"8";"Reddit"
"Wrapping my head around noSQL, specifically dynamodb";"2021-12-02 02:26:34";"https://www.reddit.com/r/nosql/comments/r6v3ar/wrapping_my_head_around_nosql_specifically/";"So, been reading up on noSQL tonight and I think I've got the idea here. Bascially (think a spreadsheet), the PK and SK are what I'm going to call by in an application and then we just replace the attributes based on the data we're putting in?  &#x200B;  Question I had is, is it normal to have a huge array of attributes.  So say for instance I'm working on an inspection app (I know the norm is task management but I like to be different and it's got a use case in my life)....  &#x200B;  I've got users, requirements (required inspections), inspections(the actual inspections themselves) and depending on the requirement (what kind of inspection it is, there could be different attributes).... that gets pretty insane, unless I've missed something.  &#x200B;  Here is kind of what I'm picturing in the following format:  pk, sk, attributes  \#USER#username, #PROFILE#username, name, address, phone, etc  \#USER#username, #REQ#<some\_identifier>, each attribute is then questions, or details about that inspection requirement  and inspections and so on....   &#x200B;  If I understand correctly, then I've got the capability to search for all the inspections the user has completed, all of the inspections the user is responsible for, the inspections completed by the user....   &#x200B;  &#x200B;  and if I understand secondary global indexes, then I can even add in a data attribute to search for completed inspections by date?  &#x200B;  Am I on the right track, I guess at this point I just needed to explain it somewhere so that I know I'm on the right track?";"Neat_Objective";"2";"Reddit"
"NoSQL productivity tools";"2021-10-28 06:09:31";"https://www.reddit.com/r/nosql/comments/qhel8g/nosql_productivity_tools/";"We are a Toronto-based software startup currently working on a new set of tools to increase productivity for companies using NoSQL technologies.  At this point, we are not selling anything, we are just looking for advice and feedback to help us build the best possible tools.  Our tools will help with the following:  1) Database diagramming  2) Visual comparison of table contents (within or across accounts)  3) Moving data between tables (within or across accounts) using visual tools  4) Synchronization of tables (within or across accounts)  5) Export of table scripts and comparison of table schemas (generating documentation in HTML & PDF)  We would love to get 30 minutes of your time to help us understand if any of these issues resonate with you and if so, your current workflows and how you are solving these problems today.  You can view more at the link below.  https://nosqlnavigator.com/  Please reply if you would like to discuss.";"braveheart2019";"0";"Reddit"
"Why is Cassandra considered column-based and DynamoDB key-value?";"2021-08-21 18:02:08";"https://www.reddit.com/r/nosql/comments/p8u0ua/why_is_cassandra_considered_columnbased_and/";"They rely on the exact same data model concept of having a table where we first identify the row / key / item and then select some columns / values in order to retrieve the wanted cell / attribute.  Here is one quote from a relevant article:  ""The top level data structure in Cassandra is the keyspace which is analogous to a relational database. The keyspace is the container for the tables and it is where you configure the replica count and placement. Keyspaces contain tables (formerly called column families) composed of rows and columns. A table schema must be defined at the time of table creation.  The top level structure for DynamoDB is the table which has the same functionality as the Cassandra table. Rows are items, and cells are attributes. In DynamoDB, it’s possible to define a schema for each item, rather than for the whole table.  Both tables store data in sparse rows—for a given row, they store only the columns present in that row. Each table must have a primary key that uniquely identifies rows or items. Every table must have a primary key which has two components.""  Sounds like pretty much the same thing. So, why the difference in terminology?";"uber_kuber";"3";"Reddit"
"Do you assign a name to your clients when connecting to redis or MongoDB?";"2021-08-10 08:05:58";"https://www.reddit.com/r/nosql/comments/p1jdmh/do_you_assign_a_name_to_your_clients_when/";"Hey all,  Lately, I was reminded about a feature to name your clients when connecting to your databases. From the NoSQL perspective, Redis and MongoDB are supporting this.  The basic idea is to identify the client against the database server. Depending on the system, the client name will be shown in several places like logs or in the monitoring endpoint.  **How it works with redis?**  Execute the [`CLIENT SETNAME`](https://redis.io/commands/client-setname) command like:      CLIENT SETNAME currency-conversion-app  It is a cheap (complexity: `O(1)`) command that can be executed without any overhead. Typically, you run it directly after the connection to the redis instance has been established.  With [`CLIENT LIST`](https://redis.io/commands/client-list) you can check who is connected:      $ CLIENT LIST     id=3 addr=172.17.0.1:62668 name=currency-conversion-app [...]     id=4 addr=172.17.0.1:62676 name=stock-exchange-rates-app [...]  **How it works with MongoDB?**  While creating a connection to MongoDB, you can provide an [`appName`](https://docs.mongodb.com/manual/reference/connection-string/#mongodb-urioption-urioption.appName) in the connection string.  Here is how it looks like in Go:      dsn := ""mongodb://root:secret@127.0.0.1:27017/?appName=currency-conversion-app""     client, err := mongo.Connect(ctx, options.Client().ApplyURI(dsn))  While checking the current operations with [`db.currentOp()`](https://docs.mongodb.com/manual/reference/method/db.currentOp/) the client name will be shown nicely.  **Useful in the real world?**  I can say, I use it all the time and it proved to be very useful.  Especially in bigger setups at work with multiple Redis nodes inside a Cluster and hundreds of clients.  While I was digging into it a bit more, I found out that several other systems, like MySQL, RabbitMQ, or NATS, which I use in combination with Redis, also support similar features.  So I documented how and especially WHY to do it here: [your database connection deserves a name](https://andygrunwald.com/blog/your-database-connection-deserves-a-name/).  I am curious: **Are you using this feature in your setup?** * If no, why not? * If yes, what was the situation where you thought, ""wow, this helped me a lot""?";"andygrunwald";"1";"Reddit"
"How to model foreign key like relationship in firestore";"2021-07-01 16:40:24";"https://www.reddit.com/r/nosql/comments/obn8au/how_to_model_foreign_key_like_relationship_in/";"Lets imagine I have this data model:  I have a student with name and age, and a student can be in a class and also in a sport team.  In a relational database I would store the students into a `student` column. And in the `class` and `sport` tables I would reference each students via a foreign key.  This has the advantage that when a student celebrates their birth date, I only need to change the `age` in one place, which is the `student` table.  With firestore which I understand to be a nosql, the things I am reading is pointing to a modeling where I have a `class` document, within which all `student` will be embedded. Same also for the `team` document.  The only problem I have with this kind of modeling is if I want to update the age of a student, I would have to update in all the places, the student structure is embedded in.  Is there a better way to achieve what I can have in relational database? Where I have data defined in one place and can be reference in other places, hence giving the benefit of needing to change that data in only one place?";"finlaydotweber";"0";"Reddit"
"How to design DDB to support finding who I am following of set of users?";"2021-06-23 22:01:37";"https://www.reddit.com/r/nosql/comments/o6kqp9/how_to_design_ddb_to_support_finding_who_i_am/";"Hey there r/nosql  &#x200B;  I'm designing a DDB to support a social graph where users can follow other users, users dont have to follow the user back though, one of the questions we need to answer is...  ***Given a user and the people following them, who of them am I following?***  It's basically finding the intersections of followers for two users, or mutual ""friends"". *Is there a key design that can support this type of lookup?* Any help is much appreciated, I've been pondering this for a long time.  **^(Note)**^(: I'm trying to avoid graph ddb as we have a partner teams that has had a lot of operational burden maintaining one.)";"thatcatpusheen";"1";"Reddit"
"NoSQL for a Relational DB Dude - Help CHANGE my thinking please!";"2021-04-28 09:14:36";"https://www.reddit.com/r/nosql/comments/n09964/nosql_for_a_relational_db_dude_help_change_my/";"I've been using relational dbs for years. Even though my brain is hard-wired for relational dbs, now I'm learning NoSQL. My biggest question has to do with relationships (1:1, 1:many, many:many). How do you determine whether to EMBED the actual data or simply include a reference/key? What are some considerations?  For example: A shopping cart website with user data, product data and order data. There are many scenarios to consider, such as....  * If I want all orders for a given user * If I want all users who ordered a specific product * If I wanted to know how many times a user ordered a specific product * If I wanted to know every product ever ordered by a user * Also, later, some data, such as a product name or user's mailing address or first name, might need to be modified, thereby having to propagate through all existing embedded data that is related. So, let's say the product ""Vit D3"" is renamed to ""Vitamin D3"", if the product is embedded in a million orders then I'd have to update a million order documents. Seems like alot of overhead! * Etc.  Seems like some scenarios would be more efficient to embed child data, while other times it seems better to use a reference/key. And, now my relational db side of my brain kicks in and I end up modeling everything with keys, like a relational db.  So, how do I change my thinking so now I think like a NoSQL guru, rather than a RDBMS guru? What's the process of evaluating these factors when modeling?  Thanks!";"webdevguycrypto";"10";"Reddit"
"Why did latest Starbase source code go offline?";"2021-04-14 00:05:34";"https://www.reddit.com/r/nosql/comments/mqcqwc/why_did_latest_starbase_source_code_go_offline/";"Where can it be found now? [Original URI](https://lweb.cfa.harvard.edu/~john/starbase/starbase.html)";"";"2";"Reddit"
"how does a NoSQL db scheme looks like?";"2021-04-12 12:15:53";"https://www.reddit.com/r/nosql/comments/mpavge/how_does_a_nosql_db_scheme_looks_like/";"I've no much experience with NoSQL db design.  I'm just looking at a MongoDB diagram designed by some colleagues at work, and it look totally like a relational db scheme. Tables, foreign keys, 1:1 and 1 to many relationships, and such. A real ERD diagram.  Is that it? or there are others ways to design NoSQL schemes?";"selfarsoner";"5";"Reddit"
"Kiwi.com: Nonstop Operations with Scylla Even Through the OVHcloud Fire";"2021-03-23 12:26:14";"https://www.reddit.com/r/nosql/comments/mbckik/kiwicom_nonstop_operations_with_scylla_even/";"&#x200B;  https://preview.redd.it/xbao12htiro61.jpg?width=800&format=pjpg&auto=webp&s=23315a0aeb2bb85cf6dba30191f2541fb71ef483  Disasters can strike any business on any day. This particular disaster, a fire at the OVHcloud Strasbourg datacenter, struck recently and the investigation and recovery are still ongoing. This is an initial report of one company’s resiliency in the face of that disaster.  ## Overview of the Incident  Less than an hour after midnight on Wednesday, March 10, 2021, in the city of Strasbourg, at 0:47 CET, [a fire began](https://us.ovhcloud.com/press/press-releases/2021/fire-our-strasbourg-site) in a room at the SBG2 datacenter of OVHcloud, the popular French cloud provider. Within hours the fire had been contained, but not before wreaking havoc. The fire nearly entirely destroyed SBG2, and gutted four of twelve rooms in the adjacent SBG1 datacenter. Additionally, combatting the fire required proactively switching off the other two datacenters, SBG3 and SBG4.  Netcraft [estimates](https://news.netcraft.com/archives/2021/03/10/ovh-fire.html) this disaster accounted for knocking out 3.6 million websites spread across 464,000 domains. Of those,184,000 websites across nearly 60,000 domains were in the French country code Top Level Domain (ccTLD) .FR — about 1 in 50 servers for the entire .FR domain. As Netcraft stated, “Websites that went offline during the fire included online banks, webmail services, news sites, online shops selling PPE to protect against coronavirus, and several countries’ government websites.”  [OVHcloud’s Strasbourg SBG2 Datacenter engulfed in flames. \(Image: SDIS du Bas Rhin \)](https://preview.redd.it/xdx7lx4xiro61.jpg?width=4000&format=pjpg&auto=webp&s=3c22e72b6b27dfb960bd201dd7d3d63dd8036cc2)  **\[This is just an excerpt. To read the story in full, please follow this link to the ScyllaDB website** [**here**](https://www.scylladb.com/2021/03/23/kiwi-com-nonstop-operations-with-scylla-even-through-the-ovhcloud-fire/)**.\]**";"PeterCorless";"0";"Reddit"
"A Shard-Aware Scylla C/C++ Driver";"2021-03-18 17:23:30";"https://www.reddit.com/r/nosql/comments/m7ujgq/a_shardaware_scylla_cc_driver/";"&#x200B;  https://preview.redd.it/jhvr3x8jbtn61.png?width=800&format=png&auto=webp&s=ff5b32c865abe4948cbd6ffb459f3d11af0c8e8d  We are happy to announce the first release of a shard-aware C/C++ driver (connector library). It’s an API-compatible fork of Datastax cpp-driver 2.15.2, currently packaged for x86\_64 CentOS 7 and Ubuntu 18.04 (with more to come!). It’s also easily compilable on most Linux distributions. The driver still works with Apache Cassandra and DataStax Enterprise (DSE), but when paired with Scylla enables shard-aware queries, delivering even greater performance than before.  [GET THE SCYLLA SHARD-AWARE C/C++ DRIVER](https://github.com/scylladb/cpp-driver)  \[This is just an excerpt. Read the blog in full on ScyllaDB's website [here](https://www.scylladb.com/2021/03/18/a-shard-aware-scylla-c-c-driver/).\]";"PeterCorless";"0";"Reddit"
"Zillow: Optimistic Concurrency with Write-Time Timestamps";"2021-03-16 17:06:49";"https://www.reddit.com/r/nosql/comments/m6chqm/zillow_optimistic_concurrency_with_writetime/";"&#x200B;  https://preview.redd.it/81pwfdepyen61.jpg?width=800&format=pjpg&auto=webp&s=d219f43845b0b92bfa29adb1dde2e85f16851212  Dan Podhola is a Principal Software Engineer at [Zillow](https://www.zillow.com/), the most-visited real estate website in the U.S. He specializes in performance tuning of high-throughput backend database services. We were fortunate to have him speak at our Scylla Summit on Optimistic Concurrency with Write-Time Timestamps. If you wish, you can watch the full presentation on-demand:  [WATCH THE ZILLOW PRESENTATION NOW](https://www.scylladb.com/presentations/zillow-optimistic-concurrency-using-write-time-timestamps/)  Dan began by describing his team’s role at Zillow. They are responsible for processing property and listing records — what is for sale or rent — and mapping those to a common Zillow property IDs, then translating different message types into a common interchange format so their teams can talk to each other using the same type of data.  They are also responsible for deciding what’s best to display. He showed a high-level diagram of what happens when they receive a message from one of their data providers. It needs to be translated into a common output format.  https://preview.redd.it/yankotlsyen61.png?width=1729&format=png&auto=webp&s=dccd6beb2aababc5b4ef08b3c7947c5f5ccec73c  “We fetch other data that we know about that property that’s also in that same format. We bundle that data together and choose a winner — I use the term ‘winner’ lightly here — and we send that bundle data out to our consumers.”  \[This is just an excerpt. You can read the blog in full at ScyllaDB's website [here](https://www.scylladb.com/2021/03/16/zillow-optimistic-concurrency-with-write-time-timestamps/).\]";"PeterCorless";"0";"Reddit"
"QOMPLX: Using Scylla with JanusGraph for Cybersecurity";"2021-03-11 19:52:56";"https://www.reddit.com/r/nosql/comments/m2xpr1/qomplx_using_scylla_with_janusgraph_for/";"&#x200B;  https://preview.redd.it/52pkaoxd3gm61.jpg?width=800&format=pjpg&auto=webp&s=fc0323ea917a248c4ed2f6cd34f6d1d4870b6179  [QOMPLX](https://www.qomplx.com/) is a company dedicated to solving complex problems, such as tackling the daunting world of cybersecurity. In this domain you need to be able to support a data model capable of rapid and repeated evolution to discover and counter new threats. This is one key reason why a graph database model is more applicable to QOMPLX’s use case than the rigidly-defined and statically-linked tables of a relational database.  &#x200B;  https://preview.redd.it/km6lhse63gm61.jpg?width=400&format=pjpg&auto=webp&s=8a43cff52f3825404937e41a55dcedb7653a2744  https://preview.redd.it/0xzpeyla3gm61.png?width=199&format=png&auto=webp&s=2bc6dcf0b9b4efc0876fb6f356846883e81b9da5  QOMPLX partnered with the graph database experts at [Expero](https://www.experoinc.com/) to implement their system with [JanusGraph](https://janusgraph.org/), which uses Scylla as an underlying fast and scalable storage layer. We had the privilege to learn from their use case at Scylla Summit this January, which we share with you today.  **\[This is just an excerpt. To watch the video or read the full blog, learning how QOMPLX uses JanusGraph, you can find more** [**here**](https://www.scylladb.com/2021/03/11/qomplx-using-scylla-with-janusgraph-for-cybersecurity/) **on the ScyllaDB website.\]**";"PeterCorless";"0";"Reddit"
"Making Shard-Aware Drivers for CDC";"2021-03-09 18:34:55";"https://www.reddit.com/r/nosql/comments/m1bmct/making_shardaware_drivers_for_cdc/";"&#x200B;  https://preview.redd.it/x47449hyf1m61.png?width=800&format=png&auto=webp&s=8e4820f2bec03c12a859212705cb361deb29ef37  [Change Data Capture (CDC)](http://docs.scylladb.com/using-scylla/cdc/) is a feature that allows users to track and react to changes in their dataset. CDC became production ready (GA) in [Scylla Open Source 4.3](https://www.scylladb.com/product/release-notes/scylla-open-source-4-3/).  Scylla’s implementation of CDC exposes a CQL-compatible interface that makes it possible to use existing tools or drivers to process CDC data. However, due to the unique way in which Scylla distributes CDC data across the cluster, the implementation of shard-awareness in some drivers might get confused and send requests to incorrect nodes or shards when reading CDC data. In this blog post, we will describe what causes this confusion, why it happens and how we solved it on the driver side.  ## Change Data Capture  In Scylla’s implementation CDC is enabled on a per-table basis. For each CDC-enabled table, a separate table called “CDC log” is created. Every time data is modified in the base table, a row is being appended to the CDC log table.  Inside a CDC log table, rows are organized into multiple partitions called “[streams](http://docs.scylladb.com/using-scylla/cdc/cdc-streams/)“. Each stream corresponds to a portion of the token ring (similarly to a vnode). In fact, a single stream corresponds to a part of a vnode which is owned by a single shard of that vnode’s primary replica. After a partition is changed in the base table, a stream is chosen based on the partition’s primary key, and then a row record describing this change is appended to that stream. Such partitioning into streams makes sure that a partition in the base table is stored on the same replicas as the CDC log rows describing changes made to it. This colocation property makes sure that the number of replicas participating in a write operation made on the base table does not increase.  **\[This is just an excerpt. To read the article in full, check it out on ScyllaDB** [**here**](https://www.scylladb.com/2021/03/09/making-shard-aware-drivers-for-cdc/)**. Also links to the latest drivers that implement this new change.\]**";"PeterCorless";"0";"Reddit"
"What are the different ways you can use MongoDB for e-commerce?";"2021-03-08 20:12:09";"https://www.reddit.com/r/nosql/comments/m0ne43/what_are_the_different_ways_you_can_use_mongodb/";"With its flexibility and scalability, MongoDB is a great option for e-commerce sites. Here are a few notable use cases.  **Product Catalogs**  Below is an example of a command using a product document with MongoDB:      db.inventory.insertOne( {       item: ""journal"",       price: 9.99,       qty: 25,       size: { h: 14, l: 21, w: 1 },       features: ""Beautiful, handmade journal."",      categories: [""writing"", ""bestseller""],      image: ""items/journal.jpg""       } )   **Shopping Cart**   The shopping cart data model needs to prevent customers from holding more items than are available in your inventory. The cart should also release any items back to your inventory when a user abandons their cart. Here is an insert() operation you can use to create the cart:      db.carts.insert({      _id: ""the_users_session_id"",      status:'active',      quantity: 3,      total: 575,      products: []});   **Payments**  Security is critical when modeling payments for e-commerce. MongoDB allows you to encrypt data files and perform automatic client-side encryption. You can also choose to only include the last four card digits, without any personally identifiable information in your model. In this case, you will meet PCI requirements without the need for encryption.  &#x200B;  [https://resources.fabric.inc/answers/mongodb-ecommerce](https://resources.fabric.inc/answers/mongodb-ecommerce)";"gibbiv";"1";"Reddit"
"Best Practices for Benchmarking Scylla";"2021-03-04 20:20:40";"https://www.reddit.com/r/nosql/comments/lxsses/best_practices_for_benchmarking_scylla/";"&#x200B;  https://preview.redd.it/awxdxyz3a2l61.png?width=800&format=png&auto=webp&s=393d745f783b0344cd63edab8f0f7683e1c54bed  Benchmarking is hard.  Or, I should say, doing a good, properly set up and calibrated, objective, and fair job of benchmarking is hard.  It is hard because there are many moving parts and nuances you should take into consideration and you must clearly understand what you are measuring. It’s not so easy to properly generate system load to reflect your real-life scenarios. It’s often not so obvious how to correctly measure and analyze the end results. After extracting benchmarking results you need to be able to read them, understand bottlenecks and other issues. You should be able to make your benchmarking results meaningful, ensure they are easily reproducible, and then be able to clearly explain these results to your peers or superiors.  There’s also hard mathematics involved: statistics and queueing theory to help with black boxes and measurements. Not to mention domain-specific knowledge of the system internals of the servers platforms, operating systems, and the software running on it.  With any Online Transaction Processing (OLTP) database — and Scylla is just one example — developers usually want to understand and measure the transaction read/write performance and what factors affect it. In such scenarios, there are usually a number of external clients constantly generating requests to the database. A number of incoming requests per unit of time called throughput or load.  *100,000 Operations per second or \[OPS\]*  Requests reach the database via a communication channel, get processed when the database is ready and then a response is sent back. The round trip time for a request to be processed is called latency. The ultimate goal of an OLTP database performance test is to find out what the latencies of requests are for various throughput rates.  *1ms per request*  There are thousands of requests that form the pattern of the workload. That’s why we don’t want to look at the latency for just individual requests, but rather, we should look at the overall results — a latency distribution. Latency distribution is a [function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) that describes how many requests were worse than some specific latency target.  *99 percentile or P99 or 99%*  Database systems can’t handle an infinite amount of load. There are limits that a system can handle. How much a system is close to its maximum is called utilization. The higher utilization the higher the latency (you can learn more about the math behind this here).  *80% utilization or 0.8*  The end-user doesn’t want to have high latencies for OLTP workloads — those types of workloads are reliant on fast updates. Therefore we target somewhere between 200ms to 10ms for 99 percentile of latency (P99) distribution. If your P99 latencies become too high, your request queues can back up, plus you risk having request timeouts in your application, which then can cascade out of hand in repeated retries, resulting in system bottlenecking.  \[This is just an excerpt. To read the article in full, which includes an in-depth guide on how to set up your benchmarks and calculate expected throughput, parallelism and latencies, check out ScyllaDB's website [here](https://www.scylladb.com/2021/03/04/best-practices-for-benchmarking-scylla/).\]";"PeterCorless";"0";"Reddit"
"ScyllaDB: Project Circe February Update";"2021-03-01 18:05:27";"https://www.reddit.com/r/nosql/comments/lvedz8/scylladb_project_circe_february_update/";"&#x200B;  https://preview.redd.it/d4fiqklg7gk61.png?width=1024&format=png&auto=webp&s=18545c829819b0a1cefb052421915a72d80d17cb  [Project Circe](https://www.scylladb.com/2021/01/12/making-scylla-a-monstrous-database-introducing-project-circe/) is our 2021 initiative to improve Scylla by adding greater capabilities for consistency, performance, scalability, stability, manageability and ease of use. For this installment of our monthly updates on Project Circe, we’ll take a deep dive into the Raft consensus protocol and the part it will play in Scylla, as well as provide a roundup of activities across our software development efforts.  ## Raft in Scylla  At Scylla Summit 2021, ScyllaDB engineering team lead Konstantin “Kostja” Osipov presented on the purpose and implementation of the Raft consensus protocol in Scylla. Best known for his work on [Lightweight Transactions (LWT)](https://www.scylladb.com/2020/07/15/getting-the-most-out-of-lightweight-transactions-in-scylla/) in Scylla using a more efficient implementation of the Paxos protocol, Kostja began with a roundup of those activities, including our recently conducted [Jepsen testing](https://www.scylladb.com/2020/12/23/jepsen-and-scylla-putting-consistency-to-the-test/) to see how our Lightweight Transactions behaved under various stresses and partitioned state conditions.  \[This is just an excerpt. To read the full blog that discusses how Scylla will be able to make schema changes and scale out better using Raft, plus a link to the video, go [here](https://www.scylladb.com/2021/03/01/project-circe-february-update/).\]";"PeterCorless";"0";"Reddit"
"Scylla University: New Lessons for February 2021";"2021-02-24 20:20:49";"https://www.reddit.com/r/nosql/comments/lrl0ui/scylla_university_new_lessons_for_february_2021/";"&#x200B;  https://preview.redd.it/ttrvbby17hj61.png?width=800&format=png&auto=webp&s=72acf9ce8de18a4ef2b8a07cfc9adc79bfa8fe35  In my [previous](https://www.scylladb.com/2021/02/16/whats-new-at-scylla-university-for-february-2021/) blog post, I wrote about the top students for 2020, the Scylla Summit Training Day, getting course completion certificates, and other news. In this blog post I’ll talk about new lessons added to [Scylla University](https://university.scylladb.com/) since our [June 2020 update](https://www.scylladb.com/2020/06/23/scylla-university-updates-june-2020/).  \[This is just an excerpt. To read the full list of new courses available in Scylla University, read more [here](https://www.scylladb.com/2021/02/24/scylla-university-new-lessons-for-february-2021/).\]";"PeterCorless";"0";"Reddit"
"Prometheus Backfilling: Recording Rules and Alerts";"2021-02-23 19:26:45";"https://www.reddit.com/r/nosql/comments/lqp9qu/prometheus_backfilling_recording_rules_and_alerts/";"&#x200B;  https://preview.redd.it/nwt9mvvns9j61.png?width=800&format=png&auto=webp&s=e6be689495c0a4747d7ed25c25c259c614f0844f  For many [Prometheus](https://prometheus.io/) users using recording rules and alerts, a known issue is how both are only generated on the fly at runtime. This limitation has two downsides. First of all, any new recording rule will not be applied to your historical data. Secondly and even more troubling, you cannot even test your rules and alerts against your historical data.  There is active work inside Prometheus to change this, but it’s not there yet. In the short term, to meet this requirement we created a simple utility to produce [OpenMetrics](https://openmetrics.io/) data to fill in the gaps. I will cover the following topics in this blog post:  * Generating OpenMetrics from Prometheus * Backfilling alerts and recording rules  \[This is just an excerpt. Please read the blog in full at ScyllaDB [here](https://www.scylladb.com/2021/02/23/prometheus-backfilling-recording-rules-and-alerts/).\]";"PeterCorless";"3";"Reddit"
"Expedia Group: Our Migration Journey to Scylla";"2021-02-18 19:28:17";"https://www.reddit.com/r/nosql/comments/lmtidg/expedia_group_our_migration_journey_to_scylla/";"&#x200B;  https://preview.redd.it/zgiamd784ai61.jpg?width=800&format=pjpg&auto=webp&s=2c3b9e78515f9e7ae61832c638b6e28713783e66  Expedia Group, the multi-billion-dollar travel brand, presented at our recent Scylla Summit 2021 virtual event. Singaram “Singa” Ragunathan and Dilip Kolosani presented their technical challenges, and how Scylla was able to solve them.  Currently there are multiple applications at Expedia built on top of Apache Cassandra. “Which comes with its own set of challenges,” Singa noted. He highlighted four top issues:  &#x200B;  https://preview.redd.it/8c0utq794ai61.png?width=1670&format=png&auto=webp&s=67e9fbf2ff70e22e4ddf8d7a36a2a851607e796a  * **Garbage Collection:** The first well-known issue is with Java Virtual Machine (JVM) Garbage Collection (GC). Singa noted, “Apache Cassandra, written in Java, brings in the onus of managing garbage collection and making sure it is appropriately tuned for the workload at hand. It takes a significant amount of time and effort, as well as expertise required, to handle and tune the GC pause for every specific use case.” * **Burst Traffic & Infrastructure Costs:** The next two interrelated issues for Expedia are burst traffic which leads to overprovisioning. “With burst traffic or a sudden peak in the workload there is significant disturbance to the p99 response time. So we end up having buffer nodes to handle this peak capacity, which results in more infrastructure costs.” * **Infrequent Releases:** “Another significant worry” for Expedia, according to Singa, was Cassandra’s infrequent release schedule. “According to the past years’ history, the number of Apache Cassandra releases has significantly slowed down.”  Showing a comparative timeline between Cassandra and Scylla, Singa continued, “We would like to compare the open source commits in Cassandra versus Scylla in a timeline chart here, and highlight the amount of releases that Scylla has gone through in the same past three year period. As you can see, it gives enough confidence towards Scylla that, given an issue or bug with a specific release, it will be soon addressed with a patch. In contrast with Apache Cassandra, one might have to wait longer.  &#x200B;  https://preview.redd.it/n3ar3jga4ai61.png?width=3840&format=png&auto=webp&s=37484b2195bcbdc74d1d2b6bc5fb97ce65863f2d  *Timeline created by Expedia showing the update frequency of Cassandra compared to Scylla.*  \[This is just an excerpt. To read the blog in full and view the full Scylla Summit 2021 presentation, go [here](https://www.scylladb.com/2021/02/18/expedia-group-our-migration-journey-to-scylla/).\]";"PeterCorless";"0";"Reddit"
"Kvrocks 1.3.0 is released";"2021-02-08 13:28:14";"https://www.reddit.com/r/nosql/comments/lfas1i/kvrocks_130_is_released/";"Kvrocks is a key value database which based on rocksdb, and compatible with the Redis protocol, intention to decrease the cost of memory and increase the capability.   Now 1.3.0 is release, more compatible with Redis [https://github.com/bitleak/kvrocks/releases/tag/v1.3.0](https://github.com/bitleak/kvrocks/releases/tag/v1.3.0)  Welcome to try!";"ShooterIT";"0";"Reddit"
"Cassandra paging";"2021-02-05 14:32:46";"https://www.reddit.com/r/nosql/comments/ld76qe/cassandra_paging/";"So I have a rather large table to read and I need to use ""ALLOW FILTERING"" . I read a little on how to avoid it and I came across pagination in Cassandra.  So we use sqlalchemy to connect to our database   My question is, how do we set the ""fetch\_size""? Is it possible to set it in the query itself?   Or do I need to use a session object and set the fetch\_size and then loop through the results?   I am somewhat new to Cassandra so a small code snippet would be helpful.  Thanks a lot";"king_booker";"0";"Reddit"
"Entity Relationships in NoSQL: One-to-one, one-to-many, many-to-many...";"2021-02-02 11:37:59";"https://www.reddit.com/r/nosql/comments/last92/entity_relationships_in_nosql_onetoone_onetomany/";"This topic pops up here from time-to-time (e.g. [6 months ago](https://www.reddit.com/r/nosql/comments/hysfas/resources_on_nosqlmongodb_database_modeling/)), when newbies coming from RDBMS ask about approaching  building entity relationships.  [Here I published](https://alex-klaus.com/nosql-entity-relationship/) a brief rundown on  ways of approaching it in NoSQL:  1. Embedded collection. 2. Reference by ID. 3. Duplicating often used fields. 4. Many-to-many relationship (array of references).  Provided examples (for RavenDB) and source code on GitHub.  Hope, it'd be useful for some. Any feedback is welcome!";"AlKla";"0";"Reddit"
"Syncing databases back and forth?";"2021-01-27 01:35:56";"https://www.reddit.com/r/nosql/comments/l5s1xf/syncing_databases_back_and_forth/";"I've been thinking about a solution that would independent individuals to work on local databases and sync/merge their local databases to a remote one. The idea would be to allow people continue to work even on intermittent network connection situations.  Things I though about or tried:  1. SQLite -> PostgreSQL/MySQL  I actually built a small system for this. I'd log all SQL in a journal and executed them again against the remote server once the user clicked in a ""Sync"" button - it would also ""download"" the log and sync remote changes to the local database. How I managed to avoid conflicts between different clients? All tables had an ID column (that was the or part of a unique index) and every client used a different ID. It worked, but was cumbersome. Main problem was in intermediate tables to implement many-to-many relationships.  2. Use the same as above, but with a K-V database with simplier relationship implemented in application level. Not sure if it would be too different from the solution above.  3. Use a blockchain-like structure? Maybe a database that implements something like [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree) (like git and bitcoin)?  Anyway, I'd like to ask if you have any suggestions. Solutions can be either at the database (preferably), library or application level.";"ilikefruits22foo";"2";"Reddit"
"Should I use SQL row or nosql JSON to store chat messages?";"2021-01-08 11:10:44";"https://www.reddit.com/r/nosql/comments/kszoi2/should_i_use_sql_row_or_nosql_json_to_store_chat/";" I am currently psql for my application and I need to store chat messages every time a user sends a message. I was wondering if I should store that as a traditional row or should I store that as a JSON data.  Also constant read and write to the database feels like a bad idea but I am not sure of how else to do it. Please let me know what you think I should do with this challenge";"warrior242";"11";"Reddit"
"Choosing between SQL & NoSQL db for storage of research article data";"2020-10-31 14:59:21";"https://www.reddit.com/r/nosql/comments/jlitpp/choosing_between_sql_nosql_db_for_storage_of/";"Hi,  Looking for guidance, as no real-world exp. with NoSQL deployment. Objective is to store research article data, this would include paper title, paper body text, paper abstract, authors ids, journal ids, publish date, categories etc.  &#x200B;  A paper is the main entity. A unique paper can have several authors, and so a single author can have co-authors.  Authors can be associated with more than 1 paper.  My instinct tells me I have structured data, with all entities (columns) known, and hence go with SQL db.  &#x200B;  I currently don't see any advantage in using NoSQL to persist that kind of data, where such structure is known in advance.  I would really appreciate critical argument against that and any support for using NoSQL in such case, and how I might ""model"" such (e.g. paper container, author container or other).  &#x200B;  With regard to use case of data, I'll be encoding the body text from all papers for NLP processing (e.g. training models for search), plus being able to list all papers per author, show all co-authors of a given author, show all papers published by a specific journal (e.g. Nature), list papers within a timeframe etc.  Thanks in advance!";"LostGoatOnHill";"8";"Reddit"
"how do you start designing a NoSQL db?";"2020-10-23 14:40:32";"https://www.reddit.com/r/nosql/comments/jgmc80/how_do_you_start_designing_a_nosql_db/";"My mind seems to like more Relational databases. The structure is more clear to me, and easy to design around a model.  So my process is to start with a relational model and next ""transform"" it into a NoSQL database, denormalizing and nesting relationships.  Anybody could point out a ""native"" NoSQL design process or some good guides/examples?";"selfarsoner";"6";"Reddit"
"TO ALL PICK MULTIVALUE USERS";"2020-10-22 21:32:57";"https://www.reddit.com/r/nosql/comments/jg6q45/to_all_pick_multivalue_users/";"I am new to the Pick world and I am looking for a community. Is there anyone on this page that uses pick? If not do you know of a page that does use Pick? I'd love to meet anyone on here currently using pick and share thoughts.   Thank you!";"Bizboosterpodcast";"2";"Reddit"
"NoSQL in a Real World Complex App?";"2020-10-05 23:45:41";"https://www.reddit.com/r/nosql/comments/j5smnl/nosql_in_a_real_world_complex_app/";"I have taken a number of courses explaining how to work with different NoSQL databases, but I'm still struggling tremendously with understanding how NoSQL is architected in the real world.  For example, I'm going through a DynamoDB course right now and the instructor talks about having to plan everything really well in advance, like the keys and local secondary indices, etc. And that you're limited to the number of local and global secondary indices you can have and that the local secondary indices have to be created at table creation time and can't be changed later. Maybe it's just me, but I have NEVER worked at or heard of a company that can define that stuff up front and have it stay valid for the life of the application. This makes me think that the only way to use NoSQL for anything real would be to define very generalized keys and indices, but I can see that falling apart really fast in a complex app.  It comes down for me that I just can't wrap my head around using NoSQL in place of the relational DB in my complex app. I have thought about breaking off pieces of functionality and using NoSQL for the smaller piece, but, ultimately, I have to correlate all of the data together for reporting and dashboards and such. I just don't understand how this viably works with NoSQL.  Perhaps what I really need are some architecture design patterns focused around NoSQL that explains how all of the different pieces come together to give me functionality that mimics what I get from a traditional RDBMS.  Am I making any sense at all? I really want to give NoSQL a chance, but I just don't know how to go about it. Thanks for your help in advance.";"djolord";"6";"Reddit"
"MongoDb vs ElasticSearch for read operations?";"2020-08-25 17:59:14";"https://www.reddit.com/r/nosql/comments/igexq5/mongodb_vs_elasticsearch_for_read_operations/";"My organization is contemplating using ElasticSearch for ALL read operations. And mongoDb as a database for write operations.  What are your views on it? We do not have a requirement of full text search as such. But what we do have is complicated queries that could involve multiple collections and various operations such as lookup(join), group by, filter criteria etc.    How do Elasticsearch query language/capabilities compare against MongoDb?";"OptimusPrime3600";"16";"Reddit"
"Dynatron - Bridge between AWS DynamoDB Document Client and Real World usage";"2020-08-10 22:43:39";"https://www.reddit.com/r/nosql/comments/i7d6ky/dynatron_bridge_between_aws_dynamodb_document/";"This library is a result of years of working with AWS DynamoDB and overcoming underwater rocks, missing optimizations and hidden issues that are very hard to catch (like hanging SSL connections in 0.2% of cases).  Homepage - [https://93v.github.io/dynatron/](https://93v.github.io/dynatron/)  Github - [https://github.com/93v/dynatron](https://github.com/93v/dynatron)  NPM Package - [https://www.npmjs.com/package/dynatron](https://www.npmjs.com/package/dynatron)";"gevorggalstyan";"4";"Reddit"
"Best free apps to model NoSQL database on for assignment.";"2020-08-04 01:05:06";"https://www.reddit.com/r/nosql/comments/i37s0g/best_free_apps_to_model_nosql_database_on_for/";"Hey,  I'm doing an assignment for university just hoping that someone where would be able to tell me what good free software is out there for modelling nosql.  Thank you!";"pixie-warrior";"0";"Reddit"
"Resources on NoSQL(MongoDb) database modeling ?";"2020-07-27 15:38:38";"https://www.reddit.com/r/nosql/comments/hysfas/resources_on_nosqlmongodb_database_modeling/";"I have been working with RDBMS until now. I am not supposed to design database in MongoDB. I found a lot of good resources to learn MongoDb (querying etc) but I am looking for good resources to get the hang of database modeling.   May be some real world industry examples?";"OptimusPrime3600";"2";"Reddit"
"The Complete DynamoDB bootcamp - free for 3 days";"2020-07-19 13:29:43";"https://www.reddit.com/r/nosql/comments/htz12b/the_complete_dynamodb_bootcamp_free_for_3_days/";" Unlimited redemption, but valid only for 3 days. This is a special offer of a paid course that I'm offering for 3 days for free as a gift to the community. Feel free to share if you want   [https://www.udemy.com/course/the-complete-dynamodb-bootcamp/?couponCode=E6F7763676D26D8CF43F](https://www.udemy.com/course/the-complete-dynamodb-bootcamp/?couponCode=E6F7763676D26D8CF43F)  Have you ever wondered if there is a course to help you learn the basics of DynamoDB quickly without getting bogged down in details? This is the course for you then! We cover all the basics of NoSQL and DynamoDB and even take on a few advanced topics - all in about 3 hours. Join in and you'll end up being a confident DynamoDB user at the end!   Full disclosure - the course doesn't have a great rating partly due to the fact that it has only gathered 12 ratings, but you can check out my profile @  [https://www.udemy.com/user/rammohan4/](https://www.udemy.com/user/rammohan4/)  to understand that most of my courses are rated at 4+ out of 5 stars.";"rmohan80";"0";"Reddit"
"Delete old documents";"2020-06-25 12:53:22";"https://www.reddit.com/r/nosql/comments/hfk22k/delete_old_documents/";"Quite an interesting case. I have an enormous MongoDB collection with lots of documents. These are two of the fields ( I changed the field names).      {     ""pidNumber"" : NumberLong(12103957251),      ""eventDate"" : ISODate(""2018-05-15T00:00:00.000+0000"")     }   I need to count all the instances where the date is older than 1 year but ONLY if there's a more recent document with the same pidNumber.   So for example: If there's only one document with pidNumber 1234 and it's from three years ago - keep it (don't count). But if on top of that there's another document with pidNumber 1234 and it's from two years ago - then count the three years old one.  Is it possible to do? Does anyone have on how to do it?  Thanks ahead!";"HeadTea";"0";"Reddit"
"data organization";"2020-06-22 16:35:13";"https://www.reddit.com/r/nosql/comments/hdtg4g/data_organization/";"I was wondering how social media apps and fitness apps organize their database to minimize costs and minimize load time for the user. There should be a collection for users and posts or workouts but should there be any data duplication? I’m worried about having to read a ton of post documents when a user opens up their feed. Any tips?";"makesmthnew";"1";"Reddit"
"Hierarchical Dirichlet Process and Relative Entropy";"2022-10-24T11:56:06Z";"http://arxiv.org/abs/2210.13142v1";"The Hierarchical Dirichlet process is a discrete random measure serving as an important prior in Bayesian non-parametrics. It is motivated with the study of groups of clustered data. Each group is modelled through a level two Dirichlet process and all groups share the same base distribution which itself is a drawn from a level one Dirichlet process. It has two concentration parameters with one at each level. The main results of the paper are the law of large numbers and large deviations for the hierarchical Dirichlet process and its mass when both concentration parameters converge to infinity. The large deviation rate functions are identified explicitly. The rate function for the hierarchical Dirichlet process consists of two terms corresponding to the relative entropies at each level. It is less than the rate function for the Dirichlet process, which reflects the fact that the number of clusters under the hierarchical Dirichlet process has a slower growth rate than under the Dirichlet process.";"Shui Feng";"0";"Arxiv"
"Spectral analysis of communication networks using Dirichlet eigenvalues";"2011-02-17T23:59:16Z";"http://arxiv.org/abs/1102.3722v2";"The spectral gap of the graph Laplacian with Dirichlet boundary conditions is computed for the graphs of several communication networks at the IP-layer, which are subgraphs of the much larger global IP-layer network. We show that the Dirichlet spectral gap of these networks is substantially larger than the standard spectral gap and is likely to remain non-zero in the infinite graph limit. We first prove this result for finite regular trees, and show that the Dirichlet spectral gap in the infinite tree limit converges to the spectral gap of the infinite tree. We also perform Dirichlet spectral clustering on the IP-layer networks and show that it often yields cuts near the network core that create genuine single-component clusters. This is much better than traditional spectral clustering where several disjoint fragments near the periphery are liable to be misleadingly classified as a single cluster. Spectral clustering is often used to identify bottlenecks or congestion; since congestion in these networks is known to peak at the core, our results suggest that Dirichlet spectral clustering may be better at finding bona-fide bottlenecks.";"Alexander Tsiatas, Iraj Saniee, Onuttom Narayan, Matthew Andrews";"0";"Arxiv"
"Dirichlet Process Mixtures of Generalized Mallows Models";"2012-03-15T11:17:56Z";"http://arxiv.org/abs/1203.3496v1";"We present a Dirichlet process mixture model over discrete incomplete rankings and study two Gibbs sampling inference techniques for estimating posterior clusterings. The first approach uses a slice sampling subcomponent for estimating cluster parameters. The second approach marginalizes out several cluster parameters by taking advantage of approximations to the conditional posteriors. We empirically demonstrate (1) the effectiveness of this approximation for improving convergence, (2) the benefits of the Dirichlet process model over alternative clustering techniques for ranked data, and (3) the applicability of the approach to exploring large realworld ranking datasets.";"Marina Meila, Harr Chen";"0";"Arxiv"
"Clustering consistency with Dirichlet process mixtures";"2022-05-25T17:21:42Z";"http://dx.doi.org/10.1093/biomet/asac051";"Dirichlet process mixtures are flexible non-parametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.";"Filippo Ascolani, Antonio Lijoi, Giovanni Rebaudo, Giacomo Zanella";"0";"Arxiv"
"A Hierarchical Dirichlet Process Model with Multiple Levels of   Clustering for Human EEG Seizure Modeling";"2012-06-18T15:02:12Z";"http://arxiv.org/abs/1206.4616v1";"Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process---the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We find the MLC-HDP's clustering to be comparable to independent human physician clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.";"Drausin Wulsin, Shane Jensen, Brian Litt";"0";"Arxiv"
"The supervised hierarchical Dirichlet process";"2014-12-17T01:16:31Z";"http://dx.doi.org/10.1109/TPAMI.2014.2315802";"We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.";"Andrew M. Dai, Amos J. Storkey";"0";"Arxiv"
"Scalable Inference for Latent Dirichlet Allocation";"2009-09-25T05:23:33Z";"http://arxiv.org/abs/0909.4603v1";"We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.";"James Petterson, Tiberio Caetano";"0";"Arxiv"
"Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process   Mixture";"2013-05-28T23:59:16Z";"http://arxiv.org/abs/1305.6659v2";"This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.";"Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin";"0";"Arxiv"
"Dirichlet-tree multinomial mixtures for clustering microbiome   compositions";"2020-08-02T05:17:02Z";"http://arxiv.org/abs/2008.00400v2";"Studying the human microbiome has gained substantial interest in recent years, and a common task in the analysis of these data is to cluster microbiome compositions into subtypes. This subdivision of samples into subgroups serves as an intermediary step in achieving personalized diagnosis and treatment. In applying existing clustering methods to modern microbiome studies including the American Gut Project (AGP) data, we found that this seemingly standard task, however, is very challenging in the microbiome composition context due to several key features of such data. Standard distance-based clustering algorithms generally do not produce reliable results as they do not take into account the heterogeneity of the cross-sample variability among the bacterial taxa, while existing model-based approaches do not allow sufficient flexibility for the identification of complex within-cluster variation from cross-cluster variation. Direct applications of such methods generally lead to overly dispersed clusters in the AGP data and such phenomenon is common for other microbiome data. To overcome these challenges, we introduce Dirichlet-tree multinomial mixtures (DTMM) as a Bayesian generative model for clustering amplicon sequencing data in microbiome studies. DTMM models the microbiome population with a mixture of Dirichlet-tree kernels that utilizes the phylogenetic tree to offer a more flexible covariance structure in characterizing within-cluster variation, and it provides a means for identifying a subset of signature taxa that distinguish the clusters. We perform extensive simulation studies to evaluate the performance of DTMM and compare it to state-of-the-art model-based and distance-based clustering methods in the microbiome context. Finally, we report a case study on the fecal data from the AGP to identify compositional clusters among individuals with inflammatory bowel disease and diabetes.";"Jialiang Mao, Li Ma";"0";"Arxiv"
"Percolation Perturbations in Potential Theory and Random Walks";"1998-04-02T10:59:37Z";"http://arxiv.org/abs/math/9804010v1";"We show that on a Cayley graph of a nonamenable group, almost surely the infinite clusters of Bernoulli percolation are transient for simple random walk, that simple random walk on these clusters has positive speed, and that these clusters admit bounded harmonic functions. A principal new finding on which these results are based is that such clusters admit invariant random subgraphs with positive isoperimetric constant.   We also show that percolation clusters in any amenable Cayley graph almost surely admit no nonconstant harmonic Dirichlet functions. Conversely, on a Cayley graph admitting nonconstant harmonic Dirichlet functions, almost surely the infinite clusters of $p$-Bernoulli percolation also have nonconstant harmonic Dirichlet functions when $p$ is sufficiently close to 1. Many conjectures and questions are presented.";"Itai Benjamini, Russell Lyons, Oded Schramm";"0";"Arxiv"
"Numerical conformal mapping with rational functions";"2019-11-09T14:16:35Z";"http://arxiv.org/abs/1911.03696v1";"New algorithms are presented for numerical conformal mapping based on rational approximations and the solution of Dirichlet problems by least-squares fitting on the boundary. The methods are targeted at regions with corners, where the Dirichlet problem is solved by the ""lightning Laplace solver"" with poles exponentially clustered near each singularity. For polygons and circular polygons, further simplifications are possible.";"Lloyd N. Trefethen";"0";"Arxiv"
"Revisiting k-means: New Algorithms via Bayesian Nonparametrics";"2011-11-02T00:09:18Z";"http://arxiv.org/abs/1111.0352v2";"Bayesian models offer great flexibility for clustering applications---Bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical Bayesian models can be utilized for sharing clusters across multiple data sets. For the most part, such flexibility is lacking in classical clustering methods such as k-means. In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired by the asymptotic connection between k-means and mixtures of Gaussians, we show that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph.";"Brian Kulis, Michael I. Jordan";"0";"Arxiv"
"Bayesian Clustering of Transcription Factor Binding Motifs";"2006-10-22T03:38:20Z";"http://arxiv.org/abs/math/0610655v1";"Genes are often regulated in living cells by proteins called transcription factors (TFs) that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based upon a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our clusters provide a means by which to organize transcription factors based on binding motif similarities, which can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC. Finally, our clustering procedure has been used in combination with discovery of evolutionarily-conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is explored but shows no substantive difference in the clustering results for our datasets. Our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications.";"Shane T. Jensen, Jun S. Liu";"0";"Arxiv"
"DIMM-SC: A Dirichlet mixture model for clustering droplet-based single   cell transcriptomic data";"2017-04-06T20:01:29Z";"http://arxiv.org/abs/1704.02007v1";"Motivation: Single cell transcriptome sequencing (scRNA-Seq) has become a revolutionary tool to study cellular and molecular processes at single cell resolution. Among existing technologies, the recently developed droplet-based platform enables efficient parallel processing of thousands of single cells with direct counting of transcript copies using Unique Molecular Identifier (UMI). Despite the technology advances, statistical methods and computational tools are still lacking for analyzing droplet-based scRNA-Seq data. Particularly, model-based approaches for clustering large-scale single cell transcriptomic data are still under-explored. Methods: We developed DIMM-SC, a Dirichlet Mixture Model for clustering droplet-based Single Cell transcriptomic data. This approach explicitly models UMI count data from scRNA-Seq experiments and characterizes variations across different cell clusters via a Dirichlet mixture prior. An expectation-maximization algorithm is used for parameter inference. Results: We performed comprehensive simulations to evaluate DIMM-SC and compared it with existing clustering methods such as K-means, CellTree and Seurat. In addition, we analyzed public scRNA-Seq datasets with known cluster labels and in-house scRNA-Seq datasets from a study of systemic sclerosis with prior biological knowledge to benchmark and validate DIMM-SC. Both simulation studies and real data applications demonstrated that overall, DIMM-SC achieves substantially improved clustering accuracy and much lower clustering variability compared to other existing clustering methods. More importantly, as a model-based approach, DIMM-SC is able to quantify the clustering uncertainty for each single cell, facilitating rigorous statistical inference and biological interpretations, which are typically unavailable from existing clustering methods.";"Zhe Sun, Ting Wang, Ke Deng, Xiao-Feng Wang, Robert Lafyatis, Ying Ding, Ming Hu, Wei Chen";"0";"Arxiv"
"Consistency Analysis for the Doubly Stochastic Dirichlet Process";"2016-05-24T10:13:19Z";"http://arxiv.org/abs/1605.07358v1";"This technical report proves components consistency for the Doubly Stochastic Dirichlet Process with exponential convergence of posterior probability. We also present the fundamental properties for DSDP as well as inference algorithms. Simulation toy experiment and real-world experiment results for single and multi-cluster also support the consistency proof. This report is also a support document for the paper ""Computationally Efficient Hyperspectral Data Learning Based on the Doubly Stochastic Dirichlet Process"".";"Xing Sun, Nelson H. C. Yung, Edmund Y. Lam, Hayden K. -H. So";"0";"Arxiv"
"Powered Dirichlet Process for Controlling the Importance of   ""Rich-Get-Richer"" Prior Assumptions in Bayesian Clustering";"2021-04-26T11:36:23Z";"http://arxiv.org/abs/2104.12485v1";"One of the most used priors in Bayesian clustering is the Dirichlet prior. It can be expressed as a Chinese Restaurant Process. This process allows nonparametric estimation of the number of clusters when partitioning datasets. Its key feature is the ""rich-get-richer"" property, which assumes a cluster has an a priori probability to get chosen linearly dependent on population. In this paper, we show that such prior is not always the best choice to model data. We derive the Powered Chinese Restaurant process from a modified version of the Dirichlet-Multinomial distribution to answer this problem. We then develop some of its fundamental properties (expected number of clusters, convergence). Unlike state-of-the-art efforts in this direction, this new formulation allows for direct control of the importance of the ""rich-get-richer"" prior.";"Gaël Poux-Médard, Julien Velcin, Sabine Loudcher";"0";"Arxiv"
"Flexible clustering via hidden hierarchical Dirichlet priors";"2022-01-18T13:59:38Z";"http://dx.doi.org/10.1111/sjos.12578";"The Bayesian approach to inference stands out for naturally allowing borrowing information across heterogeneous populations, with different samples possibly sharing the same distribution. A popular Bayesian nonparametric model for clustering probability distributions is the nested Dirichlet process, which however has the drawback of grouping distributions in a single cluster when ties are observed across samples. With the goal of achieving a flexible and effective clustering method for both samples and observations, we investigate a nonparametric prior that arises as the composition of two different discrete random structures and derive a closed-form expression for the induced distribution of the random partition, the fundamental tool regulating the clustering behavior of the model. On the one hand, this allows to gain a deeper insight into the theoretical properties of the model and, on the other hand, it yields an MCMC algorithm for evaluating Bayesian inferences of interest. Moreover, we single out limitations of this algorithm when working with more than two populations and, consequently, devise an alternative more efficient sampling scheme, which as a by-product, allows testing homogeneity between different populations. Finally, we perform a comparison with the nested Dirichlet process and provide illustrative examples of both synthetic and real data.";"Antonio Lijoi, Igor Prünster, Giovanni Rebaudo";"0";"Arxiv"
"Bayesian mixture models (in)consistency for the number of clusters";"2022-10-25T17:47:29Z";"http://arxiv.org/abs/2210.14201v1";"Bayesian nonparametric mixture models are common for modeling complex data. While these models are well-suited for density estimation, their application for clustering has some limitations. Miller and Harrison (2014) proved posterior inconsistency in the number of clusters when the true number of clusters is finite for Dirichlet process and Pitman--Yor process mixture models. In this work, we extend this result to additional Bayesian nonparametric priors such as Gibbs-type processes and finite-dimensional representations of them. The latter include the Dirichlet multinomial process and the recently proposed Pitman--Yor and normalized generalized gamma multinomial processes. We show that mixture models based on these processes are also inconsistent in the number of clusters and discuss possible solutions. Notably, we show that a post-processing algorithm introduced by Guha et al. (2021) for the Dirichlet process extends to more general models and provides a consistent method to estimate the number of components.";"Louise Alamichel, Daria Bystrova, Julyan Arbel, Guillaume Kon Kam King";"0";"Arxiv"
"A Bayesian View of the Poisson-Dirichlet Process";"2010-07-02T05:10:49Z";"http://arxiv.org/abs/1007.0296v2";"The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the Dirichlet Process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. There is a rich literature about the PDP and its derivative distributions such as the Chinese Restaurant Process (CRP). This article reviews some of the basic theory and then the major results needed for Bayesian modelling of discrete problems including details of priors, posteriors and computation.   The PDP allows one to build distributions over countable partitions. The PDP has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of PDPs, and second using a marginalised relative the CRP, one gets fragmentation and clustering properties that lets one layer partitions to build trees. This article presents the basic theory for understanding the notion of partitions and distributions over them, the PDP and the CRP, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. This article also presents a Bayesian interpretation of the Poisson-Dirichlet process based on an improper and infinite dimensional Dirichlet distribution. This means we can understand the process as just another Dirichlet and thus all its sampling properties emerge naturally.   The theory of PDPs is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. This context and basic results are also presented, as well as techniques for computing the second order Stirling numbers that occur in the posteriors for discrete distributions.";"Wray Buntine, Marcus Hutter";"0";"Arxiv"
"From here to infinity - sparse finite versus Dirichlet process mixtures   in model-based clustering";"2017-06-22T07:44:23Z";"http://arxiv.org/abs/1706.07194v3";"In model-based-clustering mixture models are used to group data points into clusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli et al (2016) are sparse finite mixtures, where the prior distribution on the weight distribution of a mixture with $K$ components is chosen in such a way that a priori the number of clusters in the data is random and is allowed to be smaller than $K$ with high probability. The number of cluster is then inferred a posteriori from the data.   The present paper makes the following contributions in the context of sparse finite mixture modelling. First, it is illustrated that the concept of sparse finite mixture is very generic and easily extended to cluster various types of non-Gaussian data, in particular discrete data and continuous multivariate data arising from non-Gaussian clusters. Second, sparse finite mixtures are compared to Dirichlet process mixtures with respect to their ability to identify the number of clusters. For both model classes, a random hyper prior is considered for the parameters determining the weight distribution. By suitable matching of these priors, it is shown that the choice of this hyper prior is far more influential on the cluster solution than whether a sparse finite mixture or a Dirichlet process mixture is taken into consideration.";"Sylvia Frühwirth-Schnatter, Gertraud Malsiner-Walli";"0";"Arxiv"
"Flexible Priors for Exemplar-based Clustering";"2012-06-13T15:52:35Z";"http://arxiv.org/abs/1206.3294v1";"Exemplar-based clustering methods have been shown to produce state-of-the-art results on a number of synthetic and real-world clustering problems. They are appealing because they offer computational benefits over latent-mean models and can handle arbitrary pairwise similarity measures between data points. However, when trying to recover underlying structure in clustering problems, tailored similarity measures are often not enough; we also desire control over the distribution of cluster sizes. Priors such as Dirichlet process priors allow the number of clusters to be unspecified while expressing priors over data partitions. To our knowledge, they have not been applied to exemplar-based models. We show how to incorporate priors, including Dirichlet process priors, into the recently introduced affinity propagation algorithm. We develop an efficient maxproduct belief propagation algorithm for our new model and demonstrate experimentally how the expanded range of clustering priors allows us to better recover true clusterings in situations where we have some information about the generating process.";"Daniel Tarlow, Richard S. Zemel, Brendan J. Frey";"0";"Arxiv"
"Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge   Sampling on Dirichlet Process Mixtures";"2018-12-25T06:14:25Z";"http://dx.doi.org/10.1093/bioinformatics/bty702";"Motivation: With the development of droplet based systems, massive single cell transcriptome data has become available, which enables analysis of cellular and molecular processes at single cell resolution and is instrumental to understanding many biological processes. While state-of-the-art clustering methods have been applied to the data, they face challenges in the following aspects: (1) the clustering quality still needs to be improved; (2) most models need prior knowledge on number of clusters, which is not always available; (3) there is a demand for faster computational speed. Results: We propose to tackle these challenges with Parallel Split Merge Sampling on Dirichlet Process Mixture Model (the Para-DPMM model). Unlike classic DPMM methods that perform sampling on each single data point, the split merge mechanism samples on the cluster level, which significantly improves convergence and optimality of the result. The model is highly parallelized and can utilize the computing power of high performance computing (HPC) clusters, enabling massive clustering on huge datasets. Experiment results show the model outperforms current widely used models in both clustering quality and computational speed. Availability: Source code is publicly available on https://github.com/tiehangd/Para_DPMM/tree/master/Para_DPMM_package";"Tiehang Duan, José P. Pinto, Xiaohui Xie";"0";"Arxiv"
"Hierarchical Latent Word Clustering";"2016-01-20T23:31:58Z";"http://arxiv.org/abs/1601.05472v1";"This paper presents a new Bayesian non-parametric model by extending the usage of Hierarchical Dirichlet Allocation to extract tree structured word clusters from text data. The inference algorithm of the model collects words in a cluster if they share similar distribution over documents. In our experiments, we observed meaningful hierarchical structures on NIPS corpus and radiology reports collected from public repositories.";"Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa";"0";"Arxiv"
"An efficient algorithm for solving elliptic problems on percolation   clusters";"2019-07-31T15:58:28Z";"http://arxiv.org/abs/1907.13571v1";"We present an efficient algorithm to solve elliptic Dirichlet problems defined on the cluster of $\mathbb{Z}^d$ supercritical Bernoulli percolation, as a generalization of the iterative method proposed by S. Armstrong, A. Hannukainen, T. Kuusi and J.-C. Mourrat. We also explore the two-scale expansion on the infinite cluster of percolation, and use it to give a rigorous analysis of the algorithm.";"Chenlin Gu";"0";"Arxiv"
"Colouring and breaking sticks: random distributions and heterogeneous   clustering";"2010-03-21T09:48:06Z";"http://arxiv.org/abs/1003.3988v1";"We begin by reviewing some probabilistic results about the Dirichlet Process and its close relatives, focussing on their implications for statistical modelling and analysis. We then introduce a class of simple mixture models in which clusters are of different `colours', with statistical characteristics that are constant within colours, but different between colours. Thus cluster identities are exchangeable only within colours. The basic form of our model is a variant on the familiar Dirichlet process, and we find that much of the standard modelling and computational machinery associated with the Dirichlet process may be readily adapted to our generalisation. The methodology is illustrated with an application to the partially-parametric clustering of gene expression profiles.";"Peter J. Green";"0";"Arxiv"
"Beta-Product Poisson-Dirichlet Processes";"2011-09-22T11:32:31Z";"http://arxiv.org/abs/1109.4777v1";"Time series data may exhibit clustering over time and, in a multiple time series context, the clustering behavior may differ across the series. This paper is motivated by the Bayesian non--parametric modeling of the dependence between the clustering structures and the distributions of different time series. We follow a Dirichlet process mixture approach and introduce a new class of multivariate dependent Dirichlet processes (DDP). The proposed DDP are represented in terms of vector of stick-breaking processes with dependent weights. The weights are beta random vectors that determine different and dependent clustering effects along the dimension of the DDP vector. We discuss some theoretical properties and provide an efficient Monte Carlo Markov Chain algorithm for posterior computation. The effectiveness of the method is illustrated with a simulation study and an application to the United States and the European Union industrial production indexes.";"Federico Bassetti, Roberto Casarin, Fabrizio Leisen";"0";"Arxiv"
"Conjoined Dirichlet Process";"2020-02-08T19:41:23Z";"http://arxiv.org/abs/2002.03223v1";"Biclustering is a class of techniques that simultaneously clusters the rows and columns of a matrix to sort heterogeneous data into homogeneous blocks. Although many algorithms have been proposed to find biclusters, existing methods suffer from the pre-specification of the number of biclusters or place constraints on the model structure. To address these issues, we develop a novel, non-parametric probabilistic biclustering method based on Dirichlet processes to identify biclusters with strong co-occurrence in both rows and columns. The proposed method utilizes dual Dirichlet process mixture models to learn row and column clusters, with the number of resulting clusters determined by the data rather than pre-specified. Probabilistic biclusters are identified by modeling the mutual dependence between the row and column clusters. We apply our method to two different applications, text mining and gene expression analysis, and demonstrate that our method improves bicluster extraction in many settings compared to existing approaches.";"Michelle N. Ngo, Dustin S. Pluta, Alexander N. Ngo, Babak Shahbaba";"0";"Arxiv"
"On the Variational Posterior of Dirichlet Process Deep Latent Gaussian   Mixture Models";"2020-06-16T08:46:18Z";"http://arxiv.org/abs/2006.08993v2";"Thanks to the reparameterization trick, deep latent Gaussian models have shown tremendous success recently in learning latent representations. The ability to couple them however with nonparamet-ric priors such as the Dirichlet Process (DP) hasn't seen similar success due to its non parameteriz-able nature. In this paper, we present an alternative treatment of the variational posterior of the Dirichlet Process Deep Latent Gaussian Mixture Model (DP-DLGMM), where we show that the prior cluster parameters and the variational posteriors of the beta distributions and cluster hidden variables can be updated in closed-form. This leads to a standard reparameterization trick on the Gaussian latent variables knowing the cluster assignments. We demonstrate our approach on standard benchmark datasets, we show that our model is capable of generating realistic samples for each cluster obtained, and manifests competitive performance in a semi-supervised setting.";"Amine Echraibi, Joachim Flocon-Cholet, Stéphane Gosselin, Sandrine Vaton";"0";"Arxiv"
"Powered Hawkes-Dirichlet Process: Challenging Textual Clustering using a   Flexible Temporal Prior";"2021-09-15T09:10:19Z";"http://arxiv.org/abs/2109.07170v1";"The textual content of a document and its publication date are intertwined. For example, the publication of a news article on a topic is influenced by previous publications on similar issues, according to underlying temporal dynamics. However, it can be challenging to retrieve meaningful information when textual information conveys little information or when temporal dynamics are hard to unveil. Furthermore, the textual content of a document is not always linked to its temporal dynamics. We develop a flexible method to create clusters of textual documents according to both their content and publication time, the Powered Dirichlet-Hawkes process (PDHP). We show PDHP yields significantly better results than state-of-the-art models when temporal information or textual content is weakly informative. The PDHP also alleviates the hypothesis that textual content and temporal dynamics are always perfectly correlated. PDHP allows retrieving textual clusters, temporal clusters, or a mixture of both with high accuracy when they are not. We demonstrate that PDHP generalizes previous work --such as the Dirichlet-Hawkes process (DHP) and Uniform process (UP). Finally, we illustrate the changes induced by PDHP over DHP and UP in a real-world application using Reddit data.";"Gaël Poux-Médard, Julien Velcin, Sabine Loudcher";"0";"Arxiv"
"Nonparametric Variable Selection, Clustering and Prediction for   High-Dimensional Regression";"2014-07-21T12:41:32Z";"http://arxiv.org/abs/1407.5472v3";"The development of parsimonious models for reliable inference and prediction of responses in high-dimensional regression settings is often challenging due to relatively small sample sizes and the presence of complex interaction patterns between a large number of covariates. We propose an efficient, nonparametric framework for simultaneous variable selection, clustering and prediction in high-throughput regression settings with continuous or discrete outcomes, called VariScan. The VariScan model utilizes the sparsity induced by Poisson-Dirichlet processes (PDPs) to group the covariates into lower-dimensional latent clusters consisting of covariates with similar patterns among the samples. The data are permitted to direct the choice of a suitable cluster allocation scheme, choosing between PDPs and their special case, a Dirichlet process. Subsequently, the latent clusters are used to build a nonlinear prediction model for the responses using an adaptive mixture of linear and nonlinear elements, thus achieving a balance between model parsimony and flexibility. We investigate theoretical properties of the VariScan procedure that differentiate the allocations patterns of PDPs and Dirichlet processes both in terms of the number and relative sizes of their clusters. Additional theoretical results guarantee the high accuracy of the model-based clustering procedure, and establish model selection and prediction consistency. Through simulation studies and analyses of benchmark data sets, we demonstrate the reliability of VariScan's clustering mechanism and show that the technique compares favorably to, and often outperforms, existing methodologies in terms of the prediction accuracies of the subject-specific responses.";"Subharup Guha, Veerabhadran Baladandayuthapani";"0";"Arxiv"
"Invariant Percolation and Harmonic Dirichlet Functions";"2004-05-24T16:55:27Z";"http://arxiv.org/abs/math/0405458v3";"The main goal of this paper is to answer question 1.10 and settle conjecture 1.11 of Benjamini-Lyons-Schramm [BLS99] relating harmonic Dirichlet functions on a graph to those of the infinite clusters in the uniqueness phase of Bernoulli percolation. We extend the result to more general invariant percolations, including the Random-Cluster model. We prove the existence of the nonuniqueness phase for the Bernoulli percolation (and make some progress for Random-Cluster model) on unimodular transitive locally finite graphs admitting nonconstant harmonic Dirichlet functions. This is done by using the device of $\ell^2$ Betti numbers.";"Damien Gaboriau";"0";"Arxiv"
"Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts";"2014-01-09T12:08:07Z";"http://arxiv.org/abs/1401.1974v4";"We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.";"Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui";"0";"Arxiv"
"Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick   Breaking Representation";"2012-06-27T16:21:35Z";"http://arxiv.org/abs/1206.6845v1";"Nonparametric Bayesian approaches to clustering, information retrieval, language modeling and object recognition have recently shown great promise as a new paradigm for unsupervised data analysis. Most contributions have focused on the Dirichlet process mixture models or extensions thereof for which efficient Gibbs samplers exist. In this paper we explore Gibbs samplers for infinite complexity mixture models in the stick breaking representation. The advantage of this representation is improved modeling flexibility. For instance, one can design the prior distribution over cluster sizes or couple multiple infinite mixture models (e.g. over time) at the level of their parameters (i.e. the dependent Dirichlet process model). However, Gibbs samplers for infinite mixture models (as recently introduced in the statistics literature) seem to mix poorly over cluster labels. Among others issues, this can have the adverse effect that labels for the same cluster in coupled mixture models are mixed up. We introduce additional moves in these samplers to improve mixing over cluster labels and to bring clusters into correspondence. An application to modeling of storm trajectories is used to illustrate these ideas.";"Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling";"0";"Arxiv"
"Functional clustering in nested designs: Modeling variability in   reproductive epidemiology studies";"2014-11-20T11:43:28Z";"http://dx.doi.org/10.1214/14-AOAS751";"We discuss functional clustering procedures for nested designs, where multiple curves are collected for each subject in the study. We start by considering the application of standard functional clustering tools to this problem, which leads to groupings based on the average profile for each subject. After discussing some of the shortcomings of this approach, we present a mixture model based on a generalization of the nested Dirichlet process that clusters subjects based on the distribution of their curves. By using mixtures of generalized Dirichlet processes, the model induces a much more flexible prior on the partition structure than other popular model-based clustering methods, allowing for different rates of introduction of new clusters as the number of observations increases. The methods are illustrated using hormone profiles from multiple menstrual cycles collected for women in the Early Pregnancy Study.";"Abel Rodriguez, David B. Dunson";"0";"Arxiv"
"Bayesian nonparametric temporal dynamic clustering via autoregressive   Dirichlet priors";"2019-10-23T10:15:26Z";"http://arxiv.org/abs/1910.10443v1";"In this paper we consider the problem of dynamic clustering, where cluster memberships may change over time and clusters may split and merge over time, thus creating new clusters and destroying existing ones. We propose a Bayesian nonparametric approach to dynamic clustering via mixture modeling. Our approach relies on a novel time-dependent nonparametric prior defined by combining: i) a copula-based transformation of a Gaussian autoregressive process; ii) the stick-breaking construction of the Dirichlet process. Posterior inference is performed through a particle Markov chain Monte Carlo algorithm which is simple, computationally efficient and scalable to massive datasets. Advantages of the proposed approach include flexibility in applications, ease of computations and interpretability. We present an application of our dynamic Bayesian nonparametric mixture model to the study the temporal dynamics of gender stereotypes in adjectives and occupations in the 20th and 21st centuries in the United States. Moreover, to highlight the flexibility of our model we present additional applications to time-dependent data with covariates and with spatial structure.";"Maria De Iorio, Stefano Favaro, Alessandra Guglielmi, Lifeng Ye";"0";"Arxiv"
"Posterior Distribution for the Number of Clusters in Dirichlet Process   Mixture Models";"2019-05-23T22:51:15Z";"http://arxiv.org/abs/1905.09959v2";"Dirichlet process mixture models (DPMM) play a central role in Bayesian nonparametrics, with applications throughout statistics and machine learning. DPMMs are generally used in clustering problems where the number of clusters is not known in advance, and the posterior distribution is treated as providing inference for this number. Recently, however, it has been shown that the DPMM is inconsistent in inferring the true number of components in certain cases. This is an asymptotic result, and it would be desirable to understand whether it holds with finite samples, and to more fully understand the full posterior. In this work, we provide a rigorous study for the posterior distribution of the number of clusters in DPMM under different prior distributions on the parameters and constraints on the distributions of the data. We provide novel lower bounds on the ratios of probabilities between $s+1$ clusters and $s$ clusters when the prior distributions on parameters are chosen to be Gaussian or uniform distributions.";"Chiao-Yu Yang, Eric Xia, Nhat Ho, Michael I. Jordan";"0";"Arxiv"
"Topic Detection from Conversational Dialogue Corpus with Parallel   Dirichlet Allocation Model and Elbow Method";"2020-06-05T10:24:43Z";"http://arxiv.org/abs/2006.03353v1";"A conversational system needs to know how to switch between topics to continue the conversation for a more extended period. For this topic detection from dialogue corpus has become an important task for a conversation and accurate prediction of conversation topics is important for creating coherent and engaging dialogue systems. In this paper, we proposed a topic detection approach with Parallel Latent Dirichlet Allocation (PLDA) Model by clustering a vocabulary of known similar words based on TF-IDF scores and Bag of Words (BOW) technique. In the experiment, we use K-mean clustering with Elbow Method for interpretation and validation of consistency within-cluster analysis to select the optimal number of clusters. We evaluate our approach by comparing it with traditional LDA and clustering technique. The experimental results show that combining PLDA with Elbow method selects the optimal number of clusters and refine the topics for the conversation.";"Haider Khalid, Vincent Wade";"0";"Arxiv"
"Joint Clustering and Registration of Functional Data";"2014-03-27T17:17:25Z";"http://arxiv.org/abs/1403.7134v1";"Curve registration and clustering are fundamental tools in the analysis of functional data. While several methods have been developed and explored for either task individually, limited work has been done to infer functional clusters and register curves simultaneously. We propose a hierarchical model for joint curve clustering and registration. Our proposal combines a Dirichlet process mixture model for clustering of common shapes, with a reproducing kernel representation of phase variability for registration. We show how inference can be carried out applying standard posterior simulation algorithms and compare our method to several alternatives in both engineered data and a benchmark analysis of the Berkeley growth data. We conclude our investigation with an application to time course gene expression.";"Yafeng Zhang, Donatello Telesca";"0";"Arxiv"
"Heterogeneous Regression Models for Clusters of Spatial Dependent Data";"2019-07-04T04:13:36Z";"http://dx.doi.org/10.1080/17421772.2020.1784989";"In economic development, there are often regions that share similar economic characteristics, and economic models on such regions tend to have similar covariate effects. In this paper, we propose a Bayesian clustered regression for spatially dependent data in order to detect clusters in the covariate effects. Our proposed method is based on the Dirichlet process which provides a probabilistic framework for simultaneous inference of the number of clusters and the clustering configurations. The usage of our method is illustrated both in simulation studies and an application to a housing cost dataset of Georgia.";"Zhihua Ma, Yishu Xue, Guanyu Hu";"0";"Arxiv"
"Dirichlet Fragmentation Processes";"2015-09-16T01:07:24Z";"http://arxiv.org/abs/1509.04781v1";"Tree structures are ubiquitous in data across many domains, and many datasets are naturally modelled by unobserved tree structures. In this paper, first we review the theory of random fragmentation processes [Bertoin, 2006], and a number of existing methods for modelling trees, including the popular nested Chinese restaurant process (nCRP). Then we define a general class of probability distributions over trees: the Dirichlet fragmentation process (DFP) through a novel combination of the theory of Dirichlet processes and random fragmentation processes. This DFP presents a stick-breaking construction, and relates to the nCRP in the same way the Dirichlet process relates to the Chinese restaurant process. Furthermore, we develop a novel hierarchical mixture model with the DFP, and empirically compare the new model to similar models in machine learning. Experiments show the DFP mixture model to be convincingly better than existing state-of-the-art approaches for hierarchical clustering and density modelling.";"Hong Ge, Yarin Gal, Zoubin Ghahramani";"0";"Arxiv"
"The semi-hierarchical Dirichlet Process and its application to   clustering homogeneous distributions";"2020-05-20T18:10:13Z";"http://arxiv.org/abs/2005.10287v4";"Assessing homogeneity of distributions is an old problem that has received considerable attention, especially in the nonparametric Bayesian literature. To this effect, we propose the semi-hierarchical Dirichlet process, a novel hierarchical prior that extends the hierarchical Dirichlet process of Teh et al. (2006) and that avoids the degeneracy issues of nested processes recently described by Camerlenghi et al. (2019a). We go beyond the simple yes/no answer to the homogeneity question and embed the proposed prior in a random partition model; this procedure allows us to give a more comprehensive response to the above question and in fact find groups of populations that are internally homogeneous when I greater or equal than 2 such populations are considered. We study theoretical properties of the semi-hierarchical Dirichlet process and of the Bayes factor for the homogeneity test when I = 2. Extensive simulation studies and applications to educational data are also discussed.";"Mario Beraha, Alessandra Guglielmi, Fernando A. Quintana";"0";"Arxiv"
"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence   Clustering";"2017-01-31T18:42:19Z";"http://arxiv.org/abs/1701.09177v5";"We propose an effective method to solve the event sequence clustering problems based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process. In this model, each event sequence belonging to a cluster is generated via the same Hawkes process with specific parameters, and different clusters correspond to different Hawkes processes. The prior distribution of the Hawkes processes is controlled via a Dirichlet distribution. We learn the model via a maximum likelihood estimator (MLE) and propose an effective variational Bayesian inference algorithm. We specifically analyze the resulting EM-type algorithm in the context of inner-outer iterations and discuss several inner iteration allocation strategies. The identifiability of our model, the convergence of our learning method, and its sample complexity are analyzed in both theoretical and empirical ways, which demonstrate the superiority of our method to other competitors. The proposed method learns the number of clusters automatically and is robust to model misspecification. Experiments on both synthetic and real-world data show that our method can learn diverse triggering patterns hidden in asynchronous event sequences and achieve encouraging performance on clustering purity and consistency.";"Hongteng Xu, Hongyuan Zha";"0";"Arxiv"
"Predictive Hierarchical Clustering: Learning clusters of CPT codes for   improving surgical outcomes";"2016-04-24T13:49:23Z";"http://arxiv.org/abs/1604.07031v2";"We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for agglomerative hierarchical clustering of current procedural terminology (CPT) codes. Our predictive hierarchical clustering aims to cluster subgroups, not individual observations, found within our data, such that the clusters discovered result in optimal performance of a classification model. Therefore, merges are chosen based on a Bayesian hypothesis test, which chooses pairings of the subgroups that result in the best model fit, as measured by held out predictive likelihoods. We place a Dirichlet prior on the probability of merging clusters, allowing us to adjust the size and sparsity of clusters. The motivation is to predict patient-specific surgical outcomes using data from ACS NSQIP (American College of Surgeon's National Surgical Quality Improvement Program). An important predictor of surgical outcomes is the actual surgical procedure performed as described by a CPT code. We use PHC to cluster CPT codes, represented as subgroups, together in a way that enables us to better predict patient-specific outcomes compared to currently used clusters based on clinical judgment.";"Elizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, Katherine Heller";"0";"Arxiv"
"Sharp L^p bounds on spectral clusters for Lipschitz metrics";"2012-07-10T17:22:01Z";"http://arxiv.org/abs/1207.2417v1";"We establish L^p bounds on L^2 normalized spectral clusters for self-adjoint elliptic Dirichlet forms with Lipschitz coefficients. In two dimensions we obtain best possible bounds for all p between $2 and infinity, up to logarithmic losses for $6<p\leq 8$. In higher dimensions we obtain best possible bounds for a limited range of p.";"Herbert Koch, Hart Smith, Daniel Tataru";"0";"Arxiv"
"Robust Bayesian inference of networks using Dirichlet t-distributions";"2012-07-05T11:07:27Z";"http://arxiv.org/abs/1207.1221v1";"Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. These models have been used extensively in applications to gene expression data, even in cases where there appears to be significant deviations from the Gaussian model. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. Dirichlet processes are commonly used to cluster independent observations; here they are used instead to cluster the dependent components of a single observation. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties.";"Michael Finegold, Mathias Drton";"0";"Arxiv"
"ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process   Mixtures";"2013-04-08T18:34:32Z";"http://arxiv.org/abs/1304.2302v1";"The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions.";"Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka";"0";"Arxiv"
"An Alternative Prior Process for Nonparametric Bayesian Clustering";"2008-01-03T01:10:20Z";"http://arxiv.org/abs/0801.0461v2";"Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit ""rich-get-richer"" characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering -- the uniform process -- for applications where the ""rich-get-richer"" property is undesirable. We also explore the cost of this process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. We compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.";"Hanna M. Wallach, Shane T. Jensen, Lee Dicker, Katherine A. Heller";"0";"Arxiv"
"Sparse Bayesian Hierarchical Modeling of High-dimensional Clustering   Problems";"2009-04-19T11:33:01Z";"http://arxiv.org/abs/0904.2906v1";"Clustering is one of the most widely used procedures in the analysis of microarray data, for example with the goal of discovering cancer subtypes based on observed heterogeneity of genetic marks between different tissues. It is well-known that in such high-dimensional settings, the existence of many noise variables can overwhelm the few signals embedded in the high-dimensional space. We propose a novel Bayesian approach based on Dirichlet process with a sparsity prior that simultaneous performs variable selection and clustering, and also discover variables that only distinguish a subset of the cluster components. Unlike previous Bayesian formulations, we use Dirichlet process (DP) for both clustering of samples as well as for regularizing the high-dimensional mean/variance structure. To solve the computational challenge brought by this double usage of DP, we propose to make use of a sequential sampling scheme embedded within Markov chain Monte Carlo (MCMC) updates to improve the naive implementation of existing algorithms for DP mixture models. Our method is demonstrated on a simulation study and illustrated with the leukemia gene expression dataset.";"Heng Lian";"0";"Arxiv"
"Infinite mixtures of multivariate normal-inverse Gaussian distributions   for clustering of skewed data";"2020-05-11T17:08:27Z";"http://arxiv.org/abs/2005.05324v1";"Mixtures of multivariate normal inverse Gaussian (MNIG) distributions can be used to cluster data that exhibit features such as skewness and heavy tails. However, for cluster analysis, using a traditional finite mixture model framework, either the number of components needs to be known $a$-$priori$ or needs to be estimated $a$-$posteriori$ using some model selection criterion after deriving results for a range of possible number of components. However, different model selection criteria can sometimes result in different number of components yielding uncertainty. Here, an infinite mixture model framework, also known as Dirichlet process mixture model, is proposed for the mixtures of MNIG distributions. This Dirichlet process mixture model approach allows the number of components to grow or decay freely from 1 to $\infty$ (in practice from 1 to $N$) and the number of components is inferred along with the parameter estimates in a Bayesian framework thus alleviating the need for model selection criteria. We provide real data applications with benchmark datasets as well as a small simulation experiment to compare with other existing models. The proposed method provides competitive clustering results to other clustering approaches for both simulation and real data and parameter recovery are illustrated using simulation studies.";"Yuan Fang, Dimitris Karlis, Sanjeena Subedi";"0";"Arxiv"
"Flexible Bayesian Product Mixture Models for Vector Autoregressions";"2021-11-16T19:31:37Z";"http://arxiv.org/abs/2111.08743v2";"Bayesian non-parametric methods based on Dirichlet process mixtures have seen tremendous success in various domains and are appealing in being able to borrow information by clustering samples that share identical parameters. However, such methods can face hurdles in heterogeneous settings where objects are expected to cluster only along a subset of axes or where clusters of samples share only a subset of identical parameters. We overcome such limitations by developing a novel class of product of Dirichlet process location-scale mixtures that enable independent clustering at multiple scales, which result in varying levels of information sharing across samples. First, we develop the approach for independent multivariate data. Subsequently we generalize it to multivariate time-series data under the framework of multi-subject Vector Autoregressive (VAR) models that is our primary focus, which go beyond parametric single-subject VAR models. We establish posterior consistency and develop efficient posterior computation for implementation. Extensive numerical studies involving VAR models show distinct advantages over competing methods, in terms of estimation, clustering, and feature selection accuracy. Our resting state fMRI analysis from the Human Connectome Project reveals biologically interpretable connectivity differences between distinct intelligence groups, while another air pollution application illustrates the superior forecasting accuracy compared to alternate methods.";"Suprateek Kundu, Joshua Lukemire";"0";"Arxiv"
"Adaptive Low-Complexity Sequential Inference for Dirichlet Process   Mixture Models";"2014-09-29T16:47:44Z";"http://arxiv.org/abs/1409.8185v3";"We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.";"Theodoros Tsiligkaridis, Keith W. Forsythe";"0";"Arxiv"
"Outcome identification in electronic health records using predictions   from an enriched Dirichlet process mixture";"2018-06-06T20:26:03Z";"http://arxiv.org/abs/1806.02411v1";"We propose a novel semiparametric model for the joint distribution of a continuous longitudinal outcome and the baseline covariates using an enriched Dirichlet process (EDP) prior. This joint model decomposes into a linear mixed model for the outcome given the covariates and marginals for the covariates. The nonparametric EDP prior is placed on the regression and spline coefficients, the error variance, and the parameters governing the predictor space. We predict the outcome at unobserved time points for subjects with data at other time points as well as for new subjects with only baseline covariates. We find improved prediction over mixed models with Dirichlet process (DP) priors when there are a large number of covariates. Our method is demonstrated with electronic health records consisting of initiators of second generation antipsychotic medications, which are known to increase the risk of diabetes. We use our model to predict laboratory values indicative of diabetes for each individual and assess incidence of suspected diabetes from the predicted dataset. Our model also serves as a functional clustering algorithm in which subjects are clustered into groups with similar longitudinal trajectories of the outcome over time.";"Bret Zeldow, James Flory, Alisa Stephens-Shields, Marsha Raebel, Jason Roy";"0";"Arxiv"
"The Greedy Dirichlet Process Filter - An Online Clustering Multi-Target   Tracker";"2018-11-14T17:09:22Z";"http://dx.doi.org/10.1109/GlobalSIP.2018.8646554";"Reliable collision avoidance is one of the main requirements for autonomous driving. Hence, it is important to correctly estimate the states of an unknown number of static and dynamic objects in real-time. Here, data association is a major challenge for every multi-target tracker. We propose a novel multi-target tracker called Greedy Dirichlet Process Filter (GDPF) based on the non-parametric Bayesian model called Dirichlet Processes and the fast posterior computation algorithm Sequential Updating and Greedy Search (SUGS). By adding a temporal dependence we get a real-time capable tracking framework without the need of a previous clustering or data association step. Real-world tests show that GDPF outperforms other multi-target tracker in terms of accuracy and stability.";"Benjamin Naujoks, Patrick Burger, Hans-Joachim Wuensche";"0";"Arxiv"
"A Dirichlet Process Mixture Model for Directional-Linear Data";"2022-12-21T00:47:24Z";"http://arxiv.org/abs/2212.10704v1";"Directional data require specialized probability models because of the non-Euclidean and periodic nature of their domain. When a directional variable is observed jointly with linear variables, modeling their dependence adds an additional layer of complexity. This paper introduces a novel Bayesian nonparametric approach for directional-linear data based on the Dirichlet process. We first extend the projected normal distribution to model the joint distribution of linear variables and a directional variable with arbitrary dimension as a projection of a higher-dimensional augmented multivariate normal distribution (MVN). We call the new distribution the semi-projected normal distribution (SPN); it possesses properties similar to the MVN. The SPN is then used as the mixture distribution in a Dirichlet process model to obtain a more flexible class of models for directional-linear data. We propose a normal conditional inverse-Wishart distribution as part of the prior distribution to address an identifiability issue inherited from the projected normal and preserve conjugacy with the SPN distribution. A Gibbs sampling algorithm is provided for posterior inference. Experiments on synthetic data and the Berkeley image database show superior performance of the Dirichlet process SPN mixture model (DPSPN) in clustering compared to other directional-linear models. We also build a hierarchical Dirichlet process model with the SPN to develop a likelihood ratio approach to bloodstain pattern analysis using the DPSPN model for density estimation to estimate the likelihood of a given pattern from a set of training data.";"Tong Zou, Hal S. Stern";"0";"Arxiv"
"Semiparametric clustered overdispersed multinomial goodness-of-fit of   log-linear models";"2016-09-23T12:03:27Z";"http://arxiv.org/abs/1609.07330v1";"Traditionally, the Dirichlet-multinomial distribution has been recognized as a key model for contingency tables generated by cluster sampling schemes. There are, however, other possible distributions appropriate for these contingency tables. This paper introduces new test-statistics capable to test log-linear modeling hypotheses with no distributional specification, when the individuals of the clusters are possibly homogeneously correlated. The estimator for the intracluster correlation coefficient proposed in Alonso-Revenga et al. (2016), valid for different cluster sizes, plays a crucial role in the construction of the goodness-of-fit test-statistic.";"Juana M. Alonso-Revenga, Nirian Martin, Leandro Pardo";"0";"Arxiv"
"Reducing over-clustering via the powered Chinese restaurant process";"2018-02-15T02:53:30Z";"http://arxiv.org/abs/1802.05392v1";"Dirichlet process mixture (DPM) models tend to produce many small clusters regardless of whether they are needed to accurately characterize the data - this is particularly true for large data sets. However, interpretability, parsimony, data storage and communication costs all are hampered by having overly many clusters. We propose a powered Chinese restaurant process to limit this kind of problem and penalize over clustering. The method is illustrated using some simulation examples and data with large and small sample size including MNIST and the Old Faithful Geyser data.";"Jun Lu, Meng Li, David Dunson";"0";"Arxiv"
"Amortized Bayesian inference for clustering models";"2018-11-24T02:17:20Z";"http://arxiv.org/abs/1811.09747v1";"We develop methods for efficient amortized approximate Bayesian inference over posterior distributions of probabilistic clustering models, such as Dirichlet process mixture models. The approach is based on mapping distributed, symmetry-invariant representations of cluster arrangements into conditional probabilities. The method parallelizes easily, yields iid samples from the approximate posterior of cluster assignments with the same computational cost of a single Gibbs sampler sweep, and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model.";"Ari Pakman, Liam Paninski";"0";"Arxiv"
"An enriched mixture model for functional clustering";"2019-07-04T17:09:04Z";"http://arxiv.org/abs/1907.02493v1";"There is an increasingly rich literature about Bayesian nonparametric models for clustering functional observations. However, most of the recent proposals rely on infinite-dimensional characterizations that might lead to overly complex cluster solutions. In addition, while prior knowledge about the functional shapes is typically available, its practical exploitation might be a difficult modeling task. Motivated by an application in e-commerce, we propose a novel enriched Dirichlet mixture model for functional data. Our proposal accommodates the incorporation of functional constraints while bounding the model complexity. To clarify the underlying partition mechanism, we characterize the prior process through a P\'olya urn scheme. These features lead to a very interpretable clustering method compared to available techniques. To overcome computational bottlenecks, we employ a variational Bayes approximation for tractable posterior inference.";"Tommaso Rigon";"0";"Arxiv"
"Bayesian clustering of replicated time-course gene expression data with   weak signals";"2012-10-18T05:53:52Z";"http://dx.doi.org/10.1214/13-AOAS650";"To identify novel dynamic patterns of gene expression, we develop a statistical method to cluster noisy measurements of gene expression collected from multiple replicates at multiple time points, with an unknown number of clusters. We propose a random-effects mixture model coupled with a Dirichlet-process prior for clustering. The mixture model formulation allows for probabilistic cluster assignments. The random-effects formulation allows for attributing the total variability in the data to the sources that are consistent with the experimental design, particularly when the noise level is high and the temporal dependence is not strong. The Dirichlet-process prior induces a prior distribution on partitions and helps to estimate the number of clusters (or mixture components) from the data. We further tackle two challenges associated with Dirichlet-process prior-based methods. One is efficient sampling. We develop a novel Metropolis-Hastings Markov Chain Monte Carlo (MCMC) procedure to sample the partitions. The other is efficient use of the MCMC samples in forming clusters. We propose a two-step procedure for posterior inference, which involves resampling and relabeling, to estimate the posterior allocation probability matrix. This matrix can be directly used in cluster assignments, while describing the uncertainty in clustering. We demonstrate the effectiveness of our model and sampling procedure through simulated data. Applying our method to a real data set collected from Drosophila adult muscle cells after five-minute Notch activation, we identify 14 clusters of different transcriptional responses among 163 differentially expressed genes, which provides novel insights into underlying transcriptional mechanisms in the Notch signaling pathway. The algorithm developed here is implemented in the R package DIRECT, available on CRAN.";"Audrey Qiuyan Fu, Steven Russell, Sarah J. Bray, Simon Tavaré";"0";"Arxiv"
"Fast search for Dirichlet process mixture models";"2009-07-10T13:23:37Z";"http://arxiv.org/abs/0907.1812v1";"Dirichlet process (DP) mixture models provide a flexible Bayesian framework for density estimation. Unfortunately, their flexibility comes at a cost: inference in DP mixture models is computationally expensive, even when conjugate distributions are used. In the common case when one seeks only a maximum a posteriori assignment of data points to clusters, we show that search algorithms provide a practical alternative to expensive MCMC and variational techniques. When a true posterior sample is desired, the solution found by search can serve as a good initializer for MCMC. Experimental results show that using these techniques is it possible to apply DP mixture models to very large data sets.";"Hal Daumé III";"0";"Arxiv"
"Generalized Polya Urn for Time-varying Dirichlet Process Mixtures";"2012-06-20T14:57:41Z";"http://arxiv.org/abs/1206.5254v1";"Dirichlet Process Mixtures (DPMs) are a popular class of statistical models to perform density estimation and clustering. However, when the data available have a distribution evolving over time, such models are inadequate. We introduce here a class of time-varying DPMs which ensures that at each time step the random distribution follows a DPM model. Our model relies on an intuitive and simple generalized Polya urn scheme. Inference is performed using Markov chain Monte Carlo and Sequential Monte Carlo. We demonstrate our model on various applications.";"Francois Caron, Manuel Davy, Arnaud Doucet";"0";"Arxiv"
"Combining Random Walks and Nonparametric Bayesian Topic Model for   Community Detection";"2016-07-19T13:46:15Z";"http://arxiv.org/abs/1607.05573v2";"Community detection has been an active research area for decades. Among all probabilistic models, Stochastic Block Model has been the most popular one. This paper introduces a novel probabilistic model: RW-HDP, based on random walks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP, random walks conducted in a social network are treated as documents; nodes are treated as words. By using Hierarchical Dirichlet Process, a nonparametric Bayesian model, we are not only able to cluster nodes into different communities, but also determine the number of communities automatically. We use Stochastic Variational Inference for our model inference, which makes our method time efficient and can be easily extended to an online learning algorithm.";"Ruimin Zhu, Wenxin Jiang";"0";"Arxiv"
"Geometric Dirichlet Means algorithm for topic inference";"2016-10-27T23:35:57Z";"http://arxiv.org/abs/1610.09034v1";"We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.";"Mikhail Yurochkin, XuanLong Nguyen";"0";"Arxiv"
"The Dirichlet problem for Fully Nonlinear Equations Arising from   Conformal Geometry";"2018-01-16T07:26:12Z";"http://arxiv.org/abs/1801.05140v2";"We study the Dirichlet problem for a class of curvature equations arising from conformal geometry on Riemannian manifolds $(M^n, g)$ with boundary where $n \geq 3$. We prove there exists a unique solution using the continuity method which is based on \emph{a priori} estimates for admissible solutions. In deriving the estimates, a crucial step is to derive a lower bound for the gradient on the boundary. This is overcome by constructing a cluster of subsolutions.";"Weisong Dong";"0";"Arxiv"
"jLDADMM: A Java package for the LDA and DMM topic models";"2018-08-11T16:47:58Z";"http://arxiv.org/abs/1808.03835v1";"In this technical report, we present jLDADMM---an easy-to-use Java toolkit for conventional topic models. jLDADMM is released to provide alternatives for topic modeling on normal or short texts. It provides implementations of the Latent Dirichlet Allocation topic model and the one-topic-per-document Dirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed Gibbs sampling. In addition, jLDADMM supplies a document clustering evaluation to compare topic models. jLDADMM is open-source and available to download at: https://github.com/datquocnguyen/jLDADMM";"Dat Quoc Nguyen";"0";"Arxiv"
"Analysis of the maximal posterior partition in the Dirichlet Process   Gaussian Mixture Model";"2016-06-10T11:27:24Z";"http://arxiv.org/abs/1606.03275v3";"Mixture models are a natural choice in many applications, but it can be difficult to place an a priori upper bound on the number of components. To circumvent this, investigators are turning increasingly to Dirichlet process mixture models (DPMMs). It is therefore important to develop an understanding of the strengths and weaknesses of this approach. This work considers the MAP (maximum a posteriori) clustering for the Gaussian DPMM (where the cluster means have Gaussian distribution and, for each cluster, the observations within the cluster have Gaussian distribution). Some desirable properties of the MAP partition are proved: `almost disjointness' of the convex hulls of clusters (they may have at most one point in common) and (with natural assumptions) the comparability of sizes of those clusters that intersect any fixed ball with the number of observations (as the latter goes to infinity). Consequently, the number of such clusters remains bounded. Furthermore, if the data arises from independent identically distributed sampling from a given distribution with bounded support then the asymptotic MAP partition of the observation space maximises a function which has a straightforward expression, which depends only on the within-group covariance parameter. As the operator norm of this covariance parameter decreases, the number of clusters in the MAP partition becomes arbitrarily large, which may lead to the overestimation of the number of mixture components.";"Łukasz Rajkowski";"0";"Arxiv"
"Determinantal Clustering Processes - A Nonparametric Bayesian Approach   to Kernel Based Semi-Supervised Clustering";"2013-09-26T12:50:04Z";"http://arxiv.org/abs/1309.6862v1";"Semi-supervised clustering is the task of clustering data points into clusters where only a fraction of the points are labelled. The true number of clusters in the data is often unknown and most models require this parameter as an input. Dirichlet process mixture models are appealing as they can infer the number of clusters from the data. However, these models do not deal with high dimensional data well and can encounter difficulties in inference. We present a novel nonparameteric Bayesian kernel based method to cluster data points without the need to prespecify the number of clusters or to model complicated densities from which data points are assumed to be generated from. The key insight is to use determinants of submatrices of a kernel matrix as a measure of how close together a set of points are. We explore some theoretical properties of the model and derive a natural Gibbs based algorithm with MCMC hyperparameter learning. The model is implemented on a variety of synthetic and real world data sets.";"Amar Shah, Zoubin Ghahramani";"0";"Arxiv"
"Cohomology of cluster varieties. I. Locally acyclic case";"2016-04-23T01:22:46Z";"http://dx.doi.org/10.2140/ant.2022.16.179";"We initiate a systematic study of the cohomology of cluster varieties. We introduce the Louise property for cluster algebras that holds for all acyclic cluster algebras, and for most cluster algebras arising from marked surfaces. For cluster varieties satisfying the Louise property and of full rank, we show that the cohomology satisfies the curious Lefschetz property of Hausel and Rodriguez-Villegas, and that the mixed Hodge structure is split over the rationals. We give a complete description of the highest weight part of the mixed Hodge structure of these cluster varieties, and develop the notion of a standard differential form on a cluster variety. We show that the point counts of these cluster varieties over finite fields can be expressed in terms of Dirichlet characters. Under an additional integrality hypothesis, the point counts are shown to be polynomials in the order of the finite field.";"Thomas Lam, David E. Speyer";"0";"Arxiv"
"A Random Finite Set Model for Data Clustering";"2017-03-14T23:35:57Z";"http://arxiv.org/abs/1703.04832v1";"The goal of data clustering is to partition data points into groups to minimize a given objective function. While most existing clustering algorithms treat each data point as vector, in many applications each datum is not a vector but a point pattern or a set of points. Moreover, many existing clustering methods require the user to specify the number of clusters, which is not available in advance. This paper proposes a new class of models for data clustering that addresses set-valued data as well as unknown number of clusters, using a Dirichlet Process mixture of Poisson random finite sets. We also develop an efficient Markov Chain Monte Carlo posterior inference technique that can learn the number of clusters and mixture parameters automatically from the data. Numerical studies are presented to demonstrate the salient features of this new model, in particular its capacity to discover extremely unbalanced clusters in data.";"Dinh Phung, Ba-Ngu Bo";"0";"Arxiv"
"A Bayesian Model for Supervised Clustering with the Dirichlet Process   Prior";"2009-07-04T22:32:58Z";"http://arxiv.org/abs/0907.0808v1";"We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to define distributions over the countably infinite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these ""reference types"") that are generic across all clusters. Inference in our framework, which requires integrating over infinitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple--but general--parameterization of our model based on a Gaussian assumption. We evaluate this model on one artificial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics.";"Hal Daumé III, Daniel Marcu";"0";"Arxiv"
"Dirichlet process approach for radio-based simultaneous localization and   mapping";"2021-07-02T06:43:59Z";"http://arxiv.org/abs/2107.00864v1";"Due to 5G millimeter wave (mmWave), spatial channel parameters are becoming highly resolvable, enabling accurate vehicle localization and mapping. We propose a novel method of radio simultaneous localization and mapping (SLAM) with the Dirichlet process (DP). The DP, which can estimate the number of clusters as well as clustering, is capable of identifying the locations of reflectors by classifying signals when such 5G signals are reflected and received from various objects. We generate birth points using the measurements from 5G mmWave signals received by the vehicle and classify objects by clustering birth points generated over time. Each time we use the DP clustering method, we can map landmarks in the environment in challenging situations where false alarms exist in the measurements and change the cardinality of received signals. Simulation results demonstrate the performance of the proposed scheme. By comparing the results with the SLAM based on the Rao-Blackwellized probability hypothesis density filter, we confirm a slight drop in SLAM performance, but as a result, we validate that it has a significant gain in computational complexity.";"Jaebok Lee, Hyowon Kim, Henk Wymeersch, Sunwoo Kim";"0";"Arxiv"
"Sampling in Dirichlet Process Mixture Models for Clustering Streaming   Data";"2022-02-27T08:51:50Z";"http://arxiv.org/abs/2202.13312v1";"Practical tools for clustering streaming data must be fast enough to handle the arrival rate of the observations. Typically, they also must adapt on the fly to possible lack of stationarity; i.e., the data statistics may be time-dependent due to various forms of drifts, changes in the number of clusters, etc. The Dirichlet Process Mixture Model (DPMM), whose Bayesian nonparametric nature allows it to adapt its complexity to the data, seems a natural choice for the streaming-data case. In its classical formulation, however, the DPMM cannot capture common types of drifts in the data statistics. Moreover, and regardless of that limitation, existing methods for online DPMM inference are too slow to handle rapid data streams. In this work we propose adapting both the DPMM and a known DPMM sampling-based non-streaming inference method for streaming-data clustering. We demonstrate the utility of the proposed method on several challenging settings, where it obtains state-of-the-art results while being on par with other methods in terms of speed.";"Or Dinari, Oren Freifeld";"0";"Arxiv"
"Consistent Model-based Clustering: using the Quasi-Bernoulli   Stick-Breaking Process";"2020-08-23T01:13:33Z";"http://arxiv.org/abs/2008.09938v3";"In mixture modeling and clustering applications, the number of components and clusters is often not known. A stick-breaking mixture model, such as the Dirichlet process mixture model, is an appealing construction that assumes infinitely many components, while shrinking the weights of most of the unused components to near zero. However, it is well-known that this shrinkage is inadequate: even when the component distribution is correctly specified, spurious weights appear and give an inconsistent estimate of the number of clusters. In this article, we propose a simple solution: when breaking each mixture weight stick into two pieces, the length of the second piece is multiplied by a quasi-Bernoulli random variable, taking value one or a small constant close to zero. This effectively creates a soft-truncation and further shrinks the unused weights. Asymptotically, we show that as long as this small constant diminishes to zero fast enough, the posterior distribution will converge to the true number of clusters. In comparison, we rigorously explore Dirichlet process mixture models using a concentration parameter that is either constant or rapidly diminishes to zero -- both of which lead to inconsistency for the number of clusters. Our proposed model is easy to implement, requiring only a small modification of a standard Gibbs sampler for mixture models. Empirically, the proposed method exhibits superior performance in simulations and a data application in clustering brain networks.";"Cheng Zeng, Jeffrey W. Miller, Leo L. Duan";"0";"Arxiv"
"Probabilistic Clustering of Time-Evolving Distance Data";"2015-04-14T20:05:45Z";"http://arxiv.org/abs/1504.03701v1";"We present a novel probabilistic clustering model for objects that are represented via pairwise distances and observed at different time points. The proposed method utilizes the information given by adjacent time points to find the underlying cluster structure and obtain a smooth cluster evolution. This approach allows the number of objects and clusters to differ at every time point, and no identification on the identities of the objects is needed. Further, the model does not require the number of clusters being specified in advance -- they are instead determined automatically using a Dirichlet process prior. We validate our model on synthetic data showing that the proposed method is more accurate than state-of-the-art clustering methods. Finally, we use our dynamic clustering model to analyze and illustrate the evolution of brain cancer patients over time.";"Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya Prabhakaran, Volker Roth, Gunnar Rätsch";"0";"Arxiv"
"Bayesian Nonparametric Graph Clustering";"2015-09-24T20:52:05Z";"http://arxiv.org/abs/1509.07535v1";"We present clustering methods for multivariate data exploiting the underlying geometry of the graphical structure between variables. As opposed to standard approaches that assume known graph structures, we first estimate the edge structure of the unknown graph using Bayesian neighborhood selection approaches, wherein we account for the uncertainty of graphical structure learning through model-averaged estimates of the suitable parameters. Subsequently, we develop a nonparametric graph clustering model on the lower dimensional projections of the graph based on Laplacian embeddings using Dirichlet process mixture models. In contrast to standard algorithmic approaches, this fully probabilistic approach allows incorporation of uncertainty in estimation and inference for both graph structure learning and clustering. More importantly, we formalize the arguments for Laplacian embeddings as suitable projections for graph clustering by providing theoretical support for the consistency of the eigenspace of the estimated graph Laplacians. We develop fast computational algorithms that allow our methods to scale to large number of nodes. Through extensive simulations we compare our clustering performance with standard clustering methods. We apply our methods to a novel pan-cancer proteomic data set, and evaluate protein networks and clusters across multiple different cancer types.";"Sayantan Banerjee, Rehan Akbani, Veerabhadran Baladandayuthapani";"0";"Arxiv"
"Microclustering: When the Cluster Sizes Grow Sublinearly with the Size   of the Data Set";"2015-12-02T18:08:48Z";"http://arxiv.org/abs/1512.00792v1";"Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some tasks, this assumption is undesirable. For example, when performing entity resolution, the size of each cluster is often unrelated to the size of the data set. Consequently, each cluster contains a negligible fraction of the total number of data points. Such tasks therefore require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the \emph{microclustering property} and introducing a new model that exhibits this property. We compare this model to several commonly used clustering models by checking model fit using real and simulated data sets.";"Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, Rebecca C. Steorts";"0";"Arxiv"
"A Nonparametric Bayesian Technique for High-Dimensional Regression";"2016-04-12T23:24:00Z";"http://dx.doi.org/10.1214/16-EJS1184";"This paper proposes a nonparametric Bayesian framework called VariScan for simultaneous clustering, variable selection, and prediction in high-throughput regression settings. Poisson-Dirichlet processes are utilized to detect lower-dimensional latent clusters of covariates. An adaptive nonlinear prediction model is constructed for the response, achieving a balance between model parsimony and flexibility. Contrary to conventional belief, cluster detection is shown to be aposteriori consistent for a general class of models as the number of covariates and subjects grows. Simulation studies and data analyses demonstrate that VariScan often outperforms several well-known statistical methods.";"Subharup Guha, Veerabhadran Baladandayuthapani";"0";"Arxiv"
"Simple approximate MAP Inference for Dirichlet processes";"2014-11-04T15:30:25Z";"http://arxiv.org/abs/1411.0939v1";"The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesian nonparametric statistical model. However, full probabilistic inference in this model is analytically intractable, so that computationally intensive techniques such as Gibb's sampling are required. As a result, DPM-based methods, which have considerable potential, are restricted to applications in which computational resources and time for inference is plentiful. For example, they would not be practical for digital signal processing on embedded hardware, where computational resources are at a serious premium. Here, we develop simplified yet statistically rigorous approximate maximum a-posteriori (MAP) inference algorithms for DPMs. This algorithm is as simple as K-means clustering, performs in experiments as well as Gibb's sampling, while requiring only a fraction of the computational effort. Unlike related small variance asymptotics, our algorithm is non-degenerate and so inherits the ""rich get richer"" property of the Dirichlet process. It also retains a non-degenerate closed-form likelihood which enables standard tools such as cross-validation to be used. This is a well-posed approximation to the MAP solution of the probabilistic DPM model.";"Yordan P. Raykov, Alexis Boukouvalas, Max A. Little";"0";"Arxiv"
"A Jentzsch-Theorem for Kapteyn, Neumann, and General Dirichlet Series";"2021-07-15T09:30:48Z";"http://dx.doi.org/10.1007/s40315-022-00468-y";"Comparing phase plots of truncated series solutions of Kepler's equation by Lagrange's power series with those by Bessel's Kapteyn series strongly suggest that a Jentzsch-type theorem holds true not only for the former but also for the latter series: each point of the boundary of the domain of convergence in the complex plane is a cluster point of zeros of sections of the series. We prove this result by studying properties of the growth function of a sequence of entire functions. For series, this growth function is computable in terms of the convergence abscissa of an associated general Dirichlet series. The proof then extends, besides including Jentzsch's classical result for power series, to general Dirichlet series, to Kapteyn, and to Neumann series of Bessel functions. Moreover, sections of Kapteyn and Neumann series generally exhibit zeros close to the real axis which can be explained, including their asymptotic linear density, by the theory of the distribution of zeros of entire functions.";"Folkmar Bornemann";"0";"Arxiv"
"A survey on Bayesian inference for Gaussian mixture model";"2021-08-20T13:23:17Z";"http://arxiv.org/abs/2108.11753v1";"Clustering has become a core technology in machine learning, largely due to its application in the field of unsupervised learning, clustering, classification, and density estimation. A frequentist approach exists to hand clustering based on mixture model which is known as the EM algorithm where the parameters of the mixture model are usually estimated into a maximum likelihood estimation framework. Bayesian approach for finite and infinite Gaussian mixture model generates point estimates for all variables as well as associated uncertainty in the form of the whole estimates' posterior distribution.   The sole aim of this survey is to give a self-contained introduction to concepts and mathematical tools in Bayesian inference for finite and infinite Gaussian mixture model in order to seamlessly introduce their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning this field and given the paucity of scope to present this discussion, e.g., the separated analysis of the generation of Dirichlet samples by stick-breaking and Polya's Urn approaches. We refer the reader to literature in the field of the Dirichlet process mixture model for a much detailed introduction to the related fields. Some excellent examples include (Frigyik et al., 2010; Murphy, 2012; Gelman et al., 2014; Hoff, 2009).   This survey is primarily a summary of purpose, significance of important background and techniques for Gaussian mixture model, e.g., Dirichlet prior, Chinese restaurant process, and most importantly the origin and complexity of the methods which shed light on their modern applications. The mathematical prerequisite is a first course in probability. Other than this modest background, the development is self-contained, with rigorous proofs provided throughout.";"Jun Lu";"0";"Arxiv"
"The multiple Dirichlet product and the multiple Dirichlet series";"2016-01-22T09:44:59Z";"http://arxiv.org/abs/1601.05924v1";"First, we define the multiple Dirichlet product and study the properties of it. From those properties, we obtain a zero-free region of a multiple Dirichlet series and a multiple Dirichlet series expression of the reciprocal of a multiple Dirichlet series.";"Tomokazu Onozuka";"0";"Arxiv"
"Dirichlet Process Parsimonious Mixtures for clustering";"2015-01-14T13:56:35Z";"http://arxiv.org/abs/1501.03347v2";"The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models.";"Faicel Chamroukhi, Marius Bartcus, Hervé Glotin";"0";"Arxiv"
"Clustering action potential spikes: Insights on the use of overfitted   finite mixture models and Dirichlet process mixture models";"2016-02-05T03:36:43Z";"http://arxiv.org/abs/1602.01915v1";"The modelling of action potentials from extracellular recordings, or spike sorting, is a rich area of neuroscience research in which latent variable models are often used. Two such models, Overfitted Finite Mixture models (OFMs) and Dirichlet Process Mixture models (DPMs) are considered to provide insights for unsupervised clustering of complex, multivariate medical data when the number of clusters is unknown. OFM and DPM are structured in a similar hierarchical fashion but they are based on different philosophies with different underlying assumptions. This study investigates how these differences impact on a real study of spike sorting, for the estimation of multivariate Gaussian location-scale mixture models in the presence of common difficulties arising from complex medical data. The results provide insights allowing the future analyst to choose an approach suited to the situation and goal of the research problem at hand.";"Zoé van Havre, Nicole White, Judith Rousseau, Kerrie Mengersen";"0";"Arxiv"
"Dirichlet process mixtures under affine transformations of the data";"2018-09-07T13:30:06Z";"http://dx.doi.org/10.1007/s00180-020-01013-y";"Location-scale Dirichlet process mixtures of Gaussians (DPM-G) have proved extremely useful in dealing with density estimation and clustering problems in a wide range of domains. Motivated by an astronomical application, in this work we address the robustness of DPM-G models to affine transformations of the data, a natural requirement for any sensible statistical method for density estimation and clustering. First, we devise a coherent prior specification of the model which makes posterior inference invariant with respect to affine transformations of the data. Second, we formalise the notion of asymptotic robustness under data transformation and show that mild assumptions on the true data generating process are sufficient to ensure that DPM-G models feature such a property. Our investigation is supported by an extensive simulation study and illustrated by the analysis of an astronomical dataset consisting of physical measurements of stars in the field of the globular cluster NGC 2419.";"Julyan Arbel, Riccardo Corradin, Bernardo Nipoti";"0";"Arxiv"
"Dirichlet Graph Variational Autoencoder";"2020-10-09T07:35:26Z";"http://arxiv.org/abs/2010.04408v2";"Graph Neural Networks (GNNs) and Variational Autoencoders (VAEs) have been widely used in modeling and generating graphs with latent factors. However, there is no clear explanation of what these latent factors are and why they perform well. In this work, we present Dirichlet Graph Variational Autoencoder (DGVAE) with graph cluster memberships as latent factors. Our study connects VAEs based graph generation and balanced graph cut, and provides a new way to understand and improve the internal mechanism of VAEs based graph generation. Specifically, we first interpret the reconstruction term of DGVAE as balanced graph cut in a principled way. Furthermore, motivated by the low pass characteristics in balanced graph cut, we propose a new variant of GNN named Heatts to encode the input graph into cluster memberships. Heatts utilizes the Taylor series for fast computation of heat kernels and has better low pass characteristics than Graph Convolutional Networks (GCN). Through experiments on graph generation and graph clustering, we demonstrate the effectiveness of our proposed framework.";"Jia Li, Tomasyu Yu, Jiajin Li, Honglei Zhang, Kangfei Zhao, YU Rong, Hong Cheng, Junzhou Huang";"0";"Arxiv"
"Dirichlet Process Mixture Models with Shrinkage Prior";"2020-10-22T02:22:10Z";"http://arxiv.org/abs/2010.11385v3";"We propose Dirichlet Process Mixture (DPM) models for prediction and cluster-wise variable selection, based on two choices of shrinkage baseline prior distributions for the linear regression coefficients, namely the Horseshoe prior and Normal-Gamma prior. We show in a simulation study that each of the two proposed DPM models tend to outperform the standard DPM model based on the non-shrinkage normal prior, in terms of predictive, variable selection, and clustering accuracy. This is especially true for the Horseshoe model, and when the number of covariates exceeds the within-cluster sample size. A real data set is analyzed to illustrate the proposed modeling methodology, where both proposed DPM models again attained better predictive accuracy.";"Dawei Ding, George Karabatsos";"0";"Arxiv"
"Bayesian Nonparametric Mixtures of Exponential Random Graph Models for   Ensembles of Networks";"2022-01-20T12:50:43Z";"http://arxiv.org/abs/2201.08153v1";"Ensembles of networks arise in various fields where multiple independent networks are observed on the same set of nodes, for example, a collection of brain networks constructed on the same brain regions for different individuals. However, there are few models that describe both the variations and characteristics of networks in an ensemble at the same time. In this paper, we propose to model the ensemble of networks using a Dirichlet Process Mixture of Exponential Random Graph Models (DPM-ERGMs), which divides the ensemble into different clusters and models each cluster of networks using a separate Exponential Random Graph Model (ERGM). By employing a Dirichlet process mixture, the number of clusters can be determined automatically and changed adaptively with the data provided. Moreover, in order to perform full Bayesian inference for DPM-ERGMs, we employ the intermediate importance sampling technique inside the Metropolis-within-slice sampling scheme, which addressed the problem of sampling from the intractable ERGMs on an infinite sample space. We also demonstrate the performance of DPM-ERGMs with both simulated and real datasets.";"Sa Ren, Xue Wang, Peng Liu, Jian Zhang";"0";"Arxiv"
"A Dirichlet Process Mixture Model for Clustering Longitudinal Gene   Expression Data";"2016-09-10T00:25:58Z";"http://arxiv.org/abs/1609.02980v2";"Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Moreover, the longitudinal gene expression data allows for intra-individual variability to be accounted for when grouping patients. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel subgroup identification method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to distinguish burn patients from trauma patients and identify interesting subgroups in trauma patients.";"Jiehuan Sun, Jose D. Herazo-Maya, Naftali Kaminski, Hongyu Zhao, Joshua L. Warren";"0";"Arxiv"
"On multilinear spectral cluster estimates for manifolds with boundary";"2006-11-14T22:41:10Z";"http://arxiv.org/abs/math/0611443v1";"We prove sharp bilinear estimates for Dirichlet or Neumann eigenfunctions in domains in the plane. These are the natural analog of earlier estimates for the boundaryless case of Burq, G\'erard, and Tzvetkov.";"Matthew D. Blair, Hart F. Smith, Christopher D. Sogge";"0";"Arxiv"
"Subcritical Lp bounds on spectral clusters for Lipschitz metrics";"2007-09-18T07:37:51Z";"http://arxiv.org/abs/0709.2764v1";"We establish asymptotic bounds on the L^p norms of spectrally localized functions in the case of two-dimensional Dirichlet forms with coefficients of Lipschitz regularity. These bounds are new for the range p>6. A key step in the proof is bounding the rate at which energy spreads for solutions to hyperbolic equations with Lipschitz coefficients.";"Herbert Koch, Hart F. Smith, Daniel Tataru";"0";"Arxiv"
"Wright-Fisher construction of the two-parameter Poisson-Dirichlet   diffusion";"2016-01-22T16:35:09Z";"http://dx.doi.org/10.1214/16-AAP1252";"The two-parameter Poisson--Dirichlet diffusion, introduced in 2009 by Petrov, extends the infinitely-many-neutral-alleles diffusion model, related to Kingman's one-parameter Poisson--Dirichlet distribution and to certain Fleming--Viot processes. The additional parameter has been shown to regulate the clustering structure of the population, but is yet to be fully understood in the way it governs the reproductive process. Here we shed some light on these dynamics by formulating a $K$-allele Wright--Fisher model for a population of size $N$, involving a uniform mutation pattern and a specific state-dependent migration mechanism. Suitably scaled, this process converges in distribution to a $K$-dimensional diffusion process as $N\to\infty$. Moreover, the descending order statistics of the $K$-dimensional diffusion converge in distribution to the two-parameter Poisson--Dirichlet diffusion as $K\to\infty$. The choice of the migration mechanism depends on a delicate balance between reinforcement and redistributive effects. The proof of convergence to the infinite-dimensional diffusion is nontrivial because the generators do not converge on a core. Our strategy for overcoming this complication is to prove \textit{a priori} that in the limit there is no ""loss of mass"", i.e., that, for each limit point of the sequence of finite-dimensional diffusions (after a reordering of components by size), allele frequencies sum to one.";"Cristina Costantini, Pierpaolo De Blasi, Stewart N. Ethier, Matteo Ruggiero, Dario Spano";"0";"Arxiv"
"Cluster point processes on manifolds";"2011-09-28T18:16:13Z";"http://arxiv.org/abs/1109.6283v1";"The probability distribution $\mu_{cl}$ of a general cluster point process in a Riemannian manifold $X$ (with independent random clusters attached to points of a configuration with distribution $\mu$) is studied via the projection of an auxiliary measure $\hat{\mu}$ in the space of configurations $\hat{\gamma}=\{(x,\bar{y})\}\subset X\times\mathfrak{X}$, where $x\in X$ indicates a cluster ""centre"" and $\bar{y}\in\mathfrak{X}:=\bigsqcup_{n} X^n$ represents a corresponding cluster relative to $x$. We show that the measure $\mu_{cl}$ is quasi-invariant with respect to the group $Diff_{0}(X)$ of compactly supported diffeomorphisms of $X$, and prove an integration-by-parts formula for $\mu_{cl}$. The associated equilibrium stochastic dynamics is then constructed using the method of Dirichlet forms. General constructions are illustrated by examples including Euclidean spaces, Lie groups, homogeneous spaces, Riemannian manifolds of nonpositive curvature and metric spaces. The paper is an extension of our earlier results for Poisson cluster measures [J. Funct. Analysis 256 (2009) 432-478] and for Gibbs cluster measures [arxiv:1007.3148], where different projection constructions were utilised.";"Leonid Bogachev, Alexei Daletskii";"0";"Arxiv"
"Flexible Models for Microclustering with Application to Entity   Resolution";"2016-10-31T04:00:11Z";"http://arxiv.org/abs/1610.09780v1";"Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.";"Giacomo Zanella, Brenda Betancourt, Hanna Wallach, Jeffrey Miller, Abbas Zaidi, Rebecca C. Steorts";"0";"Arxiv"
"Latent Mixture Modeling for Clustered Data";"2017-04-20T03:33:52Z";"http://arxiv.org/abs/1704.05993v1";"This article proposes a mixture modeling approach to estimating cluster-wise conditional distributions in clustered (grouped) data. We adapt the mixture-of-experts model to the latent distributions, and propose a model in which each cluster-wise density is represented as a mixture of latent experts with cluster-wise mixing proportions distributed as Dirichlet distribution. The model parameters are estimated by maximizing the marginal likelihood function using a newly developed Monte Carlo Expectation-Maximization algorithm. We also extend the model such that the distribution of cluster-wise mixing proportions depends on some cluster-level covariates. The finite sample performance of the proposed model is compared with some existing mixture modeling approaches as well as linear mixed model through the simulation studies. The proposed model is also illustrated with the posted land price data in Japan.";"Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo";"0";"Arxiv"
"Deep Representation Learning for Clustering of Health Tweets";"2018-12-25T00:31:22Z";"http://arxiv.org/abs/1901.00439v1";"Twitter has been a prominent social media platform for mining population-level health data and accurate clustering of health-related tweets into topics is important for extracting relevant health insights. In this work, we propose deep convolutional autoencoders for learning compact representations of health-related tweets, further to be employed in clustering. We compare our method to several conventional tweet representation methods including bag-of-words, term frequency-inverse document frequency, Latent Dirichlet Allocation and Non-negative Matrix Factorization with 3 different clustering algorithms. Our results show that the clustering performance using proposed representation learning scheme significantly outperforms that of conventional methods for all experiments of different number of clusters. In addition, we propose a constraint on the learned representations during the neural network training in order to further enhance the clustering performance. All in all, this study introduces utilization of deep neural network-based architectures, i.e., deep convolutional autoencoders, for learning informative representations of health-related tweets.";"Oguzhan Gencoglu";"0";"Arxiv"
"Model-Based Hierarchical Clustering";"2013-01-16T15:53:05Z";"http://arxiv.org/abs/1301.3899v1";"We present an approach to model-based hierarchical clustering by formulating an objective function based on a Bayesian analysis. This model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model. Features can have either a unique distribution in every cluster or a common distribution over some (or even all) of the clusters. The cluster subsets over which these features have such a common distribution correspond to the nodes (clusters) of the tree representing the hierarchy. We apply this general model to the problem of document clustering for which we use a multinomial likelihood function and Dirichlet priors. Our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters. The regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters, the depth of the tree and the subset of features to be modeled as having a common distribution at each node. We present experimental results on both synthetic data and a real document collection.";"Shivakumar Vaithyanathan, Byron E Dom";"0";"Arxiv"
"Unification of HDP and LDA Models for Optimal Topic Clustering of   Subject Specific Question Banks";"2020-10-04T18:21:20Z";"http://arxiv.org/abs/2011.01035v1";"There has been an increasingly popular trend in Universities for curriculum transformation to make teaching more interactive and suitable for online courses. An increase in the popularity of online courses would result in an increase in the number of course-related queries for academics. This, coupled with the fact that if lectures were delivered in a video on demand format, there would be no fixed time where the majority of students could ask questions. When questions are asked in a lecture there is a negligible chance of having similar questions repeatedly, but asynchronously this is more likely. In order to reduce the time spent on answering each individual question, clustering them is an ideal choice. There are different unsupervised models fit for text clustering, of which the Latent Dirichlet Allocation model is the most commonly used. We use the Hierarchical Dirichlet Process to determine an optimal topic number input for our LDA model runs. Due to the probabilistic nature of these topic models, the outputs of them vary for different runs. The general trend we found is that not all the topics were being used for clustering on the first run of the LDA model, which results in a less effective clustering. To tackle probabilistic output, we recursively use the LDA model on the effective topics being used until we obtain an efficiency ratio of 1. Through our experimental results we also establish a reasoning on how Zeno's paradox is avoided.";"Nikhil Fernandes, Alexandra Gkolia, Nicolas Pizzo, James Davenport, Akshar Nair";"0";"Arxiv"
"Common Failure Modes of Subcluster-based Sampling in Dirichlet Process   Gaussian Mixture Models -- and a Deep-learning Solution";"2022-03-25T14:12:33Z";"http://arxiv.org/abs/2203.13661v1";"The Dirichlet Process Gaussian Mixture Model (DPGMM) is often used to cluster data when the number of clusters is unknown. One main DPGMM inference paradigm relies on sampling. Here we consider a known state-of-art sampler (proposed by Chang and Fisher III (2013) and improved by Dinari et al. (2019)), analyze its failure modes, and show how to improve it, often drastically. Concretely, in that sampler, whenever a new cluster is formed it is augmented with two subclusters whose labels are initialized at random. Upon their evolution, the subclusters serve to propose a split of the parent cluster. We show that the random initialization is often problematic and hurts the otherwise-effective sampler. Specifically, we demonstrate that this initialization tends to lead to poor split proposals and/or too many iterations before a desired split is accepted. This slows convergence and can damage the clustering. As a remedy, we propose two drop-in-replacement options for the subcluster-initialization subroutine. The first is an intuitive heuristic while the second is based on deep learning. We show that the proposed approach yields better splits, which in turn translate to substantial improvements in performance, results, and stability.";"Vlad Winter, Or Dinari, Oren Freifeld";"0";"Arxiv"
"Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation";"2016-04-02T17:27:24Z";"http://arxiv.org/abs/1604.00533v1";"Color image segmentation is a crucial step in many computer vision and pattern recognition applications. This article introduces an adaptive and unsupervised clustering approach based on Voronoi regions, which can be applied to solve the color image segmentation problem. The proposed method performs region splitting and merging within Voronoi regions of the Dirichlet Tessellated image (also called a Voronoi diagram) , which improves the efficiency and the accuracy of the number of clusters and cluster centroids estimation process. Furthermore, the proposed method uses cluster centroid proximity to merge proximal clusters in order to find the final number of clusters and cluster centroids. In contrast to the existing adaptive unsupervised cluster-based image segmentation algorithms, the proposed method uses K-means clustering algorithm in place of the Fuzzy C-means algorithm to find the final segmented image. The proposed method was evaluated on three different unsupervised image segmentation evaluation benchmarks and its results were compared with two other adaptive unsupervised cluster-based image segmentation algorithms. The experimental results reported in this article confirm that the proposed method outperforms the existing algorithms in terms of the quality of image segmentation results. Also, the proposed method results in the lowest average execution time per image compared to the existing methods reported in this article.";"R. Hettiarachchi, J. F. Peters";"0";"Arxiv"
"Gaussian Latent Dirichlet Allocation for Discrete Human State Discovery";"2022-06-28T18:33:46Z";"http://arxiv.org/abs/2206.14233v1";"In this article we propose and validate an unsupervised probabilistic model, Gaussian Latent Dirichlet Allocation (GLDA), for the problem of discrete state discovery from repeated, multivariate psychophysiological samples collected from multiple, inherently distinct, individuals. Psychology and medical research heavily involves measuring potentially related but individually inconclusive variables from a cohort of participants to derive diagnosis, necessitating clustering analysis. Traditional probabilistic clustering models such as Gaussian Mixture Model (GMM) assume a global mixture of component distributions, which may not be realistic for observations from different patients. The GLDA model borrows the individual-specific mixture structure from a popular topic model Latent Dirichlet Allocation (LDA) in Natural Language Processing and merges it with the Gaussian component distributions of GMM to suit continuous type data. We implemented GLDA using STAN (a probabilistic modeling language) and applied it on two datasets, one containing Ecological Momentary Assessments (EMA) and the other heart measures from electrocardiogram and impedance cardiograph. We found that in both datasets the GLDA-learned class weights achieved significantly higher correlations with clinically assessed depression, anxiety, and stress scores than those produced by the baseline GMM. Our findings demonstrate the advantage of GLDA over conventional finite mixture models for human state discovery from repeated multivariate data, likely due to better characterization of potential underlying between-participant differences. Future work is required to validate the utility of this model on a broader range of applications.";"Congyu Wu, Aaron Fisher, David Schnyer";"0";"Arxiv"
