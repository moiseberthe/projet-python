"CFP Open Cosmos Conf 2023";"2022-12-05 16:11:15";"https://www.reddit.com/r/nosql/comments/zd9dom/cfp_open_cosmos_conf_2023/";"Hey everyone!  Please check out the CFP for Cosmos Conf 2023!  Share how you've added speed, scale, and reliability to your applications with Azure Cosmos DB.  Event is March 28 and CFP closes Feb 1.  [https://aka.ms/CosmosConf2023CFP](https://aka.ms/CosmosConf2023CFP)  &#x200B;  Any questions about the event?  Leave them in the comments!";"jaydestro";"Reddit"
"[Noob] Which NoSQL DB to choose for report data?";"2022-09-08 22:48:36";"https://www.reddit.com/r/nosql/comments/x9bcd4/noob_which_nosql_db_to_choose_for_report_data/";"Sorry for this noob question:    I think about trying to automate some reports  I need to create periodically.   And as a first step, I think about collecting (text and) data in documents within a NoSQL database.     There will be just a few documents for each report, like ""Chapter 1 text"", ""Chapter 1 prepared data"", Chapter 1 raw data"", and so on for like 6-8 chapters, and the prepared and raw data will be unstructured: tables, pictures, graphs, whatever. Each document will also include (or be tagged with) the current date and the customer name, so I'll be able to easily select all documents that are necessary for the report about customer x in month y.   So... which NoSQL database will be suited for my strange requirements? Maybe one with an easy to use frontend/client which allows me to easily interact with the database, display & manipulate documents etc.?   Thanks for your hints :)";"e_hyde";"Reddit"
"How do I populate message id for the chat application in NoSql?";"2022-08-02 08:13:44";"https://www.reddit.com/r/nosql/comments/we4170/how_do_i_populate_message_id_for_the_chat/";"I plan to use dynamoDB to store the chat messages across the rooms. I use a table (rooms) in RDS to create room\_id when the room is created and then use this id (pk) in the DynamoDB when storing messages.  My table in DynamoDB might look like this.  PK: Room#<room\_id>  SK: Msg\_\_<msg\_id>  Since I storing messages just in DynamoDB unlike room\_id I don't have the ids for it from RDS. How do I generate them as I store the messages?";"Ok-Outlandishness-74";"Reddit"
"Best key/query to access document in MongoDB collection?";"2022-07-25 02:11:39";"https://www.reddit.com/r/nosql/comments/w7a33n/best_keyquery_to_access_document_in_mongodb/";" Im currently creating a backend using FastAPI and MongoDB. I have a users collection that stores a bunch of User documents. Originally, for all my REST routes I planned on using a path like /user/{id} , but, correct me if I'm wrong, it would be difficult using Mongo's ID field as that ID is completely inaccessible from my frontend. Instead, I'm now considering using a field like the User's unique Spotify user id which is obtained when the user authenticates with Spotify. I'm also considering compositing this user\_id key with their phone number for an even more secure key. I would appreciate any direction regarding the best way to securely access a user's entry in the DB/collection. Thanks.";"charbrinks";"Reddit"
"Best NoSQL system for economic database";"2022-07-16 17:56:58";"https://www.reddit.com/r/nosql/comments/w0jsh4/best_nosql_system_for_economic_database/";"I work in the economic team of a non-profit where I am responsible for all data related matters. Our team produces and consumes a lot of economic indicators and numbers. At the moment a lot of these numbers are buried within PDF documents (papers, articles,...) on our sharepoint.  I have been playing with the idea to save all relevant numbers in a database system. The numbers would have different tags depending on the subject and the specific indicator, a timestamp and a specific indicator name. Preferably, I would also be able to store the tags in a hierarchical way.  The following examples come to mind:  * **E-Commerce Sales for the fashion industry for Belgium in 2021:** The number should be saved in the following way:  &#8203;          {             ""tags"": {                 ""level 1"": ""E-Commerce"",                 ""level 2"": ""Fashion""             },             ""geography"": {""land"":""Belgium""}             ""timestamp"": {""year"":""2021""},             ""indicator"": ""E-Commerce Sales for the fashion industry for Belgium in 2021""                       }  * **Fashion Sales in Brussels in Q1 of 2022**:  &#8203;          {             ""tags"": {                 ""level 1"": ""Fashion"",             },             ""geography"": {""land"":""Belgium"", ""region"":""Brussels""}             ""timestamp"": {""year"":""2021"", ""quarter"":""Q4""},             ""indicator"": ""Fashion Sales in Brussels in Q1 of 2022""                       }  As you can see, values do not necessarily have the same amount of hierarchical levels in tags. Furthermore, both timestamp and geography have different levels of precision for different data points. I would like to save the possible tags in a table somewhere, so that for the different hierarchical tag levels, there is some kind of lookup to that specific table. I am quite convinced that my use case asks for a NoSQL implementation but since I am a newbie in NoSQL I don't know which system I should pick.  So my question is the following: Which system should I pick based on my specific use case? Thanks!";"SDR3078";"Reddit"
"How hard is NoSQL to learn?";"2022-06-24 19:54:36";"https://www.reddit.com/r/nosql/comments/vjuanp/how_hard_is_nosql_to_learn/";"Coming from SQL and so used to relationship data. How hard is is to change gears? Is there a GUI that can show me the basics?";"seanner_vt2";"Reddit"
"Ever worried about testing a NoSQL & .NET bundle?";"2022-06-19 12:40:42";"https://www.reddit.com/r/nosql/comments/vfsg1l/ever_worried_about_testing_a_nosql_net_bundle/";"Integration testing can be a real pain/cost of NoSQL development and may affect (or contribute to) the choice of the NoSQL engine on the basis of development convenience.  [Here](https://alex-klaus.com/ravendb-yabt-testing/) I compared the .NET SDKs and NuGet packages for 4 NoSQL engines: **RavenDB**, **CosmosDB**, **MongoDB** and **DynamoDB**. It seems that RavenDB comes first in the race, CosmosDB – last, and the rest sit in the middle.  What do you guys think? Did I miss anything?";"AlKla";"Reddit"
"Which free and opensource NoSQL database provides feature for creating group/bucket of documents?";"2022-05-27 17:03:54";"https://www.reddit.com/r/nosql/comments/uz06rz/which_free_and_opensource_nosql_database_provides/";"I am learning CouchDB. As I understand it, documents in the database cannot be grouped into categories, such as, for example, all receipt documents can be put into a receipt bucket, invoices can be put into invoice bucket etc.  Are there any free and opensource NoSQL databases that provide this feature of grouping documents according to category?";"_448";"Reddit"
"Post/Comment DB design: Postgresql v/s CouchDB";"2022-03-28 16:09:34";"https://www.reddit.com/r/nosql/comments/tqa57l/postcomment_db_design_postgresql_vs_couchdb/";"I am comparing DB design for a simple ""Post and Comment"" system using Postgres and CouchDB. With Postgres I can design the following tables:   user_info {email, pass_hash, pass_salt, ...}  post_info {post_id, creator_email, title, text, ...}  comment_info {comment_id, creator_email, post_id, parent_comment_id, text, ...}   But if I use CouchDB, there is a concept of creating per-user tables. So I was thinking of the following design:   user_table {email, table_id}  user_<table_id> {email, pass_hash, pass_salt, ...}  post_<table_id> {post_id, <table_id>_creator_email, title, text, ...}  comment_<table_id> {comment_id, <table_id>_creator_email, <table_id>_post_id, <table_id>_parent_comment_id, text, ...}   I am in no way expert in Postgres and CouchDB, so my question is, is this the correct way to design per-user CouchDB tables? What is the better way? And what is the efficient way to create/use CRUD queries?";"_448";"Reddit"
"A little toolkit to help manage Redis";"2022-02-21 10:01:37";"https://www.reddit.com/r/nosql/comments/sxp2e1/a_little_toolkit_to_help_manage_redis/";"Docker image with several Redis CLI tools, and a tool to dump Redis data. The image includes browser-based IDE, filebrowser, and scheduler.  Simply run on a server or k8s cluster and schedule tasks.   It also has local Redis running and can be used as a better substitute for the local development environment instead of a standard Redis docker image.  [https://github.com/bluxmit/alnoda-workspaces/tree/main/workspaces/codeserver-workspace](https://github.com/bluxmit/alnoda-workspaces/tree/main/workspaces/codeserver-workspace)";"Bluxmit";"Reddit"
"Noob Question - I've never used NoSQL before";"2022-02-02 17:16:09";"https://www.reddit.com/r/nosql/comments/siskfb/noob_question_ive_never_used_nosql_before/";"So I've worked with SQL practically every day ever since I started my career with data. I've never had to work with NoSQL but I do know that it's the opposite of SQL right, so non-relational. Now my question is how do you work with NoSQL? If I wanted to pull some data from a NoSQL database how do you query that? I've tried to google this but I haven't found anything - unless I have and it's nothing like querying something from MySQL or MSSQL. If someone can provide an example that'd be awesome.";"kkjeb";"Reddit"
"Looking for good use cases for NoSQL";"2022-01-08 16:23:09";"https://www.reddit.com/r/nosql/comments/rz27cq/looking_for_good_use_cases_for_nosql/";"I’m fairly experienced with RDBMS and have watched a few tutorials and videos explaining NoSQL databases. I generally understand the technical differences at a theoretical level but am struggling to come up with some good use cases where NoSQL (particularly a document db such as MongoDB) is clearly a better choice over a RDBMS. I would also be interested in examples of use cases for a graph db such as Gremlin. Could anyone provide examples? Links to videos or blogs are also welcome.";"";"Reddit"
"Wrapping my head around noSQL, specifically dynamodb";"2021-12-02 02:26:34";"https://www.reddit.com/r/nosql/comments/r6v3ar/wrapping_my_head_around_nosql_specifically/";"So, been reading up on noSQL tonight and I think I've got the idea here. Bascially (think a spreadsheet), the PK and SK are what I'm going to call by in an application and then we just replace the attributes based on the data we're putting in?  &#x200B;  Question I had is, is it normal to have a huge array of attributes.  So say for instance I'm working on an inspection app (I know the norm is task management but I like to be different and it's got a use case in my life)....  &#x200B;  I've got users, requirements (required inspections), inspections(the actual inspections themselves) and depending on the requirement (what kind of inspection it is, there could be different attributes).... that gets pretty insane, unless I've missed something.  &#x200B;  Here is kind of what I'm picturing in the following format:  pk, sk, attributes  \#USER#username, #PROFILE#username, name, address, phone, etc  \#USER#username, #REQ#<some\_identifier>, each attribute is then questions, or details about that inspection requirement  and inspections and so on....   &#x200B;  If I understand correctly, then I've got the capability to search for all the inspections the user has completed, all of the inspections the user is responsible for, the inspections completed by the user....   &#x200B;  &#x200B;  and if I understand secondary global indexes, then I can even add in a data attribute to search for completed inspections by date?  &#x200B;  Am I on the right track, I guess at this point I just needed to explain it somewhere so that I know I'm on the right track?";"Neat_Objective";"Reddit"
"NoSQL productivity tools";"2021-10-28 06:09:31";"https://www.reddit.com/r/nosql/comments/qhel8g/nosql_productivity_tools/";"We are a Toronto-based software startup currently working on a new set of tools to increase productivity for companies using NoSQL technologies.  At this point, we are not selling anything, we are just looking for advice and feedback to help us build the best possible tools.  Our tools will help with the following:  1) Database diagramming  2) Visual comparison of table contents (within or across accounts)  3) Moving data between tables (within or across accounts) using visual tools  4) Synchronization of tables (within or across accounts)  5) Export of table scripts and comparison of table schemas (generating documentation in HTML & PDF)  We would love to get 30 minutes of your time to help us understand if any of these issues resonate with you and if so, your current workflows and how you are solving these problems today.  You can view more at the link below.  https://nosqlnavigator.com/  Please reply if you would like to discuss.";"braveheart2019";"Reddit"
"Why is Cassandra considered column-based and DynamoDB key-value?";"2021-08-21 18:02:08";"https://www.reddit.com/r/nosql/comments/p8u0ua/why_is_cassandra_considered_columnbased_and/";"They rely on the exact same data model concept of having a table where we first identify the row / key / item and then select some columns / values in order to retrieve the wanted cell / attribute.  Here is one quote from a relevant article:  ""The top level data structure in Cassandra is the keyspace which is analogous to a relational database. The keyspace is the container for the tables and it is where you configure the replica count and placement. Keyspaces contain tables (formerly called column families) composed of rows and columns. A table schema must be defined at the time of table creation.  The top level structure for DynamoDB is the table which has the same functionality as the Cassandra table. Rows are items, and cells are attributes. In DynamoDB, it’s possible to define a schema for each item, rather than for the whole table.  Both tables store data in sparse rows—for a given row, they store only the columns present in that row. Each table must have a primary key that uniquely identifies rows or items. Every table must have a primary key which has two components.""  Sounds like pretty much the same thing. So, why the difference in terminology?";"uber_kuber";"Reddit"
"Do you assign a name to your clients when connecting to redis or MongoDB?";"2021-08-10 08:05:58";"https://www.reddit.com/r/nosql/comments/p1jdmh/do_you_assign_a_name_to_your_clients_when/";"Hey all,  Lately, I was reminded about a feature to name your clients when connecting to your databases. From the NoSQL perspective, Redis and MongoDB are supporting this.  The basic idea is to identify the client against the database server. Depending on the system, the client name will be shown in several places like logs or in the monitoring endpoint.  **How it works with redis?**  Execute the [`CLIENT SETNAME`](https://redis.io/commands/client-setname) command like:      CLIENT SETNAME currency-conversion-app  It is a cheap (complexity: `O(1)`) command that can be executed without any overhead. Typically, you run it directly after the connection to the redis instance has been established.  With [`CLIENT LIST`](https://redis.io/commands/client-list) you can check who is connected:      $ CLIENT LIST     id=3 addr=172.17.0.1:62668 name=currency-conversion-app [...]     id=4 addr=172.17.0.1:62676 name=stock-exchange-rates-app [...]  **How it works with MongoDB?**  While creating a connection to MongoDB, you can provide an [`appName`](https://docs.mongodb.com/manual/reference/connection-string/#mongodb-urioption-urioption.appName) in the connection string.  Here is how it looks like in Go:      dsn := ""mongodb://root:secret@127.0.0.1:27017/?appName=currency-conversion-app""     client, err := mongo.Connect(ctx, options.Client().ApplyURI(dsn))  While checking the current operations with [`db.currentOp()`](https://docs.mongodb.com/manual/reference/method/db.currentOp/) the client name will be shown nicely.  **Useful in the real world?**  I can say, I use it all the time and it proved to be very useful.  Especially in bigger setups at work with multiple Redis nodes inside a Cluster and hundreds of clients.  While I was digging into it a bit more, I found out that several other systems, like MySQL, RabbitMQ, or NATS, which I use in combination with Redis, also support similar features.  So I documented how and especially WHY to do it here: [your database connection deserves a name](https://andygrunwald.com/blog/your-database-connection-deserves-a-name/).  I am curious: **Are you using this feature in your setup?** * If no, why not? * If yes, what was the situation where you thought, ""wow, this helped me a lot""?";"andygrunwald";"Reddit"
"How to model foreign key like relationship in firestore";"2021-07-01 16:40:24";"https://www.reddit.com/r/nosql/comments/obn8au/how_to_model_foreign_key_like_relationship_in/";"Lets imagine I have this data model:  I have a student with name and age, and a student can be in a class and also in a sport team.  In a relational database I would store the students into a `student` column. And in the `class` and `sport` tables I would reference each students via a foreign key.  This has the advantage that when a student celebrates their birth date, I only need to change the `age` in one place, which is the `student` table.  With firestore which I understand to be a nosql, the things I am reading is pointing to a modeling where I have a `class` document, within which all `student` will be embedded. Same also for the `team` document.  The only problem I have with this kind of modeling is if I want to update the age of a student, I would have to update in all the places, the student structure is embedded in.  Is there a better way to achieve what I can have in relational database? Where I have data defined in one place and can be reference in other places, hence giving the benefit of needing to change that data in only one place?";"finlaydotweber";"Reddit"
"How to design DDB to support finding who I am following of set of users?";"2021-06-23 22:01:37";"https://www.reddit.com/r/nosql/comments/o6kqp9/how_to_design_ddb_to_support_finding_who_i_am/";"Hey there r/nosql  &#x200B;  I'm designing a DDB to support a social graph where users can follow other users, users dont have to follow the user back though, one of the questions we need to answer is...  ***Given a user and the people following them, who of them am I following?***  It's basically finding the intersections of followers for two users, or mutual ""friends"". *Is there a key design that can support this type of lookup?* Any help is much appreciated, I've been pondering this for a long time.  **^(Note)**^(: I'm trying to avoid graph ddb as we have a partner teams that has had a lot of operational burden maintaining one.)";"thatcatpusheen";"Reddit"
"NoSQL for a Relational DB Dude - Help CHANGE my thinking please!";"2021-04-28 09:14:36";"https://www.reddit.com/r/nosql/comments/n09964/nosql_for_a_relational_db_dude_help_change_my/";"I've been using relational dbs for years. Even though my brain is hard-wired for relational dbs, now I'm learning NoSQL. My biggest question has to do with relationships (1:1, 1:many, many:many). How do you determine whether to EMBED the actual data or simply include a reference/key? What are some considerations?  For example: A shopping cart website with user data, product data and order data. There are many scenarios to consider, such as....  * If I want all orders for a given user * If I want all users who ordered a specific product * If I wanted to know how many times a user ordered a specific product * If I wanted to know every product ever ordered by a user * Also, later, some data, such as a product name or user's mailing address or first name, might need to be modified, thereby having to propagate through all existing embedded data that is related. So, let's say the product ""Vit D3"" is renamed to ""Vitamin D3"", if the product is embedded in a million orders then I'd have to update a million order documents. Seems like alot of overhead! * Etc.  Seems like some scenarios would be more efficient to embed child data, while other times it seems better to use a reference/key. And, now my relational db side of my brain kicks in and I end up modeling everything with keys, like a relational db.  So, how do I change my thinking so now I think like a NoSQL guru, rather than a RDBMS guru? What's the process of evaluating these factors when modeling?  Thanks!";"webdevguycrypto";"Reddit"
"Why did latest Starbase source code go offline?";"2021-04-14 00:05:34";"https://www.reddit.com/r/nosql/comments/mqcqwc/why_did_latest_starbase_source_code_go_offline/";"Where can it be found now? [Original URI](https://lweb.cfa.harvard.edu/~john/starbase/starbase.html)";"";"Reddit"
"how does a NoSQL db scheme looks like?";"2021-04-12 12:15:53";"https://www.reddit.com/r/nosql/comments/mpavge/how_does_a_nosql_db_scheme_looks_like/";"I've no much experience with NoSQL db design.  I'm just looking at a MongoDB diagram designed by some colleagues at work, and it look totally like a relational db scheme. Tables, foreign keys, 1:1 and 1 to many relationships, and such. A real ERD diagram.  Is that it? or there are others ways to design NoSQL schemes?";"selfarsoner";"Reddit"
"Kiwi.com: Nonstop Operations with Scylla Even Through the OVHcloud Fire";"2021-03-23 12:26:14";"https://www.reddit.com/r/nosql/comments/mbckik/kiwicom_nonstop_operations_with_scylla_even/";"&#x200B;  https://preview.redd.it/xbao12htiro61.jpg?width=800&format=pjpg&auto=webp&s=23315a0aeb2bb85cf6dba30191f2541fb71ef483  Disasters can strike any business on any day. This particular disaster, a fire at the OVHcloud Strasbourg datacenter, struck recently and the investigation and recovery are still ongoing. This is an initial report of one company’s resiliency in the face of that disaster.  ## Overview of the Incident  Less than an hour after midnight on Wednesday, March 10, 2021, in the city of Strasbourg, at 0:47 CET, [a fire began](https://us.ovhcloud.com/press/press-releases/2021/fire-our-strasbourg-site) in a room at the SBG2 datacenter of OVHcloud, the popular French cloud provider. Within hours the fire had been contained, but not before wreaking havoc. The fire nearly entirely destroyed SBG2, and gutted four of twelve rooms in the adjacent SBG1 datacenter. Additionally, combatting the fire required proactively switching off the other two datacenters, SBG3 and SBG4.  Netcraft [estimates](https://news.netcraft.com/archives/2021/03/10/ovh-fire.html) this disaster accounted for knocking out 3.6 million websites spread across 464,000 domains. Of those,184,000 websites across nearly 60,000 domains were in the French country code Top Level Domain (ccTLD) .FR — about 1 in 50 servers for the entire .FR domain. As Netcraft stated, “Websites that went offline during the fire included online banks, webmail services, news sites, online shops selling PPE to protect against coronavirus, and several countries’ government websites.”  [OVHcloud’s Strasbourg SBG2 Datacenter engulfed in flames. \(Image: SDIS du Bas Rhin \)](https://preview.redd.it/xdx7lx4xiro61.jpg?width=4000&format=pjpg&auto=webp&s=3c22e72b6b27dfb960bd201dd7d3d63dd8036cc2)  **\[This is just an excerpt. To read the story in full, please follow this link to the ScyllaDB website** [**here**](https://www.scylladb.com/2021/03/23/kiwi-com-nonstop-operations-with-scylla-even-through-the-ovhcloud-fire/)**.\]**";"PeterCorless";"Reddit"
"A Shard-Aware Scylla C/C++ Driver";"2021-03-18 17:23:30";"https://www.reddit.com/r/nosql/comments/m7ujgq/a_shardaware_scylla_cc_driver/";"&#x200B;  https://preview.redd.it/jhvr3x8jbtn61.png?width=800&format=png&auto=webp&s=ff5b32c865abe4948cbd6ffb459f3d11af0c8e8d  We are happy to announce the first release of a shard-aware C/C++ driver (connector library). It’s an API-compatible fork of Datastax cpp-driver 2.15.2, currently packaged for x86\_64 CentOS 7 and Ubuntu 18.04 (with more to come!). It’s also easily compilable on most Linux distributions. The driver still works with Apache Cassandra and DataStax Enterprise (DSE), but when paired with Scylla enables shard-aware queries, delivering even greater performance than before.  [GET THE SCYLLA SHARD-AWARE C/C++ DRIVER](https://github.com/scylladb/cpp-driver)  \[This is just an excerpt. Read the blog in full on ScyllaDB's website [here](https://www.scylladb.com/2021/03/18/a-shard-aware-scylla-c-c-driver/).\]";"PeterCorless";"Reddit"
"Zillow: Optimistic Concurrency with Write-Time Timestamps";"2021-03-16 17:06:49";"https://www.reddit.com/r/nosql/comments/m6chqm/zillow_optimistic_concurrency_with_writetime/";"&#x200B;  https://preview.redd.it/81pwfdepyen61.jpg?width=800&format=pjpg&auto=webp&s=d219f43845b0b92bfa29adb1dde2e85f16851212  Dan Podhola is a Principal Software Engineer at [Zillow](https://www.zillow.com/), the most-visited real estate website in the U.S. He specializes in performance tuning of high-throughput backend database services. We were fortunate to have him speak at our Scylla Summit on Optimistic Concurrency with Write-Time Timestamps. If you wish, you can watch the full presentation on-demand:  [WATCH THE ZILLOW PRESENTATION NOW](https://www.scylladb.com/presentations/zillow-optimistic-concurrency-using-write-time-timestamps/)  Dan began by describing his team’s role at Zillow. They are responsible for processing property and listing records — what is for sale or rent — and mapping those to a common Zillow property IDs, then translating different message types into a common interchange format so their teams can talk to each other using the same type of data.  They are also responsible for deciding what’s best to display. He showed a high-level diagram of what happens when they receive a message from one of their data providers. It needs to be translated into a common output format.  https://preview.redd.it/yankotlsyen61.png?width=1729&format=png&auto=webp&s=dccd6beb2aababc5b4ef08b3c7947c5f5ccec73c  “We fetch other data that we know about that property that’s also in that same format. We bundle that data together and choose a winner — I use the term ‘winner’ lightly here — and we send that bundle data out to our consumers.”  \[This is just an excerpt. You can read the blog in full at ScyllaDB's website [here](https://www.scylladb.com/2021/03/16/zillow-optimistic-concurrency-with-write-time-timestamps/).\]";"PeterCorless";"Reddit"
"QOMPLX: Using Scylla with JanusGraph for Cybersecurity";"2021-03-11 19:52:56";"https://www.reddit.com/r/nosql/comments/m2xpr1/qomplx_using_scylla_with_janusgraph_for/";"&#x200B;  https://preview.redd.it/52pkaoxd3gm61.jpg?width=800&format=pjpg&auto=webp&s=fc0323ea917a248c4ed2f6cd34f6d1d4870b6179  [QOMPLX](https://www.qomplx.com/) is a company dedicated to solving complex problems, such as tackling the daunting world of cybersecurity. In this domain you need to be able to support a data model capable of rapid and repeated evolution to discover and counter new threats. This is one key reason why a graph database model is more applicable to QOMPLX’s use case than the rigidly-defined and statically-linked tables of a relational database.  &#x200B;  https://preview.redd.it/km6lhse63gm61.jpg?width=400&format=pjpg&auto=webp&s=8a43cff52f3825404937e41a55dcedb7653a2744  https://preview.redd.it/0xzpeyla3gm61.png?width=199&format=png&auto=webp&s=2bc6dcf0b9b4efc0876fb6f356846883e81b9da5  QOMPLX partnered with the graph database experts at [Expero](https://www.experoinc.com/) to implement their system with [JanusGraph](https://janusgraph.org/), which uses Scylla as an underlying fast and scalable storage layer. We had the privilege to learn from their use case at Scylla Summit this January, which we share with you today.  **\[This is just an excerpt. To watch the video or read the full blog, learning how QOMPLX uses JanusGraph, you can find more** [**here**](https://www.scylladb.com/2021/03/11/qomplx-using-scylla-with-janusgraph-for-cybersecurity/) **on the ScyllaDB website.\]**";"PeterCorless";"Reddit"
"Making Shard-Aware Drivers for CDC";"2021-03-09 18:34:55";"https://www.reddit.com/r/nosql/comments/m1bmct/making_shardaware_drivers_for_cdc/";"&#x200B;  https://preview.redd.it/x47449hyf1m61.png?width=800&format=png&auto=webp&s=8e4820f2bec03c12a859212705cb361deb29ef37  [Change Data Capture (CDC)](http://docs.scylladb.com/using-scylla/cdc/) is a feature that allows users to track and react to changes in their dataset. CDC became production ready (GA) in [Scylla Open Source 4.3](https://www.scylladb.com/product/release-notes/scylla-open-source-4-3/).  Scylla’s implementation of CDC exposes a CQL-compatible interface that makes it possible to use existing tools or drivers to process CDC data. However, due to the unique way in which Scylla distributes CDC data across the cluster, the implementation of shard-awareness in some drivers might get confused and send requests to incorrect nodes or shards when reading CDC data. In this blog post, we will describe what causes this confusion, why it happens and how we solved it on the driver side.  ## Change Data Capture  In Scylla’s implementation CDC is enabled on a per-table basis. For each CDC-enabled table, a separate table called “CDC log” is created. Every time data is modified in the base table, a row is being appended to the CDC log table.  Inside a CDC log table, rows are organized into multiple partitions called “[streams](http://docs.scylladb.com/using-scylla/cdc/cdc-streams/)“. Each stream corresponds to a portion of the token ring (similarly to a vnode). In fact, a single stream corresponds to a part of a vnode which is owned by a single shard of that vnode’s primary replica. After a partition is changed in the base table, a stream is chosen based on the partition’s primary key, and then a row record describing this change is appended to that stream. Such partitioning into streams makes sure that a partition in the base table is stored on the same replicas as the CDC log rows describing changes made to it. This colocation property makes sure that the number of replicas participating in a write operation made on the base table does not increase.  **\[This is just an excerpt. To read the article in full, check it out on ScyllaDB** [**here**](https://www.scylladb.com/2021/03/09/making-shard-aware-drivers-for-cdc/)**. Also links to the latest drivers that implement this new change.\]**";"PeterCorless";"Reddit"
"What are the different ways you can use MongoDB for e-commerce?";"2021-03-08 20:12:09";"https://www.reddit.com/r/nosql/comments/m0ne43/what_are_the_different_ways_you_can_use_mongodb/";"With its flexibility and scalability, MongoDB is a great option for e-commerce sites. Here are a few notable use cases.  **Product Catalogs**  Below is an example of a command using a product document with MongoDB:      db.inventory.insertOne( {       item: ""journal"",       price: 9.99,       qty: 25,       size: { h: 14, l: 21, w: 1 },       features: ""Beautiful, handmade journal."",      categories: [""writing"", ""bestseller""],      image: ""items/journal.jpg""       } )   **Shopping Cart**   The shopping cart data model needs to prevent customers from holding more items than are available in your inventory. The cart should also release any items back to your inventory when a user abandons their cart. Here is an insert() operation you can use to create the cart:      db.carts.insert({      _id: ""the_users_session_id"",      status:'active',      quantity: 3,      total: 575,      products: []});   **Payments**  Security is critical when modeling payments for e-commerce. MongoDB allows you to encrypt data files and perform automatic client-side encryption. You can also choose to only include the last four card digits, without any personally identifiable information in your model. In this case, you will meet PCI requirements without the need for encryption.  &#x200B;  [https://resources.fabric.inc/answers/mongodb-ecommerce](https://resources.fabric.inc/answers/mongodb-ecommerce)";"gibbiv";"Reddit"
"Best Practices for Benchmarking Scylla";"2021-03-04 20:20:40";"https://www.reddit.com/r/nosql/comments/lxsses/best_practices_for_benchmarking_scylla/";"&#x200B;  https://preview.redd.it/awxdxyz3a2l61.png?width=800&format=png&auto=webp&s=393d745f783b0344cd63edab8f0f7683e1c54bed  Benchmarking is hard.  Or, I should say, doing a good, properly set up and calibrated, objective, and fair job of benchmarking is hard.  It is hard because there are many moving parts and nuances you should take into consideration and you must clearly understand what you are measuring. It’s not so easy to properly generate system load to reflect your real-life scenarios. It’s often not so obvious how to correctly measure and analyze the end results. After extracting benchmarking results you need to be able to read them, understand bottlenecks and other issues. You should be able to make your benchmarking results meaningful, ensure they are easily reproducible, and then be able to clearly explain these results to your peers or superiors.  There’s also hard mathematics involved: statistics and queueing theory to help with black boxes and measurements. Not to mention domain-specific knowledge of the system internals of the servers platforms, operating systems, and the software running on it.  With any Online Transaction Processing (OLTP) database — and Scylla is just one example — developers usually want to understand and measure the transaction read/write performance and what factors affect it. In such scenarios, there are usually a number of external clients constantly generating requests to the database. A number of incoming requests per unit of time called throughput or load.  *100,000 Operations per second or \[OPS\]*  Requests reach the database via a communication channel, get processed when the database is ready and then a response is sent back. The round trip time for a request to be processed is called latency. The ultimate goal of an OLTP database performance test is to find out what the latencies of requests are for various throughput rates.  *1ms per request*  There are thousands of requests that form the pattern of the workload. That’s why we don’t want to look at the latency for just individual requests, but rather, we should look at the overall results — a latency distribution. Latency distribution is a [function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) that describes how many requests were worse than some specific latency target.  *99 percentile or P99 or 99%*  Database systems can’t handle an infinite amount of load. There are limits that a system can handle. How much a system is close to its maximum is called utilization. The higher utilization the higher the latency (you can learn more about the math behind this here).  *80% utilization or 0.8*  The end-user doesn’t want to have high latencies for OLTP workloads — those types of workloads are reliant on fast updates. Therefore we target somewhere between 200ms to 10ms for 99 percentile of latency (P99) distribution. If your P99 latencies become too high, your request queues can back up, plus you risk having request timeouts in your application, which then can cascade out of hand in repeated retries, resulting in system bottlenecking.  \[This is just an excerpt. To read the article in full, which includes an in-depth guide on how to set up your benchmarks and calculate expected throughput, parallelism and latencies, check out ScyllaDB's website [here](https://www.scylladb.com/2021/03/04/best-practices-for-benchmarking-scylla/).\]";"PeterCorless";"Reddit"
"ScyllaDB: Project Circe February Update";"2021-03-01 18:05:27";"https://www.reddit.com/r/nosql/comments/lvedz8/scylladb_project_circe_february_update/";"&#x200B;  https://preview.redd.it/d4fiqklg7gk61.png?width=1024&format=png&auto=webp&s=18545c829819b0a1cefb052421915a72d80d17cb  [Project Circe](https://www.scylladb.com/2021/01/12/making-scylla-a-monstrous-database-introducing-project-circe/) is our 2021 initiative to improve Scylla by adding greater capabilities for consistency, performance, scalability, stability, manageability and ease of use. For this installment of our monthly updates on Project Circe, we’ll take a deep dive into the Raft consensus protocol and the part it will play in Scylla, as well as provide a roundup of activities across our software development efforts.  ## Raft in Scylla  At Scylla Summit 2021, ScyllaDB engineering team lead Konstantin “Kostja” Osipov presented on the purpose and implementation of the Raft consensus protocol in Scylla. Best known for his work on [Lightweight Transactions (LWT)](https://www.scylladb.com/2020/07/15/getting-the-most-out-of-lightweight-transactions-in-scylla/) in Scylla using a more efficient implementation of the Paxos protocol, Kostja began with a roundup of those activities, including our recently conducted [Jepsen testing](https://www.scylladb.com/2020/12/23/jepsen-and-scylla-putting-consistency-to-the-test/) to see how our Lightweight Transactions behaved under various stresses and partitioned state conditions.  \[This is just an excerpt. To read the full blog that discusses how Scylla will be able to make schema changes and scale out better using Raft, plus a link to the video, go [here](https://www.scylladb.com/2021/03/01/project-circe-february-update/).\]";"PeterCorless";"Reddit"
"Scylla University: New Lessons for February 2021";"2021-02-24 20:20:49";"https://www.reddit.com/r/nosql/comments/lrl0ui/scylla_university_new_lessons_for_february_2021/";"&#x200B;  https://preview.redd.it/ttrvbby17hj61.png?width=800&format=png&auto=webp&s=72acf9ce8de18a4ef2b8a07cfc9adc79bfa8fe35  In my [previous](https://www.scylladb.com/2021/02/16/whats-new-at-scylla-university-for-february-2021/) blog post, I wrote about the top students for 2020, the Scylla Summit Training Day, getting course completion certificates, and other news. In this blog post I’ll talk about new lessons added to [Scylla University](https://university.scylladb.com/) since our [June 2020 update](https://www.scylladb.com/2020/06/23/scylla-university-updates-june-2020/).  \[This is just an excerpt. To read the full list of new courses available in Scylla University, read more [here](https://www.scylladb.com/2021/02/24/scylla-university-new-lessons-for-february-2021/).\]";"PeterCorless";"Reddit"
"Prometheus Backfilling: Recording Rules and Alerts";"2021-02-23 19:26:45";"https://www.reddit.com/r/nosql/comments/lqp9qu/prometheus_backfilling_recording_rules_and_alerts/";"&#x200B;  https://preview.redd.it/nwt9mvvns9j61.png?width=800&format=png&auto=webp&s=e6be689495c0a4747d7ed25c25c259c614f0844f  For many [Prometheus](https://prometheus.io/) users using recording rules and alerts, a known issue is how both are only generated on the fly at runtime. This limitation has two downsides. First of all, any new recording rule will not be applied to your historical data. Secondly and even more troubling, you cannot even test your rules and alerts against your historical data.  There is active work inside Prometheus to change this, but it’s not there yet. In the short term, to meet this requirement we created a simple utility to produce [OpenMetrics](https://openmetrics.io/) data to fill in the gaps. I will cover the following topics in this blog post:  * Generating OpenMetrics from Prometheus * Backfilling alerts and recording rules  \[This is just an excerpt. Please read the blog in full at ScyllaDB [here](https://www.scylladb.com/2021/02/23/prometheus-backfilling-recording-rules-and-alerts/).\]";"PeterCorless";"Reddit"
"Expedia Group: Our Migration Journey to Scylla";"2021-02-18 19:28:17";"https://www.reddit.com/r/nosql/comments/lmtidg/expedia_group_our_migration_journey_to_scylla/";"&#x200B;  https://preview.redd.it/zgiamd784ai61.jpg?width=800&format=pjpg&auto=webp&s=2c3b9e78515f9e7ae61832c638b6e28713783e66  Expedia Group, the multi-billion-dollar travel brand, presented at our recent Scylla Summit 2021 virtual event. Singaram “Singa” Ragunathan and Dilip Kolosani presented their technical challenges, and how Scylla was able to solve them.  Currently there are multiple applications at Expedia built on top of Apache Cassandra. “Which comes with its own set of challenges,” Singa noted. He highlighted four top issues:  &#x200B;  https://preview.redd.it/8c0utq794ai61.png?width=1670&format=png&auto=webp&s=67e9fbf2ff70e22e4ddf8d7a36a2a851607e796a  * **Garbage Collection:** The first well-known issue is with Java Virtual Machine (JVM) Garbage Collection (GC). Singa noted, “Apache Cassandra, written in Java, brings in the onus of managing garbage collection and making sure it is appropriately tuned for the workload at hand. It takes a significant amount of time and effort, as well as expertise required, to handle and tune the GC pause for every specific use case.” * **Burst Traffic & Infrastructure Costs:** The next two interrelated issues for Expedia are burst traffic which leads to overprovisioning. “With burst traffic or a sudden peak in the workload there is significant disturbance to the p99 response time. So we end up having buffer nodes to handle this peak capacity, which results in more infrastructure costs.” * **Infrequent Releases:** “Another significant worry” for Expedia, according to Singa, was Cassandra’s infrequent release schedule. “According to the past years’ history, the number of Apache Cassandra releases has significantly slowed down.”  Showing a comparative timeline between Cassandra and Scylla, Singa continued, “We would like to compare the open source commits in Cassandra versus Scylla in a timeline chart here, and highlight the amount of releases that Scylla has gone through in the same past three year period. As you can see, it gives enough confidence towards Scylla that, given an issue or bug with a specific release, it will be soon addressed with a patch. In contrast with Apache Cassandra, one might have to wait longer.  &#x200B;  https://preview.redd.it/n3ar3jga4ai61.png?width=3840&format=png&auto=webp&s=37484b2195bcbdc74d1d2b6bc5fb97ce65863f2d  *Timeline created by Expedia showing the update frequency of Cassandra compared to Scylla.*  \[This is just an excerpt. To read the blog in full and view the full Scylla Summit 2021 presentation, go [here](https://www.scylladb.com/2021/02/18/expedia-group-our-migration-journey-to-scylla/).\]";"PeterCorless";"Reddit"
"Kvrocks 1.3.0 is released";"2021-02-08 13:28:14";"https://www.reddit.com/r/nosql/comments/lfas1i/kvrocks_130_is_released/";"Kvrocks is a key value database which based on rocksdb, and compatible with the Redis protocol, intention to decrease the cost of memory and increase the capability.   Now 1.3.0 is release, more compatible with Redis [https://github.com/bitleak/kvrocks/releases/tag/v1.3.0](https://github.com/bitleak/kvrocks/releases/tag/v1.3.0)  Welcome to try!";"ShooterIT";"Reddit"
"Cassandra paging";"2021-02-05 14:32:46";"https://www.reddit.com/r/nosql/comments/ld76qe/cassandra_paging/";"So I have a rather large table to read and I need to use ""ALLOW FILTERING"" . I read a little on how to avoid it and I came across pagination in Cassandra.  So we use sqlalchemy to connect to our database   My question is, how do we set the ""fetch\_size""? Is it possible to set it in the query itself?   Or do I need to use a session object and set the fetch\_size and then loop through the results?   I am somewhat new to Cassandra so a small code snippet would be helpful.  Thanks a lot";"king_booker";"Reddit"
"Entity Relationships in NoSQL: One-to-one, one-to-many, many-to-many...";"2021-02-02 11:37:59";"https://www.reddit.com/r/nosql/comments/last92/entity_relationships_in_nosql_onetoone_onetomany/";"This topic pops up here from time-to-time (e.g. [6 months ago](https://www.reddit.com/r/nosql/comments/hysfas/resources_on_nosqlmongodb_database_modeling/)), when newbies coming from RDBMS ask about approaching  building entity relationships.  [Here I published](https://alex-klaus.com/nosql-entity-relationship/) a brief rundown on  ways of approaching it in NoSQL:  1. Embedded collection. 2. Reference by ID. 3. Duplicating often used fields. 4. Many-to-many relationship (array of references).  Provided examples (for RavenDB) and source code on GitHub.  Hope, it'd be useful for some. Any feedback is welcome!";"AlKla";"Reddit"
"Syncing databases back and forth?";"2021-01-27 01:35:56";"https://www.reddit.com/r/nosql/comments/l5s1xf/syncing_databases_back_and_forth/";"I've been thinking about a solution that would independent individuals to work on local databases and sync/merge their local databases to a remote one. The idea would be to allow people continue to work even on intermittent network connection situations.  Things I though about or tried:  1. SQLite -> PostgreSQL/MySQL  I actually built a small system for this. I'd log all SQL in a journal and executed them again against the remote server once the user clicked in a ""Sync"" button - it would also ""download"" the log and sync remote changes to the local database. How I managed to avoid conflicts between different clients? All tables had an ID column (that was the or part of a unique index) and every client used a different ID. It worked, but was cumbersome. Main problem was in intermediate tables to implement many-to-many relationships.  2. Use the same as above, but with a K-V database with simplier relationship implemented in application level. Not sure if it would be too different from the solution above.  3. Use a blockchain-like structure? Maybe a database that implements something like [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree) (like git and bitcoin)?  Anyway, I'd like to ask if you have any suggestions. Solutions can be either at the database (preferably), library or application level.";"ilikefruits22foo";"Reddit"
"Should I use SQL row or nosql JSON to store chat messages?";"2021-01-08 11:10:44";"https://www.reddit.com/r/nosql/comments/kszoi2/should_i_use_sql_row_or_nosql_json_to_store_chat/";" I am currently psql for my application and I need to store chat messages every time a user sends a message. I was wondering if I should store that as a traditional row or should I store that as a JSON data.  Also constant read and write to the database feels like a bad idea but I am not sure of how else to do it. Please let me know what you think I should do with this challenge";"warrior242";"Reddit"
"Choosing between SQL & NoSQL db for storage of research article data";"2020-10-31 14:59:21";"https://www.reddit.com/r/nosql/comments/jlitpp/choosing_between_sql_nosql_db_for_storage_of/";"Hi,  Looking for guidance, as no real-world exp. with NoSQL deployment. Objective is to store research article data, this would include paper title, paper body text, paper abstract, authors ids, journal ids, publish date, categories etc.  &#x200B;  A paper is the main entity. A unique paper can have several authors, and so a single author can have co-authors.  Authors can be associated with more than 1 paper.  My instinct tells me I have structured data, with all entities (columns) known, and hence go with SQL db.  &#x200B;  I currently don't see any advantage in using NoSQL to persist that kind of data, where such structure is known in advance.  I would really appreciate critical argument against that and any support for using NoSQL in such case, and how I might ""model"" such (e.g. paper container, author container or other).  &#x200B;  With regard to use case of data, I'll be encoding the body text from all papers for NLP processing (e.g. training models for search), plus being able to list all papers per author, show all co-authors of a given author, show all papers published by a specific journal (e.g. Nature), list papers within a timeframe etc.  Thanks in advance!";"LostGoatOnHill";"Reddit"
"how do you start designing a NoSQL db?";"2020-10-23 14:40:32";"https://www.reddit.com/r/nosql/comments/jgmc80/how_do_you_start_designing_a_nosql_db/";"My mind seems to like more Relational databases. The structure is more clear to me, and easy to design around a model.  So my process is to start with a relational model and next ""transform"" it into a NoSQL database, denormalizing and nesting relationships.  Anybody could point out a ""native"" NoSQL design process or some good guides/examples?";"selfarsoner";"Reddit"
"TO ALL PICK MULTIVALUE USERS";"2020-10-22 21:32:57";"https://www.reddit.com/r/nosql/comments/jg6q45/to_all_pick_multivalue_users/";"I am new to the Pick world and I am looking for a community. Is there anyone on this page that uses pick? If not do you know of a page that does use Pick? I'd love to meet anyone on here currently using pick and share thoughts.   Thank you!";"Bizboosterpodcast";"Reddit"
"NoSQL in a Real World Complex App?";"2020-10-05 23:45:41";"https://www.reddit.com/r/nosql/comments/j5smnl/nosql_in_a_real_world_complex_app/";"I have taken a number of courses explaining how to work with different NoSQL databases, but I'm still struggling tremendously with understanding how NoSQL is architected in the real world.  For example, I'm going through a DynamoDB course right now and the instructor talks about having to plan everything really well in advance, like the keys and local secondary indices, etc. And that you're limited to the number of local and global secondary indices you can have and that the local secondary indices have to be created at table creation time and can't be changed later. Maybe it's just me, but I have NEVER worked at or heard of a company that can define that stuff up front and have it stay valid for the life of the application. This makes me think that the only way to use NoSQL for anything real would be to define very generalized keys and indices, but I can see that falling apart really fast in a complex app.  It comes down for me that I just can't wrap my head around using NoSQL in place of the relational DB in my complex app. I have thought about breaking off pieces of functionality and using NoSQL for the smaller piece, but, ultimately, I have to correlate all of the data together for reporting and dashboards and such. I just don't understand how this viably works with NoSQL.  Perhaps what I really need are some architecture design patterns focused around NoSQL that explains how all of the different pieces come together to give me functionality that mimics what I get from a traditional RDBMS.  Am I making any sense at all? I really want to give NoSQL a chance, but I just don't know how to go about it. Thanks for your help in advance.";"djolord";"Reddit"
"MongoDb vs ElasticSearch for read operations?";"2020-08-25 17:59:14";"https://www.reddit.com/r/nosql/comments/igexq5/mongodb_vs_elasticsearch_for_read_operations/";"My organization is contemplating using ElasticSearch for ALL read operations. And mongoDb as a database for write operations.  What are your views on it? We do not have a requirement of full text search as such. But what we do have is complicated queries that could involve multiple collections and various operations such as lookup(join), group by, filter criteria etc.    How do Elasticsearch query language/capabilities compare against MongoDb?";"OptimusPrime3600";"Reddit"
"⚡️ Dynatron - Bridge between AWS DynamoDB Document Client and Real World usage";"2020-08-10 22:43:39";"https://www.reddit.com/r/nosql/comments/i7d6ky/dynatron_bridge_between_aws_dynamodb_document/";"This library is a result of years of working with AWS DynamoDB and overcoming underwater rocks, missing optimizations and hidden issues that are very hard to catch (like hanging SSL connections in 0.2% of cases).  Homepage - [https://93v.github.io/dynatron/](https://93v.github.io/dynatron/)  Github - [https://github.com/93v/dynatron](https://github.com/93v/dynatron)  NPM Package - [https://www.npmjs.com/package/dynatron](https://www.npmjs.com/package/dynatron)";"gevorggalstyan";"Reddit"
"Best free apps to model NoSQL database on for assignment.";"2020-08-04 01:05:06";"https://www.reddit.com/r/nosql/comments/i37s0g/best_free_apps_to_model_nosql_database_on_for/";"Hey,  I'm doing an assignment for university just hoping that someone where would be able to tell me what good free software is out there for modelling nosql.  Thank you!";"pixie-warrior";"Reddit"
"Resources on NoSQL(MongoDb) database modeling ?";"2020-07-27 15:38:38";"https://www.reddit.com/r/nosql/comments/hysfas/resources_on_nosqlmongodb_database_modeling/";"I have been working with RDBMS until now. I am not supposed to design database in MongoDB. I found a lot of good resources to learn MongoDb (querying etc) but I am looking for good resources to get the hang of database modeling.   May be some real world industry examples?";"OptimusPrime3600";"Reddit"
"The Complete DynamoDB bootcamp - free for 3 days";"2020-07-19 13:29:43";"https://www.reddit.com/r/nosql/comments/htz12b/the_complete_dynamodb_bootcamp_free_for_3_days/";" Unlimited redemption, but valid only for 3 days. This is a special offer of a paid course that I'm offering for 3 days for free as a gift to the community. Feel free to share if you want   [https://www.udemy.com/course/the-complete-dynamodb-bootcamp/?couponCode=E6F7763676D26D8CF43F](https://www.udemy.com/course/the-complete-dynamodb-bootcamp/?couponCode=E6F7763676D26D8CF43F)  Have you ever wondered if there is a course to help you learn the basics of DynamoDB quickly without getting bogged down in details? This is the course for you then! We cover all the basics of NoSQL and DynamoDB and even take on a few advanced topics - all in about 3 hours. Join in and you'll end up being a confident DynamoDB user at the end!   Full disclosure - the course doesn't have a great rating partly due to the fact that it has only gathered 12 ratings, but you can check out my profile @  [https://www.udemy.com/user/rammohan4/](https://www.udemy.com/user/rammohan4/)  to understand that most of my courses are rated at 4+ out of 5 stars.";"rmohan80";"Reddit"
"Delete old documents";"2020-06-25 12:53:22";"https://www.reddit.com/r/nosql/comments/hfk22k/delete_old_documents/";"Quite an interesting case. I have an enormous MongoDB collection with lots of documents. These are two of the fields ( I changed the field names).      {     ""pidNumber"" : NumberLong(12103957251),      ""eventDate"" : ISODate(""2018-05-15T00:00:00.000+0000"")     }   I need to count all the instances where the date is older than 1 year but ONLY if there's a more recent document with the same pidNumber.   So for example: If there's only one document with pidNumber 1234 and it's from three years ago - keep it (don't count). But if on top of that there's another document with pidNumber 1234 and it's from two years ago - then count the three years old one.  Is it possible to do? Does anyone have on how to do it?  Thanks ahead!";"HeadTea";"Reddit"
"data organization";"2020-06-22 16:35:13";"https://www.reddit.com/r/nosql/comments/hdtg4g/data_organization/";"I was wondering how social media apps and fitness apps organize their database to minimize costs and minimize load time for the user. There should be a collection for users and posts or workouts but should there be any data duplication? I’m worried about having to read a ton of post documents when a user opens up their feed. Any tips?";"makesmthnew";"Reddit"
"Hash and range for simple table";"2020-05-26 22:27:02";"https://www.reddit.com/r/nosql/comments/gr4xis/hash_and_range_for_simple_table/";"Hello,  I have a simple table that takes in numeric data sampled over time for a device.  So something like:  * Date * Time * Device * Data1 * Data2  Technically, device is optional as there is only one device at the moment, but I might add more at some point in the future. Remembering my dim and distant past, I think I would probably use date/time/device as a composite key in a relational DB, but I am not quite sure what I should select here?  Could it be device as the Hash and Date and Time as ranges?     EDIT: Ah, only one range attribute..";"quarky_uk";"Reddit"
"Hierarchical Dirichlet Process and Relative Entropy";"2022-10-24T11:56:06Z";"http://arxiv.org/abs/2210.13142v1";"The Hierarchical Dirichlet process is a discrete random measure serving as an important prior in Bayesian non-parametrics. It is motivated with the study of groups of clustered data. Each group is modelled through a level two Dirichlet process and all groups share the same base distribution which itself is a drawn from a level one Dirichlet process. It has two concentration parameters with one at each level. The main results of the paper are the law of large numbers and large deviations for the hierarchical Dirichlet process and its mass when both concentration parameters converge to infinity. The large deviation rate functions are identified explicitly. The rate function for the hierarchical Dirichlet process consists of two terms corresponding to the relative entropies at each level. It is less than the rate function for the Dirichlet process, which reflects the fact that the number of clusters under the hierarchical Dirichlet process has a slower growth rate than under the Dirichlet process.";"Shui Feng";"Arxiv"
"Spectral analysis of communication networks using Dirichlet eigenvalues";"2011-02-17T23:59:16Z";"http://arxiv.org/abs/1102.3722v2";"The spectral gap of the graph Laplacian with Dirichlet boundary conditions is computed for the graphs of several communication networks at the IP-layer, which are subgraphs of the much larger global IP-layer network. We show that the Dirichlet spectral gap of these networks is substantially larger than the standard spectral gap and is likely to remain non-zero in the infinite graph limit. We first prove this result for finite regular trees, and show that the Dirichlet spectral gap in the infinite tree limit converges to the spectral gap of the infinite tree. We also perform Dirichlet spectral clustering on the IP-layer networks and show that it often yields cuts near the network core that create genuine single-component clusters. This is much better than traditional spectral clustering where several disjoint fragments near the periphery are liable to be misleadingly classified as a single cluster. Spectral clustering is often used to identify bottlenecks or congestion; since congestion in these networks is known to peak at the core, our results suggest that Dirichlet spectral clustering may be better at finding bona-fide bottlenecks.";"Alexander Tsiatas, Iraj Saniee, Onuttom Narayan, Matthew Andrews";"Arxiv"
"Dirichlet Process Mixtures of Generalized Mallows Models";"2012-03-15T11:17:56Z";"http://arxiv.org/abs/1203.3496v1";"We present a Dirichlet process mixture model over discrete incomplete rankings and study two Gibbs sampling inference techniques for estimating posterior clusterings. The first approach uses a slice sampling subcomponent for estimating cluster parameters. The second approach marginalizes out several cluster parameters by taking advantage of approximations to the conditional posteriors. We empirically demonstrate (1) the effectiveness of this approximation for improving convergence, (2) the benefits of the Dirichlet process model over alternative clustering techniques for ranked data, and (3) the applicability of the approach to exploring large realworld ranking datasets.";"Marina Meila, Harr Chen";"Arxiv"
"Clustering consistency with Dirichlet process mixtures";"2022-05-25T17:21:42Z";"http://dx.doi.org/10.1093/biomet/asac051";"Dirichlet process mixtures are flexible non-parametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.";"Filippo Ascolani, Antonio Lijoi, Giovanni Rebaudo, Giacomo Zanella";"Arxiv"
"A Hierarchical Dirichlet Process Model with Multiple Levels of   Clustering for Human EEG Seizure Modeling";"2012-06-18T15:02:12Z";"http://arxiv.org/abs/1206.4616v1";"Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process---the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We find the MLC-HDP's clustering to be comparable to independent human physician clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.";"Drausin Wulsin, Shane Jensen, Brian Litt";"Arxiv"
"The supervised hierarchical Dirichlet process";"2014-12-17T01:16:31Z";"http://dx.doi.org/10.1109/TPAMI.2014.2315802";"We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.";"Andrew M. Dai, Amos J. Storkey";"Arxiv"
"Scalable Inference for Latent Dirichlet Allocation";"2009-09-25T05:23:33Z";"http://arxiv.org/abs/0909.4603v1";"We investigate the problem of learning a topic model - the well-known Latent Dirichlet Allocation - in a distributed manner, using a cluster of C processors and dividing the corpus to be learned equally among them. We propose a simple approximated method that can be tuned, trading speed for accuracy according to the task at hand. Our approach is asynchronous, and therefore suitable for clusters of heterogenous machines.";"James Petterson, Tiberio Caetano";"Arxiv"
"Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process   Mixture";"2013-05-28T23:59:16Z";"http://arxiv.org/abs/1305.6659v2";"This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.";"Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin";"Arxiv"
"Dirichlet-tree multinomial mixtures for clustering microbiome   compositions";"2020-08-02T05:17:02Z";"http://arxiv.org/abs/2008.00400v2";"Studying the human microbiome has gained substantial interest in recent years, and a common task in the analysis of these data is to cluster microbiome compositions into subtypes. This subdivision of samples into subgroups serves as an intermediary step in achieving personalized diagnosis and treatment. In applying existing clustering methods to modern microbiome studies including the American Gut Project (AGP) data, we found that this seemingly standard task, however, is very challenging in the microbiome composition context due to several key features of such data. Standard distance-based clustering algorithms generally do not produce reliable results as they do not take into account the heterogeneity of the cross-sample variability among the bacterial taxa, while existing model-based approaches do not allow sufficient flexibility for the identification of complex within-cluster variation from cross-cluster variation. Direct applications of such methods generally lead to overly dispersed clusters in the AGP data and such phenomenon is common for other microbiome data. To overcome these challenges, we introduce Dirichlet-tree multinomial mixtures (DTMM) as a Bayesian generative model for clustering amplicon sequencing data in microbiome studies. DTMM models the microbiome population with a mixture of Dirichlet-tree kernels that utilizes the phylogenetic tree to offer a more flexible covariance structure in characterizing within-cluster variation, and it provides a means for identifying a subset of signature taxa that distinguish the clusters. We perform extensive simulation studies to evaluate the performance of DTMM and compare it to state-of-the-art model-based and distance-based clustering methods in the microbiome context. Finally, we report a case study on the fecal data from the AGP to identify compositional clusters among individuals with inflammatory bowel disease and diabetes.";"Jialiang Mao, Li Ma";"Arxiv"
"Percolation Perturbations in Potential Theory and Random Walks";"1998-04-02T10:59:37Z";"http://arxiv.org/abs/math/9804010v1";"We show that on a Cayley graph of a nonamenable group, almost surely the infinite clusters of Bernoulli percolation are transient for simple random walk, that simple random walk on these clusters has positive speed, and that these clusters admit bounded harmonic functions. A principal new finding on which these results are based is that such clusters admit invariant random subgraphs with positive isoperimetric constant.   We also show that percolation clusters in any amenable Cayley graph almost surely admit no nonconstant harmonic Dirichlet functions. Conversely, on a Cayley graph admitting nonconstant harmonic Dirichlet functions, almost surely the infinite clusters of $p$-Bernoulli percolation also have nonconstant harmonic Dirichlet functions when $p$ is sufficiently close to 1. Many conjectures and questions are presented.";"Itai Benjamini, Russell Lyons, Oded Schramm";"Arxiv"
"Numerical conformal mapping with rational functions";"2019-11-09T14:16:35Z";"http://arxiv.org/abs/1911.03696v1";"New algorithms are presented for numerical conformal mapping based on rational approximations and the solution of Dirichlet problems by least-squares fitting on the boundary. The methods are targeted at regions with corners, where the Dirichlet problem is solved by the ""lightning Laplace solver"" with poles exponentially clustered near each singularity. For polygons and circular polygons, further simplifications are possible.";"Lloyd N. Trefethen";"Arxiv"
"Revisiting k-means: New Algorithms via Bayesian Nonparametrics";"2011-11-02T00:09:18Z";"http://arxiv.org/abs/1111.0352v2";"Bayesian models offer great flexibility for clustering applications---Bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical Bayesian models can be utilized for sharing clusters across multiple data sets. For the most part, such flexibility is lacking in classical clustering methods such as k-means. In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired by the asymptotic connection between k-means and mixtures of Gaussians, we show that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph.";"Brian Kulis, Michael I. Jordan";"Arxiv"
"Bayesian Clustering of Transcription Factor Binding Motifs";"2006-10-22T03:38:20Z";"http://arxiv.org/abs/math/0610655v1";"Genes are often regulated in living cells by proteins called transcription factors (TFs) that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based upon a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our clusters provide a means by which to organize transcription factors based on binding motif similarities, which can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC. Finally, our clustering procedure has been used in combination with discovery of evolutionarily-conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is explored but shows no substantive difference in the clustering results for our datasets. Our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications.";"Shane T. Jensen, Jun S. Liu";"Arxiv"
"DIMM-SC: A Dirichlet mixture model for clustering droplet-based single   cell transcriptomic data";"2017-04-06T20:01:29Z";"http://arxiv.org/abs/1704.02007v1";"Motivation: Single cell transcriptome sequencing (scRNA-Seq) has become a revolutionary tool to study cellular and molecular processes at single cell resolution. Among existing technologies, the recently developed droplet-based platform enables efficient parallel processing of thousands of single cells with direct counting of transcript copies using Unique Molecular Identifier (UMI). Despite the technology advances, statistical methods and computational tools are still lacking for analyzing droplet-based scRNA-Seq data. Particularly, model-based approaches for clustering large-scale single cell transcriptomic data are still under-explored. Methods: We developed DIMM-SC, a Dirichlet Mixture Model for clustering droplet-based Single Cell transcriptomic data. This approach explicitly models UMI count data from scRNA-Seq experiments and characterizes variations across different cell clusters via a Dirichlet mixture prior. An expectation-maximization algorithm is used for parameter inference. Results: We performed comprehensive simulations to evaluate DIMM-SC and compared it with existing clustering methods such as K-means, CellTree and Seurat. In addition, we analyzed public scRNA-Seq datasets with known cluster labels and in-house scRNA-Seq datasets from a study of systemic sclerosis with prior biological knowledge to benchmark and validate DIMM-SC. Both simulation studies and real data applications demonstrated that overall, DIMM-SC achieves substantially improved clustering accuracy and much lower clustering variability compared to other existing clustering methods. More importantly, as a model-based approach, DIMM-SC is able to quantify the clustering uncertainty for each single cell, facilitating rigorous statistical inference and biological interpretations, which are typically unavailable from existing clustering methods.";"Zhe Sun, Ting Wang, Ke Deng, Xiao-Feng Wang, Robert Lafyatis, Ying Ding, Ming Hu, Wei Chen";"Arxiv"
"Consistency Analysis for the Doubly Stochastic Dirichlet Process";"2016-05-24T10:13:19Z";"http://arxiv.org/abs/1605.07358v1";"This technical report proves components consistency for the Doubly Stochastic Dirichlet Process with exponential convergence of posterior probability. We also present the fundamental properties for DSDP as well as inference algorithms. Simulation toy experiment and real-world experiment results for single and multi-cluster also support the consistency proof. This report is also a support document for the paper ""Computationally Efficient Hyperspectral Data Learning Based on the Doubly Stochastic Dirichlet Process"".";"Xing Sun, Nelson H. C. Yung, Edmund Y. Lam, Hayden K. -H. So";"Arxiv"
"Powered Dirichlet Process for Controlling the Importance of   ""Rich-Get-Richer"" Prior Assumptions in Bayesian Clustering";"2021-04-26T11:36:23Z";"http://arxiv.org/abs/2104.12485v1";"One of the most used priors in Bayesian clustering is the Dirichlet prior. It can be expressed as a Chinese Restaurant Process. This process allows nonparametric estimation of the number of clusters when partitioning datasets. Its key feature is the ""rich-get-richer"" property, which assumes a cluster has an a priori probability to get chosen linearly dependent on population. In this paper, we show that such prior is not always the best choice to model data. We derive the Powered Chinese Restaurant process from a modified version of the Dirichlet-Multinomial distribution to answer this problem. We then develop some of its fundamental properties (expected number of clusters, convergence). Unlike state-of-the-art efforts in this direction, this new formulation allows for direct control of the importance of the ""rich-get-richer"" prior.";"Gaël Poux-Médard, Julien Velcin, Sabine Loudcher";"Arxiv"
"Flexible clustering via hidden hierarchical Dirichlet priors";"2022-01-18T13:59:38Z";"http://dx.doi.org/10.1111/sjos.12578";"The Bayesian approach to inference stands out for naturally allowing borrowing information across heterogeneous populations, with different samples possibly sharing the same distribution. A popular Bayesian nonparametric model for clustering probability distributions is the nested Dirichlet process, which however has the drawback of grouping distributions in a single cluster when ties are observed across samples. With the goal of achieving a flexible and effective clustering method for both samples and observations, we investigate a nonparametric prior that arises as the composition of two different discrete random structures and derive a closed-form expression for the induced distribution of the random partition, the fundamental tool regulating the clustering behavior of the model. On the one hand, this allows to gain a deeper insight into the theoretical properties of the model and, on the other hand, it yields an MCMC algorithm for evaluating Bayesian inferences of interest. Moreover, we single out limitations of this algorithm when working with more than two populations and, consequently, devise an alternative more efficient sampling scheme, which as a by-product, allows testing homogeneity between different populations. Finally, we perform a comparison with the nested Dirichlet process and provide illustrative examples of both synthetic and real data.";"Antonio Lijoi, Igor Prünster, Giovanni Rebaudo";"Arxiv"
"Bayesian mixture models (in)consistency for the number of clusters";"2022-10-25T17:47:29Z";"http://arxiv.org/abs/2210.14201v1";"Bayesian nonparametric mixture models are common for modeling complex data. While these models are well-suited for density estimation, their application for clustering has some limitations. Miller and Harrison (2014) proved posterior inconsistency in the number of clusters when the true number of clusters is finite for Dirichlet process and Pitman--Yor process mixture models. In this work, we extend this result to additional Bayesian nonparametric priors such as Gibbs-type processes and finite-dimensional representations of them. The latter include the Dirichlet multinomial process and the recently proposed Pitman--Yor and normalized generalized gamma multinomial processes. We show that mixture models based on these processes are also inconsistent in the number of clusters and discuss possible solutions. Notably, we show that a post-processing algorithm introduced by Guha et al. (2021) for the Dirichlet process extends to more general models and provides a consistent method to estimate the number of components.";"Louise Alamichel, Daria Bystrova, Julyan Arbel, Guillaume Kon Kam King";"Arxiv"
"A Bayesian View of the Poisson-Dirichlet Process";"2010-07-02T05:10:49Z";"http://arxiv.org/abs/1007.0296v2";"The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the Dirichlet Process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. There is a rich literature about the PDP and its derivative distributions such as the Chinese Restaurant Process (CRP). This article reviews some of the basic theory and then the major results needed for Bayesian modelling of discrete problems including details of priors, posteriors and computation.   The PDP allows one to build distributions over countable partitions. The PDP has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of PDPs, and second using a marginalised relative the CRP, one gets fragmentation and clustering properties that lets one layer partitions to build trees. This article presents the basic theory for understanding the notion of partitions and distributions over them, the PDP and the CRP, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. This article also presents a Bayesian interpretation of the Poisson-Dirichlet process based on an improper and infinite dimensional Dirichlet distribution. This means we can understand the process as just another Dirichlet and thus all its sampling properties emerge naturally.   The theory of PDPs is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. This context and basic results are also presented, as well as techniques for computing the second order Stirling numbers that occur in the posteriors for discrete distributions.";"Wray Buntine, Marcus Hutter";"Arxiv"
"From here to infinity - sparse finite versus Dirichlet process mixtures   in model-based clustering";"2017-06-22T07:44:23Z";"http://arxiv.org/abs/1706.07194v3";"In model-based-clustering mixture models are used to group data points into clusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli et al (2016) are sparse finite mixtures, where the prior distribution on the weight distribution of a mixture with $K$ components is chosen in such a way that a priori the number of clusters in the data is random and is allowed to be smaller than $K$ with high probability. The number of cluster is then inferred a posteriori from the data.   The present paper makes the following contributions in the context of sparse finite mixture modelling. First, it is illustrated that the concept of sparse finite mixture is very generic and easily extended to cluster various types of non-Gaussian data, in particular discrete data and continuous multivariate data arising from non-Gaussian clusters. Second, sparse finite mixtures are compared to Dirichlet process mixtures with respect to their ability to identify the number of clusters. For both model classes, a random hyper prior is considered for the parameters determining the weight distribution. By suitable matching of these priors, it is shown that the choice of this hyper prior is far more influential on the cluster solution than whether a sparse finite mixture or a Dirichlet process mixture is taken into consideration.";"Sylvia Frühwirth-Schnatter, Gertraud Malsiner-Walli";"Arxiv"
"Flexible Priors for Exemplar-based Clustering";"2012-06-13T15:52:35Z";"http://arxiv.org/abs/1206.3294v1";"Exemplar-based clustering methods have been shown to produce state-of-the-art results on a number of synthetic and real-world clustering problems. They are appealing because they offer computational benefits over latent-mean models and can handle arbitrary pairwise similarity measures between data points. However, when trying to recover underlying structure in clustering problems, tailored similarity measures are often not enough; we also desire control over the distribution of cluster sizes. Priors such as Dirichlet process priors allow the number of clusters to be unspecified while expressing priors over data partitions. To our knowledge, they have not been applied to exemplar-based models. We show how to incorporate priors, including Dirichlet process priors, into the recently introduced affinity propagation algorithm. We develop an efficient maxproduct belief propagation algorithm for our new model and demonstrate experimentally how the expanded range of clustering priors allows us to better recover true clusterings in situations where we have some information about the generating process.";"Daniel Tarlow, Richard S. Zemel, Brendan J. Frey";"Arxiv"
"Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge   Sampling on Dirichlet Process Mixtures";"2018-12-25T06:14:25Z";"http://dx.doi.org/10.1093/bioinformatics/bty702";"Motivation: With the development of droplet based systems, massive single cell transcriptome data has become available, which enables analysis of cellular and molecular processes at single cell resolution and is instrumental to understanding many biological processes. While state-of-the-art clustering methods have been applied to the data, they face challenges in the following aspects: (1) the clustering quality still needs to be improved; (2) most models need prior knowledge on number of clusters, which is not always available; (3) there is a demand for faster computational speed. Results: We propose to tackle these challenges with Parallel Split Merge Sampling on Dirichlet Process Mixture Model (the Para-DPMM model). Unlike classic DPMM methods that perform sampling on each single data point, the split merge mechanism samples on the cluster level, which significantly improves convergence and optimality of the result. The model is highly parallelized and can utilize the computing power of high performance computing (HPC) clusters, enabling massive clustering on huge datasets. Experiment results show the model outperforms current widely used models in both clustering quality and computational speed. Availability: Source code is publicly available on https://github.com/tiehangd/Para_DPMM/tree/master/Para_DPMM_package";"Tiehang Duan, José P. Pinto, Xiaohui Xie";"Arxiv"
"Hierarchical Latent Word Clustering";"2016-01-20T23:31:58Z";"http://arxiv.org/abs/1601.05472v1";"This paper presents a new Bayesian non-parametric model by extending the usage of Hierarchical Dirichlet Allocation to extract tree structured word clusters from text data. The inference algorithm of the model collects words in a cluster if they share similar distribution over documents. In our experiments, we observed meaningful hierarchical structures on NIPS corpus and radiology reports collected from public repositories.";"Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa";"Arxiv"
"An efficient algorithm for solving elliptic problems on percolation   clusters";"2019-07-31T15:58:28Z";"http://arxiv.org/abs/1907.13571v1";"We present an efficient algorithm to solve elliptic Dirichlet problems defined on the cluster of $\mathbb{Z}^d$ supercritical Bernoulli percolation, as a generalization of the iterative method proposed by S. Armstrong, A. Hannukainen, T. Kuusi and J.-C. Mourrat. We also explore the two-scale expansion on the infinite cluster of percolation, and use it to give a rigorous analysis of the algorithm.";"Chenlin Gu";"Arxiv"
"Colouring and breaking sticks: random distributions and heterogeneous   clustering";"2010-03-21T09:48:06Z";"http://arxiv.org/abs/1003.3988v1";"We begin by reviewing some probabilistic results about the Dirichlet Process and its close relatives, focussing on their implications for statistical modelling and analysis. We then introduce a class of simple mixture models in which clusters are of different `colours', with statistical characteristics that are constant within colours, but different between colours. Thus cluster identities are exchangeable only within colours. The basic form of our model is a variant on the familiar Dirichlet process, and we find that much of the standard modelling and computational machinery associated with the Dirichlet process may be readily adapted to our generalisation. The methodology is illustrated with an application to the partially-parametric clustering of gene expression profiles.";"Peter J. Green";"Arxiv"
"Beta-Product Poisson-Dirichlet Processes";"2011-09-22T11:32:31Z";"http://arxiv.org/abs/1109.4777v1";"Time series data may exhibit clustering over time and, in a multiple time series context, the clustering behavior may differ across the series. This paper is motivated by the Bayesian non--parametric modeling of the dependence between the clustering structures and the distributions of different time series. We follow a Dirichlet process mixture approach and introduce a new class of multivariate dependent Dirichlet processes (DDP). The proposed DDP are represented in terms of vector of stick-breaking processes with dependent weights. The weights are beta random vectors that determine different and dependent clustering effects along the dimension of the DDP vector. We discuss some theoretical properties and provide an efficient Monte Carlo Markov Chain algorithm for posterior computation. The effectiveness of the method is illustrated with a simulation study and an application to the United States and the European Union industrial production indexes.";"Federico Bassetti, Roberto Casarin, Fabrizio Leisen";"Arxiv"
"Conjoined Dirichlet Process";"2020-02-08T19:41:23Z";"http://arxiv.org/abs/2002.03223v1";"Biclustering is a class of techniques that simultaneously clusters the rows and columns of a matrix to sort heterogeneous data into homogeneous blocks. Although many algorithms have been proposed to find biclusters, existing methods suffer from the pre-specification of the number of biclusters or place constraints on the model structure. To address these issues, we develop a novel, non-parametric probabilistic biclustering method based on Dirichlet processes to identify biclusters with strong co-occurrence in both rows and columns. The proposed method utilizes dual Dirichlet process mixture models to learn row and column clusters, with the number of resulting clusters determined by the data rather than pre-specified. Probabilistic biclusters are identified by modeling the mutual dependence between the row and column clusters. We apply our method to two different applications, text mining and gene expression analysis, and demonstrate that our method improves bicluster extraction in many settings compared to existing approaches.";"Michelle N. Ngo, Dustin S. Pluta, Alexander N. Ngo, Babak Shahbaba";"Arxiv"
"On the Variational Posterior of Dirichlet Process Deep Latent Gaussian   Mixture Models";"2020-06-16T08:46:18Z";"http://arxiv.org/abs/2006.08993v2";"Thanks to the reparameterization trick, deep latent Gaussian models have shown tremendous success recently in learning latent representations. The ability to couple them however with nonparamet-ric priors such as the Dirichlet Process (DP) hasn't seen similar success due to its non parameteriz-able nature. In this paper, we present an alternative treatment of the variational posterior of the Dirichlet Process Deep Latent Gaussian Mixture Model (DP-DLGMM), where we show that the prior cluster parameters and the variational posteriors of the beta distributions and cluster hidden variables can be updated in closed-form. This leads to a standard reparameterization trick on the Gaussian latent variables knowing the cluster assignments. We demonstrate our approach on standard benchmark datasets, we show that our model is capable of generating realistic samples for each cluster obtained, and manifests competitive performance in a semi-supervised setting.";"Amine Echraibi, Joachim Flocon-Cholet, Stéphane Gosselin, Sandrine Vaton";"Arxiv"
"Powered Hawkes-Dirichlet Process: Challenging Textual Clustering using a   Flexible Temporal Prior";"2021-09-15T09:10:19Z";"http://arxiv.org/abs/2109.07170v1";"The textual content of a document and its publication date are intertwined. For example, the publication of a news article on a topic is influenced by previous publications on similar issues, according to underlying temporal dynamics. However, it can be challenging to retrieve meaningful information when textual information conveys little information or when temporal dynamics are hard to unveil. Furthermore, the textual content of a document is not always linked to its temporal dynamics. We develop a flexible method to create clusters of textual documents according to both their content and publication time, the Powered Dirichlet-Hawkes process (PDHP). We show PDHP yields significantly better results than state-of-the-art models when temporal information or textual content is weakly informative. The PDHP also alleviates the hypothesis that textual content and temporal dynamics are always perfectly correlated. PDHP allows retrieving textual clusters, temporal clusters, or a mixture of both with high accuracy when they are not. We demonstrate that PDHP generalizes previous work --such as the Dirichlet-Hawkes process (DHP) and Uniform process (UP). Finally, we illustrate the changes induced by PDHP over DHP and UP in a real-world application using Reddit data.";"Gaël Poux-Médard, Julien Velcin, Sabine Loudcher";"Arxiv"
"Nonparametric Variable Selection, Clustering and Prediction for   High-Dimensional Regression";"2014-07-21T12:41:32Z";"http://arxiv.org/abs/1407.5472v3";"The development of parsimonious models for reliable inference and prediction of responses in high-dimensional regression settings is often challenging due to relatively small sample sizes and the presence of complex interaction patterns between a large number of covariates. We propose an efficient, nonparametric framework for simultaneous variable selection, clustering and prediction in high-throughput regression settings with continuous or discrete outcomes, called VariScan. The VariScan model utilizes the sparsity induced by Poisson-Dirichlet processes (PDPs) to group the covariates into lower-dimensional latent clusters consisting of covariates with similar patterns among the samples. The data are permitted to direct the choice of a suitable cluster allocation scheme, choosing between PDPs and their special case, a Dirichlet process. Subsequently, the latent clusters are used to build a nonlinear prediction model for the responses using an adaptive mixture of linear and nonlinear elements, thus achieving a balance between model parsimony and flexibility. We investigate theoretical properties of the VariScan procedure that differentiate the allocations patterns of PDPs and Dirichlet processes both in terms of the number and relative sizes of their clusters. Additional theoretical results guarantee the high accuracy of the model-based clustering procedure, and establish model selection and prediction consistency. Through simulation studies and analyses of benchmark data sets, we demonstrate the reliability of VariScan's clustering mechanism and show that the technique compares favorably to, and often outperforms, existing methodologies in terms of the prediction accuracies of the subject-specific responses.";"Subharup Guha, Veerabhadran Baladandayuthapani";"Arxiv"
"Invariant Percolation and Harmonic Dirichlet Functions";"2004-05-24T16:55:27Z";"http://arxiv.org/abs/math/0405458v3";"The main goal of this paper is to answer question 1.10 and settle conjecture 1.11 of Benjamini-Lyons-Schramm [BLS99] relating harmonic Dirichlet functions on a graph to those of the infinite clusters in the uniqueness phase of Bernoulli percolation. We extend the result to more general invariant percolations, including the Random-Cluster model. We prove the existence of the nonuniqueness phase for the Bernoulli percolation (and make some progress for Random-Cluster model) on unimodular transitive locally finite graphs admitting nonconstant harmonic Dirichlet functions. This is done by using the device of $\ell^2$ Betti numbers.";"Damien Gaboriau";"Arxiv"
"Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts";"2014-01-09T12:08:07Z";"http://arxiv.org/abs/1401.1974v4";"We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.";"Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui";"Arxiv"
"Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick   Breaking Representation";"2012-06-27T16:21:35Z";"http://arxiv.org/abs/1206.6845v1";"Nonparametric Bayesian approaches to clustering, information retrieval, language modeling and object recognition have recently shown great promise as a new paradigm for unsupervised data analysis. Most contributions have focused on the Dirichlet process mixture models or extensions thereof for which efficient Gibbs samplers exist. In this paper we explore Gibbs samplers for infinite complexity mixture models in the stick breaking representation. The advantage of this representation is improved modeling flexibility. For instance, one can design the prior distribution over cluster sizes or couple multiple infinite mixture models (e.g. over time) at the level of their parameters (i.e. the dependent Dirichlet process model). However, Gibbs samplers for infinite mixture models (as recently introduced in the statistics literature) seem to mix poorly over cluster labels. Among others issues, this can have the adverse effect that labels for the same cluster in coupled mixture models are mixed up. We introduce additional moves in these samplers to improve mixing over cluster labels and to bring clusters into correspondence. An application to modeling of storm trajectories is used to illustrate these ideas.";"Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling";"Arxiv"
"Functional clustering in nested designs: Modeling variability in   reproductive epidemiology studies";"2014-11-20T11:43:28Z";"http://dx.doi.org/10.1214/14-AOAS751";"We discuss functional clustering procedures for nested designs, where multiple curves are collected for each subject in the study. We start by considering the application of standard functional clustering tools to this problem, which leads to groupings based on the average profile for each subject. After discussing some of the shortcomings of this approach, we present a mixture model based on a generalization of the nested Dirichlet process that clusters subjects based on the distribution of their curves. By using mixtures of generalized Dirichlet processes, the model induces a much more flexible prior on the partition structure than other popular model-based clustering methods, allowing for different rates of introduction of new clusters as the number of observations increases. The methods are illustrated using hormone profiles from multiple menstrual cycles collected for women in the Early Pregnancy Study.";"Abel Rodriguez, David B. Dunson";"Arxiv"
"Posterior Distribution for the Number of Clusters in Dirichlet Process   Mixture Models";"2019-05-23T22:51:15Z";"http://arxiv.org/abs/1905.09959v2";"Dirichlet process mixture models (DPMM) play a central role in Bayesian nonparametrics, with applications throughout statistics and machine learning. DPMMs are generally used in clustering problems where the number of clusters is not known in advance, and the posterior distribution is treated as providing inference for this number. Recently, however, it has been shown that the DPMM is inconsistent in inferring the true number of components in certain cases. This is an asymptotic result, and it would be desirable to understand whether it holds with finite samples, and to more fully understand the full posterior. In this work, we provide a rigorous study for the posterior distribution of the number of clusters in DPMM under different prior distributions on the parameters and constraints on the distributions of the data. We provide novel lower bounds on the ratios of probabilities between $s+1$ clusters and $s$ clusters when the prior distributions on parameters are chosen to be Gaussian or uniform distributions.";"Chiao-Yu Yang, Eric Xia, Nhat Ho, Michael I. Jordan";"Arxiv"
"Bayesian nonparametric temporal dynamic clustering via autoregressive   Dirichlet priors";"2019-10-23T10:15:26Z";"http://arxiv.org/abs/1910.10443v1";"In this paper we consider the problem of dynamic clustering, where cluster memberships may change over time and clusters may split and merge over time, thus creating new clusters and destroying existing ones. We propose a Bayesian nonparametric approach to dynamic clustering via mixture modeling. Our approach relies on a novel time-dependent nonparametric prior defined by combining: i) a copula-based transformation of a Gaussian autoregressive process; ii) the stick-breaking construction of the Dirichlet process. Posterior inference is performed through a particle Markov chain Monte Carlo algorithm which is simple, computationally efficient and scalable to massive datasets. Advantages of the proposed approach include flexibility in applications, ease of computations and interpretability. We present an application of our dynamic Bayesian nonparametric mixture model to the study the temporal dynamics of gender stereotypes in adjectives and occupations in the 20th and 21st centuries in the United States. Moreover, to highlight the flexibility of our model we present additional applications to time-dependent data with covariates and with spatial structure.";"Maria De Iorio, Stefano Favaro, Alessandra Guglielmi, Lifeng Ye";"Arxiv"
"Topic Detection from Conversational Dialogue Corpus with Parallel   Dirichlet Allocation Model and Elbow Method";"2020-06-05T10:24:43Z";"http://arxiv.org/abs/2006.03353v1";"A conversational system needs to know how to switch between topics to continue the conversation for a more extended period. For this topic detection from dialogue corpus has become an important task for a conversation and accurate prediction of conversation topics is important for creating coherent and engaging dialogue systems. In this paper, we proposed a topic detection approach with Parallel Latent Dirichlet Allocation (PLDA) Model by clustering a vocabulary of known similar words based on TF-IDF scores and Bag of Words (BOW) technique. In the experiment, we use K-mean clustering with Elbow Method for interpretation and validation of consistency within-cluster analysis to select the optimal number of clusters. We evaluate our approach by comparing it with traditional LDA and clustering technique. The experimental results show that combining PLDA with Elbow method selects the optimal number of clusters and refine the topics for the conversation.";"Haider Khalid, Vincent Wade";"Arxiv"
"Joint Clustering and Registration of Functional Data";"2014-03-27T17:17:25Z";"http://arxiv.org/abs/1403.7134v1";"Curve registration and clustering are fundamental tools in the analysis of functional data. While several methods have been developed and explored for either task individually, limited work has been done to infer functional clusters and register curves simultaneously. We propose a hierarchical model for joint curve clustering and registration. Our proposal combines a Dirichlet process mixture model for clustering of common shapes, with a reproducing kernel representation of phase variability for registration. We show how inference can be carried out applying standard posterior simulation algorithms and compare our method to several alternatives in both engineered data and a benchmark analysis of the Berkeley growth data. We conclude our investigation with an application to time course gene expression.";"Yafeng Zhang, Donatello Telesca";"Arxiv"
"Heterogeneous Regression Models for Clusters of Spatial Dependent Data";"2019-07-04T04:13:36Z";"http://dx.doi.org/10.1080/17421772.2020.1784989";"In economic development, there are often regions that share similar economic characteristics, and economic models on such regions tend to have similar covariate effects. In this paper, we propose a Bayesian clustered regression for spatially dependent data in order to detect clusters in the covariate effects. Our proposed method is based on the Dirichlet process which provides a probabilistic framework for simultaneous inference of the number of clusters and the clustering configurations. The usage of our method is illustrated both in simulation studies and an application to a housing cost dataset of Georgia.";"Zhihua Ma, Yishu Xue, Guanyu Hu";"Arxiv"
"Dirichlet Fragmentation Processes";"2015-09-16T01:07:24Z";"http://arxiv.org/abs/1509.04781v1";"Tree structures are ubiquitous in data across many domains, and many datasets are naturally modelled by unobserved tree structures. In this paper, first we review the theory of random fragmentation processes [Bertoin, 2006], and a number of existing methods for modelling trees, including the popular nested Chinese restaurant process (nCRP). Then we define a general class of probability distributions over trees: the Dirichlet fragmentation process (DFP) through a novel combination of the theory of Dirichlet processes and random fragmentation processes. This DFP presents a stick-breaking construction, and relates to the nCRP in the same way the Dirichlet process relates to the Chinese restaurant process. Furthermore, we develop a novel hierarchical mixture model with the DFP, and empirically compare the new model to similar models in machine learning. Experiments show the DFP mixture model to be convincingly better than existing state-of-the-art approaches for hierarchical clustering and density modelling.";"Hong Ge, Yarin Gal, Zoubin Ghahramani";"Arxiv"
"The semi-hierarchical Dirichlet Process and its application to   clustering homogeneous distributions";"2020-05-20T18:10:13Z";"http://arxiv.org/abs/2005.10287v4";"Assessing homogeneity of distributions is an old problem that has received considerable attention, especially in the nonparametric Bayesian literature. To this effect, we propose the semi-hierarchical Dirichlet process, a novel hierarchical prior that extends the hierarchical Dirichlet process of Teh et al. (2006) and that avoids the degeneracy issues of nested processes recently described by Camerlenghi et al. (2019a). We go beyond the simple yes/no answer to the homogeneity question and embed the proposed prior in a random partition model; this procedure allows us to give a more comprehensive response to the above question and in fact find groups of populations that are internally homogeneous when I greater or equal than 2 such populations are considered. We study theoretical properties of the semi-hierarchical Dirichlet process and of the Bayes factor for the homogeneity test when I = 2. Extensive simulation studies and applications to educational data are also discussed.";"Mario Beraha, Alessandra Guglielmi, Fernando A. Quintana";"Arxiv"
"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence   Clustering";"2017-01-31T18:42:19Z";"http://arxiv.org/abs/1701.09177v5";"We propose an effective method to solve the event sequence clustering problems based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process. In this model, each event sequence belonging to a cluster is generated via the same Hawkes process with specific parameters, and different clusters correspond to different Hawkes processes. The prior distribution of the Hawkes processes is controlled via a Dirichlet distribution. We learn the model via a maximum likelihood estimator (MLE) and propose an effective variational Bayesian inference algorithm. We specifically analyze the resulting EM-type algorithm in the context of inner-outer iterations and discuss several inner iteration allocation strategies. The identifiability of our model, the convergence of our learning method, and its sample complexity are analyzed in both theoretical and empirical ways, which demonstrate the superiority of our method to other competitors. The proposed method learns the number of clusters automatically and is robust to model misspecification. Experiments on both synthetic and real-world data show that our method can learn diverse triggering patterns hidden in asynchronous event sequences and achieve encouraging performance on clustering purity and consistency.";"Hongteng Xu, Hongyuan Zha";"Arxiv"
"Predictive Hierarchical Clustering: Learning clusters of CPT codes for   improving surgical outcomes";"2016-04-24T13:49:23Z";"http://arxiv.org/abs/1604.07031v2";"We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for agglomerative hierarchical clustering of current procedural terminology (CPT) codes. Our predictive hierarchical clustering aims to cluster subgroups, not individual observations, found within our data, such that the clusters discovered result in optimal performance of a classification model. Therefore, merges are chosen based on a Bayesian hypothesis test, which chooses pairings of the subgroups that result in the best model fit, as measured by held out predictive likelihoods. We place a Dirichlet prior on the probability of merging clusters, allowing us to adjust the size and sparsity of clusters. The motivation is to predict patient-specific surgical outcomes using data from ACS NSQIP (American College of Surgeon's National Surgical Quality Improvement Program). An important predictor of surgical outcomes is the actual surgical procedure performed as described by a CPT code. We use PHC to cluster CPT codes, represented as subgroups, together in a way that enables us to better predict patient-specific outcomes compared to currently used clusters based on clinical judgment.";"Elizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, Katherine Heller";"Arxiv"
"Sharp L^p bounds on spectral clusters for Lipschitz metrics";"2012-07-10T17:22:01Z";"http://arxiv.org/abs/1207.2417v1";"We establish L^p bounds on L^2 normalized spectral clusters for self-adjoint elliptic Dirichlet forms with Lipschitz coefficients. In two dimensions we obtain best possible bounds for all p between $2 and infinity, up to logarithmic losses for $6<p\leq 8$. In higher dimensions we obtain best possible bounds for a limited range of p.";"Herbert Koch, Hart Smith, Daniel Tataru";"Arxiv"
"Robust Bayesian inference of networks using Dirichlet t-distributions";"2012-07-05T11:07:27Z";"http://arxiv.org/abs/1207.1221v1";"Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. These models have been used extensively in applications to gene expression data, even in cases where there appears to be significant deviations from the Gaussian model. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. Dirichlet processes are commonly used to cluster independent observations; here they are used instead to cluster the dependent components of a single observation. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties.";"Michael Finegold, Mathias Drton";"Arxiv"
"ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process   Mixtures";"2013-04-08T18:34:32Z";"http://arxiv.org/abs/1304.2302v1";"The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions.";"Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka";"Arxiv"
"Sparse Bayesian Hierarchical Modeling of High-dimensional Clustering   Problems";"2009-04-19T11:33:01Z";"http://arxiv.org/abs/0904.2906v1";"Clustering is one of the most widely used procedures in the analysis of microarray data, for example with the goal of discovering cancer subtypes based on observed heterogeneity of genetic marks between different tissues. It is well-known that in such high-dimensional settings, the existence of many noise variables can overwhelm the few signals embedded in the high-dimensional space. We propose a novel Bayesian approach based on Dirichlet process with a sparsity prior that simultaneous performs variable selection and clustering, and also discover variables that only distinguish a subset of the cluster components. Unlike previous Bayesian formulations, we use Dirichlet process (DP) for both clustering of samples as well as for regularizing the high-dimensional mean/variance structure. To solve the computational challenge brought by this double usage of DP, we propose to make use of a sequential sampling scheme embedded within Markov chain Monte Carlo (MCMC) updates to improve the naive implementation of existing algorithms for DP mixture models. Our method is demonstrated on a simulation study and illustrated with the leukemia gene expression dataset.";"Heng Lian";"Arxiv"
"An Alternative Prior Process for Nonparametric Bayesian Clustering";"2008-01-03T01:10:20Z";"http://arxiv.org/abs/0801.0461v2";"Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit ""rich-get-richer"" characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering -- the uniform process -- for applications where the ""rich-get-richer"" property is undesirable. We also explore the cost of this process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. We compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.";"Hanna M. Wallach, Shane T. Jensen, Lee Dicker, Katherine A. Heller";"Arxiv"
"Infinite mixtures of multivariate normal-inverse Gaussian distributions   for clustering of skewed data";"2020-05-11T17:08:27Z";"http://arxiv.org/abs/2005.05324v1";"Mixtures of multivariate normal inverse Gaussian (MNIG) distributions can be used to cluster data that exhibit features such as skewness and heavy tails. However, for cluster analysis, using a traditional finite mixture model framework, either the number of components needs to be known $a$-$priori$ or needs to be estimated $a$-$posteriori$ using some model selection criterion after deriving results for a range of possible number of components. However, different model selection criteria can sometimes result in different number of components yielding uncertainty. Here, an infinite mixture model framework, also known as Dirichlet process mixture model, is proposed for the mixtures of MNIG distributions. This Dirichlet process mixture model approach allows the number of components to grow or decay freely from 1 to $\infty$ (in practice from 1 to $N$) and the number of components is inferred along with the parameter estimates in a Bayesian framework thus alleviating the need for model selection criteria. We provide real data applications with benchmark datasets as well as a small simulation experiment to compare with other existing models. The proposed method provides competitive clustering results to other clustering approaches for both simulation and real data and parameter recovery are illustrated using simulation studies.";"Yuan Fang, Dimitris Karlis, Sanjeena Subedi";"Arxiv"
"Flexible Bayesian Product Mixture Models for Vector Autoregressions";"2021-11-16T19:31:37Z";"http://arxiv.org/abs/2111.08743v2";"Bayesian non-parametric methods based on Dirichlet process mixtures have seen tremendous success in various domains and are appealing in being able to borrow information by clustering samples that share identical parameters. However, such methods can face hurdles in heterogeneous settings where objects are expected to cluster only along a subset of axes or where clusters of samples share only a subset of identical parameters. We overcome such limitations by developing a novel class of product of Dirichlet process location-scale mixtures that enable independent clustering at multiple scales, which result in varying levels of information sharing across samples. First, we develop the approach for independent multivariate data. Subsequently we generalize it to multivariate time-series data under the framework of multi-subject Vector Autoregressive (VAR) models that is our primary focus, which go beyond parametric single-subject VAR models. We establish posterior consistency and develop efficient posterior computation for implementation. Extensive numerical studies involving VAR models show distinct advantages over competing methods, in terms of estimation, clustering, and feature selection accuracy. Our resting state fMRI analysis from the Human Connectome Project reveals biologically interpretable connectivity differences between distinct intelligence groups, while another air pollution application illustrates the superior forecasting accuracy compared to alternate methods.";"Suprateek Kundu, Joshua Lukemire";"Arxiv"
